
Experimental details:
    Model     : resnet
    Optimizer : adam
    Learning  : 0.001
    Global Rounds   : 30

    Fraction of users  : 1.0
    Local Batch size   : 256
    Local Epochs       : 5.0

Running model  iid_dec-False_ED-False_pe1_a5_e30_le5.0
device:  cuda
sampling for dataset: mnist
Files already downloaded and verified
dataset sample num: (50000, 32, 32, 3)
Files already downloaded and verified
Files already downloaded and verified
get dataset time: 2.650
Use i.i.d. sampling
sample dataset time: 0.027
user data samples: [10000, 10000, 10000, 10000, 10000]
Namespace(exact_diffusion=False, decentralized=False, edge_prob=1, epochs=30, num_users=5, frac=1.0, local_ep=5.0, local_bs=256, lr=0.001, momentum=0.9, num_workers=16, model='resnet', batch_size=256, weight_decay=0.0005, dataset='mnist', backbone='resnet18', num_classes=10, gpu='0', optimizer='adam', save_name_suffix='', iid=1, verbose=0, seed=1, feature_dim=128, temperature=0.5, k=200, ssl_method='simclr', x_noniid=False, dirichlet=False, test_intermediate=False, dir_beta=0.5, imagenet_based_cluster=False, y_partition=False, log_file_name='results/05-18_04:52_testingPhi_iid_dec-false_ED-false_ep0_e30_le5_a5/skew_ssl_comm', num_clusters=1, imagenet100=False, y_partition_skew=True, y_partition_ratio=0.0, x_shift_dirichlet=False, reg_scale=1, load_pretrained_path='', full_size=False, local_rank=0, distributed_training=False, log_directory='results/05-18_04:52_testingPhi_iid_dec-false_ED-false_ep0_e30_le5_a5', emd=0, dist_url='env://', average_without_bn=False, model_continue_training=0, finetuning_epoch=60, script_name='', x_shift_skew=False, load_dataset_to_memory=False)
output model: save/05-18_04:53_3082253iid_dec-False_ED-False_pe1_a5_e30_le5.0
number of users per round: 5
total number of rounds: 6
Training uses 0.8 andd validation 0.8 of the dataset
Training uses 0.8 andd validation 0.8 of the dataset
Training uses 0.8 andd validation 0.8 of the dataset
Training uses 0.8 andd validation 0.8 of the dataset
Training uses 0.8 andd validation 0.8 of the dataset

 | Global Training Round : 1 | Model : 05-18_04:53_3082253

Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 0.0 | [0/8000 (0%)]	Loss: 6.156796                         LR: 0.0010  Feat: 0.752 Epoch Time: 8.866 Model Time: 8.184 Data Time: 0.682 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 0.0 | [2560/8000 (32%)]	Loss: 5.683749                         LR: 0.0010  Feat: 0.884 Epoch Time: 10.334 Model Time: 0.050 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 0.0 | [5120/8000 (65%)]	Loss: 5.471533                         LR: 0.0010  Feat: 0.950 Epoch Time: 11.779 Model Time: 0.050 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 0.0 | [7680/8000 (97%)]	Loss: 5.418124                         LR: 0.0010  Feat: 0.954 Epoch Time: 13.227 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 1.0 | [0/8000 (0%)]	Loss: 5.480686                         LR: 0.0010  Feat: 0.957 Epoch Time: 14.099 Model Time: 0.054 Data Time: 0.694 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 1.0 | [2560/8000 (32%)]	Loss: 5.524838                         LR: 0.0010  Feat: 0.960 Epoch Time: 15.549 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 1.0 | [5120/8000 (65%)]	Loss: 5.462189                         LR: 0.0010  Feat: 0.973 Epoch Time: 16.999 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 1.0 | [7680/8000 (97%)]	Loss: 5.440617                         LR: 0.0010  Feat: 0.975 Epoch Time: 18.451 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 2.0 | [0/8000 (0%)]	Loss: 5.376995                         LR: 0.0010  Feat: 0.976 Epoch Time: 19.383 Model Time: 0.054 Data Time: 0.741 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 2.0 | [2560/8000 (32%)]	Loss: 5.379883                         LR: 0.0010  Feat: 0.970 Epoch Time: 20.834 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 2.0 | [5120/8000 (65%)]	Loss: 5.319749                         LR: 0.0010  Feat: 0.979 Epoch Time: 22.284 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 2.0 | [7680/8000 (97%)]	Loss: 5.449133                         LR: 0.0010  Feat: 0.984 Epoch Time: 23.737 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 3.0 | [0/8000 (0%)]	Loss: 5.224276                         LR: 0.0010  Feat: 0.985 Epoch Time: 24.727 Model Time: 0.060 Data Time: 0.787 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 3.0 | [2560/8000 (32%)]	Loss: 5.387412                         LR: 0.0010  Feat: 0.977 Epoch Time: 26.183 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 3.0 | [5120/8000 (65%)]	Loss: 5.329482                         LR: 0.0010  Feat: 0.985 Epoch Time: 27.638 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 3.0 | [7680/8000 (97%)]	Loss: 5.400664                         LR: 0.0010  Feat: 0.984 Epoch Time: 29.093 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 4.0 | [0/8000 (0%)]	Loss: 5.312851                         LR: 0.0010  Feat: 0.987 Epoch Time: 29.998 Model Time: 0.055 Data Time: 0.715 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 4.0 | [2560/8000 (32%)]	Loss: 5.284895                         LR: 0.0010  Feat: 0.986 Epoch Time: 31.456 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 4.0 | [5120/8000 (65%)]	Loss: 5.283781                         LR: 0.0010  Feat: 0.987 Epoch Time: 32.913 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 0 | Local Epoch : 4.0 | [7680/8000 (97%)]	Loss: 5.254240                         LR: 0.0010  Feat: 0.982 Epoch Time: 34.372 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 0.0 | [0/8000 (0%)]	Loss: 6.106090                         LR: 0.0010  Feat: 0.744 Epoch Time: 0.855 Model Time: 0.061 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 0.0 | [2560/8000 (32%)]	Loss: 5.723476                         LR: 0.0010  Feat: 0.905 Epoch Time: 2.312 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 0.0 | [5120/8000 (65%)]	Loss: 5.617901                         LR: 0.0010  Feat: 0.939 Epoch Time: 3.771 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 0.0 | [7680/8000 (97%)]	Loss: 5.533007                         LR: 0.0010  Feat: 0.934 Epoch Time: 5.231 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 1.0 | [0/8000 (0%)]	Loss: 5.582250                         LR: 0.0010  Feat: 0.942 Epoch Time: 6.099 Model Time: 0.055 Data Time: 0.675 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 1.0 | [2560/8000 (32%)]	Loss: 5.499986                         LR: 0.0010  Feat: 0.968 Epoch Time: 7.565 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 1.0 | [5120/8000 (65%)]	Loss: 5.475535                         LR: 0.0010  Feat: 0.974 Epoch Time: 9.026 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 1.0 | [7680/8000 (97%)]	Loss: 5.387370                         LR: 0.0010  Feat: 0.978 Epoch Time: 10.489 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 2.0 | [0/8000 (0%)]	Loss: 5.454195                         LR: 0.0010  Feat: 0.976 Epoch Time: 11.358 Model Time: 0.055 Data Time: 0.677 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 2.0 | [2560/8000 (32%)]	Loss: 5.438658                         LR: 0.0010  Feat: 0.987 Epoch Time: 12.825 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 2.0 | [5120/8000 (65%)]	Loss: 5.428207                         LR: 0.0010  Feat: 0.983 Epoch Time: 14.288 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 2.0 | [7680/8000 (97%)]	Loss: 5.327285                         LR: 0.0010  Feat: 0.989 Epoch Time: 15.753 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 3.0 | [0/8000 (0%)]	Loss: 5.354987                         LR: 0.0010  Feat: 0.987 Epoch Time: 16.628 Model Time: 0.054 Data Time: 0.683 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 3.0 | [2560/8000 (32%)]	Loss: 5.417269                         LR: 0.0010  Feat: 0.994 Epoch Time: 18.096 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 3.0 | [5120/8000 (65%)]	Loss: 5.366250                         LR: 0.0010  Feat: 0.992 Epoch Time: 19.562 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 3.0 | [7680/8000 (97%)]	Loss: 5.357536                         LR: 0.0010  Feat: 0.993 Epoch Time: 21.028 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 4.0 | [0/8000 (0%)]	Loss: 5.326417                         LR: 0.0010  Feat: 0.995 Epoch Time: 21.900 Model Time: 0.056 Data Time: 0.679 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 4.0 | [2560/8000 (32%)]	Loss: 5.281960                         LR: 0.0010  Feat: 0.994 Epoch Time: 23.368 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 4.0 | [5120/8000 (65%)]	Loss: 5.341852                         LR: 0.0010  Feat: 0.992 Epoch Time: 24.834 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 0 | Local Epoch : 4.0 | [7680/8000 (97%)]	Loss: 5.307157                         LR: 0.0010  Feat: 0.997 Epoch Time: 26.304 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 0.0 | [0/8000 (0%)]	Loss: 6.137561                         LR: 0.0010  Feat: 0.756 Epoch Time: 0.843 Model Time: 0.056 Data Time: 0.695 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 0.0 | [2560/8000 (32%)]	Loss: 5.688514                         LR: 0.0010  Feat: 0.893 Epoch Time: 2.315 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 0.0 | [5120/8000 (65%)]	Loss: 5.524006                         LR: 0.0010  Feat: 0.944 Epoch Time: 3.786 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 0.0 | [7680/8000 (97%)]	Loss: 5.505008                         LR: 0.0010  Feat: 0.950 Epoch Time: 5.261 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 1.0 | [0/8000 (0%)]	Loss: 5.540977                         LR: 0.0010  Feat: 0.953 Epoch Time: 6.140 Model Time: 0.060 Data Time: 0.685 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 1.0 | [2560/8000 (32%)]	Loss: 5.478874                         LR: 0.0010  Feat: 0.970 Epoch Time: 7.612 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 1.0 | [5120/8000 (65%)]	Loss: 5.502041                         LR: 0.0010  Feat: 0.976 Epoch Time: 9.081 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 1.0 | [7680/8000 (97%)]	Loss: 5.393515                         LR: 0.0010  Feat: 0.973 Epoch Time: 10.551 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 2.0 | [0/8000 (0%)]	Loss: 5.466749                         LR: 0.0010  Feat: 0.967 Epoch Time: 11.556 Model Time: 0.055 Data Time: 0.806 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 2.0 | [2560/8000 (32%)]	Loss: 5.337893                         LR: 0.0010  Feat: 0.981 Epoch Time: 13.029 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 2.0 | [5120/8000 (65%)]	Loss: 5.381695                         LR: 0.0010  Feat: 0.978 Epoch Time: 14.501 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 2.0 | [7680/8000 (97%)]	Loss: 5.294807                         LR: 0.0010  Feat: 0.987 Epoch Time: 15.972 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 3.0 | [0/8000 (0%)]	Loss: 5.350524                         LR: 0.0010  Feat: 0.985 Epoch Time: 16.899 Model Time: 0.056 Data Time: 0.733 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 3.0 | [2560/8000 (32%)]	Loss: 5.317912                         LR: 0.0010  Feat: 0.983 Epoch Time: 18.372 Model Time: 0.051 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 3.0 | [5120/8000 (65%)]	Loss: 5.318600                         LR: 0.0010  Feat: 0.982 Epoch Time: 19.842 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 3.0 | [7680/8000 (97%)]	Loss: 5.348634                         LR: 0.0010  Feat: 0.980 Epoch Time: 21.312 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 4.0 | [0/8000 (0%)]	Loss: 5.363091                         LR: 0.0010  Feat: 0.984 Epoch Time: 22.201 Model Time: 0.057 Data Time: 0.696 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 4.0 | [2560/8000 (32%)]	Loss: 5.299587                         LR: 0.0010  Feat: 0.983 Epoch Time: 23.675 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 4.0 | [5120/8000 (65%)]	Loss: 5.243967                         LR: 0.0010  Feat: 0.987 Epoch Time: 25.147 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 0 | Local Epoch : 4.0 | [7680/8000 (97%)]	Loss: 5.214766                         LR: 0.0010  Feat: 0.986 Epoch Time: 26.619 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 0.0 | [0/8000 (0%)]	Loss: 6.113833                         LR: 0.0010  Feat: 0.762 Epoch Time: 0.854 Model Time: 0.057 Data Time: 0.706 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 0.0 | [2560/8000 (32%)]	Loss: 5.659458                         LR: 0.0010  Feat: 0.892 Epoch Time: 2.331 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 0.0 | [5120/8000 (65%)]	Loss: 5.531029                         LR: 0.0010  Feat: 0.933 Epoch Time: 3.804 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 0.0 | [7680/8000 (97%)]	Loss: 5.564297                         LR: 0.0010  Feat: 0.970 Epoch Time: 5.276 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 1.0 | [0/8000 (0%)]	Loss: 5.497834                         LR: 0.0010  Feat: 0.971 Epoch Time: 6.201 Model Time: 0.054 Data Time: 0.732 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 1.0 | [2560/8000 (32%)]	Loss: 5.453740                         LR: 0.0010  Feat: 0.975 Epoch Time: 7.677 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 1.0 | [5120/8000 (65%)]	Loss: 5.518751                         LR: 0.0010  Feat: 0.974 Epoch Time: 9.150 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 1.0 | [7680/8000 (97%)]	Loss: 5.338030                         LR: 0.0010  Feat: 0.981 Epoch Time: 10.624 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 2.0 | [0/8000 (0%)]	Loss: 5.490265                         LR: 0.0010  Feat: 0.975 Epoch Time: 11.516 Model Time: 0.055 Data Time: 0.695 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 2.0 | [2560/8000 (32%)]	Loss: 5.445077                         LR: 0.0010  Feat: 0.978 Epoch Time: 12.992 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 2.0 | [5120/8000 (65%)]	Loss: 5.317375                         LR: 0.0010  Feat: 0.979 Epoch Time: 14.471 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 2.0 | [7680/8000 (97%)]	Loss: 5.386302                         LR: 0.0010  Feat: 0.981 Epoch Time: 15.945 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 3.0 | [0/8000 (0%)]	Loss: 5.375019                         LR: 0.0010  Feat: 0.981 Epoch Time: 16.831 Model Time: 0.054 Data Time: 0.694 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 3.0 | [2560/8000 (32%)]	Loss: 5.312082                         LR: 0.0010  Feat: 0.986 Epoch Time: 18.305 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 3.0 | [5120/8000 (65%)]	Loss: 5.277603                         LR: 0.0010  Feat: 0.986 Epoch Time: 19.779 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 3.0 | [7680/8000 (97%)]	Loss: 5.224716                         LR: 0.0010  Feat: 0.982 Epoch Time: 21.252 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 4.0 | [0/8000 (0%)]	Loss: 5.298906                         LR: 0.0010  Feat: 0.985 Epoch Time: 22.139 Model Time: 0.055 Data Time: 0.691 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 4.0 | [2560/8000 (32%)]	Loss: 5.278435                         LR: 0.0010  Feat: 0.987 Epoch Time: 23.615 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 4.0 | [5120/8000 (65%)]	Loss: 5.283765                         LR: 0.0010  Feat: 0.984 Epoch Time: 25.091 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 0 | Local Epoch : 4.0 | [7680/8000 (97%)]	Loss: 5.325174                         LR: 0.0010  Feat: 0.993 Epoch Time: 26.567 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 0.0 | [0/8000 (0%)]	Loss: 6.128836                         LR: 0.0010  Feat: 0.763 Epoch Time: 0.831 Model Time: 0.056 Data Time: 0.683 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 0.0 | [2560/8000 (32%)]	Loss: 5.656107                         LR: 0.0010  Feat: 0.922 Epoch Time: 2.309 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 0.0 | [5120/8000 (65%)]	Loss: 5.601590                         LR: 0.0010  Feat: 0.948 Epoch Time: 3.786 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 0.0 | [7680/8000 (97%)]	Loss: 5.552990                         LR: 0.0010  Feat: 0.968 Epoch Time: 5.263 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 1.0 | [0/8000 (0%)]	Loss: 5.500895                         LR: 0.0010  Feat: 0.970 Epoch Time: 6.135 Model Time: 0.056 Data Time: 0.681 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 1.0 | [2560/8000 (32%)]	Loss: 5.517544                         LR: 0.0010  Feat: 0.980 Epoch Time: 7.613 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 1.0 | [5120/8000 (65%)]	Loss: 5.463591                         LR: 0.0010  Feat: 0.973 Epoch Time: 9.087 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 1.0 | [7680/8000 (97%)]	Loss: 5.454631                         LR: 0.0010  Feat: 0.984 Epoch Time: 10.562 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 2.0 | [0/8000 (0%)]	Loss: 5.459560                         LR: 0.0010  Feat: 0.983 Epoch Time: 11.444 Model Time: 0.055 Data Time: 0.688 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 2.0 | [2560/8000 (32%)]	Loss: 5.323200                         LR: 0.0010  Feat: 0.996 Epoch Time: 13.087 Model Time: 0.052 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 2.0 | [5120/8000 (65%)]	Loss: 5.417893                         LR: 0.0010  Feat: 0.990 Epoch Time: 14.566 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 2.0 | [7680/8000 (97%)]	Loss: 5.368576                         LR: 0.0010  Feat: 0.974 Epoch Time: 16.045 Model Time: 0.053 Data Time: 0.095 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 3.0 | [0/8000 (0%)]	Loss: 5.407617                         LR: 0.0010  Feat: 0.978 Epoch Time: 16.930 Model Time: 0.056 Data Time: 0.692 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 3.0 | [2560/8000 (32%)]	Loss: 5.332002                         LR: 0.0010  Feat: 0.979 Epoch Time: 18.418 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 3.0 | [5120/8000 (65%)]	Loss: 5.372921                         LR: 0.0010  Feat: 0.990 Epoch Time: 19.897 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 3.0 | [7680/8000 (97%)]	Loss: 5.328627                         LR: 0.0010  Feat: 0.989 Epoch Time: 21.375 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 4.0 | [0/8000 (0%)]	Loss: 5.347591                         LR: 0.0010  Feat: 0.984 Epoch Time: 22.244 Model Time: 0.056 Data Time: 0.672 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 4.0 | [2560/8000 (32%)]	Loss: 5.301443                         LR: 0.0010  Feat: 0.991 Epoch Time: 23.729 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 4.0 | [5120/8000 (65%)]	Loss: 5.306657                         LR: 0.0010  Feat: 0.998 Epoch Time: 25.209 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 0 | Local Epoch : 4.0 | [7680/8000 (97%)]	Loss: 5.271062                         LR: 0.0010  Feat: 0.999 Epoch Time: 26.689 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Federated averaging

 | Global Training Round : 2 | Model : 05-18_04:53_3082253

Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 5.0 | [0/8000 (0%)]	Loss: 5.382731                         LR: 0.0003  Feat: 0.988 Epoch Time: 0.840 Model Time: 0.058 Data Time: 0.689 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 5.0 | [2560/8000 (32%)]	Loss: 5.336561                         LR: 0.0003  Feat: 0.967 Epoch Time: 2.320 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 5.0 | [5120/8000 (65%)]	Loss: 5.243845                         LR: 0.0003  Feat: 0.977 Epoch Time: 3.797 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 5.0 | [7680/8000 (97%)]	Loss: 5.302769                         LR: 0.0003  Feat: 0.975 Epoch Time: 5.275 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 6.0 | [0/8000 (0%)]	Loss: 5.307546                         LR: 0.0003  Feat: 0.975 Epoch Time: 6.302 Model Time: 0.058 Data Time: 0.819 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 6.0 | [2560/8000 (32%)]	Loss: 5.253271                         LR: 0.0003  Feat: 0.978 Epoch Time: 7.783 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 6.0 | [5120/8000 (65%)]	Loss: 5.312488                         LR: 0.0003  Feat: 0.978 Epoch Time: 9.262 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 6.0 | [7680/8000 (97%)]	Loss: 5.209993                         LR: 0.0003  Feat: 0.972 Epoch Time: 10.740 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 7.0 | [0/8000 (0%)]	Loss: 5.287786                         LR: 0.0003  Feat: 0.974 Epoch Time: 11.664 Model Time: 0.054 Data Time: 0.732 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 7.0 | [2560/8000 (32%)]	Loss: 5.191611                         LR: 0.0003  Feat: 0.971 Epoch Time: 13.148 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 7.0 | [5120/8000 (65%)]	Loss: 5.207403                         LR: 0.0003  Feat: 0.971 Epoch Time: 14.628 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 7.0 | [7680/8000 (97%)]	Loss: 5.238542                         LR: 0.0003  Feat: 0.981 Epoch Time: 16.106 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 8.0 | [0/8000 (0%)]	Loss: 5.204432                         LR: 0.0003  Feat: 0.981 Epoch Time: 17.025 Model Time: 0.055 Data Time: 0.726 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 8.0 | [2560/8000 (32%)]	Loss: 5.204663                         LR: 0.0003  Feat: 0.981 Epoch Time: 18.505 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 8.0 | [5120/8000 (65%)]	Loss: 5.120214                         LR: 0.0003  Feat: 0.977 Epoch Time: 19.984 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 8.0 | [7680/8000 (97%)]	Loss: 5.164018                         LR: 0.0003  Feat: 0.973 Epoch Time: 21.464 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 9.0 | [0/8000 (0%)]	Loss: 5.170816                         LR: 0.0003  Feat: 0.977 Epoch Time: 22.355 Model Time: 0.056 Data Time: 0.695 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 9.0 | [2560/8000 (32%)]	Loss: 5.146686                         LR: 0.0003  Feat: 0.981 Epoch Time: 23.836 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 9.0 | [5120/8000 (65%)]	Loss: 5.189142                         LR: 0.0003  Feat: 0.975 Epoch Time: 25.315 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 1 | Local Epoch : 9.0 | [7680/8000 (97%)]	Loss: 5.202073                         LR: 0.0003  Feat: 0.976 Epoch Time: 26.794 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 5.0 | [0/8000 (0%)]	Loss: 5.428291                         LR: 0.0003  Feat: 0.991 Epoch Time: 0.834 Model Time: 0.060 Data Time: 0.683 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 5.0 | [2560/8000 (32%)]	Loss: 5.391335                         LR: 0.0003  Feat: 0.980 Epoch Time: 2.314 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 5.0 | [5120/8000 (65%)]	Loss: 5.287200                         LR: 0.0003  Feat: 0.980 Epoch Time: 3.791 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 5.0 | [7680/8000 (97%)]	Loss: 5.313089                         LR: 0.0003  Feat: 0.977 Epoch Time: 5.269 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 6.0 | [0/8000 (0%)]	Loss: 5.241116                         LR: 0.0003  Feat: 0.983 Epoch Time: 6.150 Model Time: 0.057 Data Time: 0.683 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 6.0 | [2560/8000 (32%)]	Loss: 5.306029                         LR: 0.0003  Feat: 0.980 Epoch Time: 7.633 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 6.0 | [5120/8000 (65%)]	Loss: 5.205062                         LR: 0.0003  Feat: 0.978 Epoch Time: 9.111 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 6.0 | [7680/8000 (97%)]	Loss: 5.177589                         LR: 0.0003  Feat: 0.985 Epoch Time: 10.589 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 7.0 | [0/8000 (0%)]	Loss: 5.184643                         LR: 0.0003  Feat: 0.984 Epoch Time: 11.479 Model Time: 0.055 Data Time: 0.693 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 7.0 | [2560/8000 (32%)]	Loss: 5.252618                         LR: 0.0003  Feat: 0.986 Epoch Time: 12.960 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 7.0 | [5120/8000 (65%)]	Loss: 5.252851                         LR: 0.0003  Feat: 0.986 Epoch Time: 14.441 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 7.0 | [7680/8000 (97%)]	Loss: 5.168793                         LR: 0.0003  Feat: 0.979 Epoch Time: 15.921 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 8.0 | [0/8000 (0%)]	Loss: 5.254720                         LR: 0.0003  Feat: 0.979 Epoch Time: 16.809 Model Time: 0.061 Data Time: 0.692 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 8.0 | [2560/8000 (32%)]	Loss: 5.199816                         LR: 0.0003  Feat: 0.981 Epoch Time: 18.294 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 8.0 | [5120/8000 (65%)]	Loss: 5.147828                         LR: 0.0003  Feat: 0.982 Epoch Time: 19.778 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 8.0 | [7680/8000 (97%)]	Loss: 5.171494                         LR: 0.0003  Feat: 0.983 Epoch Time: 21.261 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 9.0 | [0/8000 (0%)]	Loss: 5.135604                         LR: 0.0003  Feat: 0.983 Epoch Time: 22.151 Model Time: 0.059 Data Time: 0.696 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 9.0 | [2560/8000 (32%)]	Loss: 5.104940                         LR: 0.0003  Feat: 0.976 Epoch Time: 23.635 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 9.0 | [5120/8000 (65%)]	Loss: 5.104747                         LR: 0.0003  Feat: 0.977 Epoch Time: 25.119 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 1 | Local Epoch : 9.0 | [7680/8000 (97%)]	Loss: 5.162998                         LR: 0.0003  Feat: 0.976 Epoch Time: 26.599 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 5.0 | [0/8000 (0%)]	Loss: 5.403347                         LR: 0.0003  Feat: 0.986 Epoch Time: 0.970 Model Time: 0.057 Data Time: 0.820 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 5.0 | [2560/8000 (32%)]	Loss: 5.264466                         LR: 0.0003  Feat: 0.975 Epoch Time: 2.454 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 5.0 | [5120/8000 (65%)]	Loss: 5.327199                         LR: 0.0003  Feat: 0.978 Epoch Time: 3.935 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 5.0 | [7680/8000 (97%)]	Loss: 5.335830                         LR: 0.0003  Feat: 0.979 Epoch Time: 5.417 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 6.0 | [0/8000 (0%)]	Loss: 5.280355                         LR: 0.0003  Feat: 0.977 Epoch Time: 6.395 Model Time: 0.058 Data Time: 0.774 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 6.0 | [2560/8000 (32%)]	Loss: 5.263339                         LR: 0.0003  Feat: 0.977 Epoch Time: 7.878 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 6.0 | [5120/8000 (65%)]	Loss: 5.243297                         LR: 0.0003  Feat: 0.981 Epoch Time: 9.358 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 6.0 | [7680/8000 (97%)]	Loss: 5.197166                         LR: 0.0003  Feat: 0.979 Epoch Time: 10.840 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 7.0 | [0/8000 (0%)]	Loss: 5.245316                         LR: 0.0003  Feat: 0.977 Epoch Time: 11.759 Model Time: 0.057 Data Time: 0.719 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 7.0 | [2560/8000 (32%)]	Loss: 5.179448                         LR: 0.0003  Feat: 0.982 Epoch Time: 13.242 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 7.0 | [5120/8000 (65%)]	Loss: 5.175126                         LR: 0.0003  Feat: 0.986 Epoch Time: 14.721 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 7.0 | [7680/8000 (97%)]	Loss: 5.205551                         LR: 0.0003  Feat: 0.981 Epoch Time: 16.200 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 8.0 | [0/8000 (0%)]	Loss: 5.194119                         LR: 0.0003  Feat: 0.984 Epoch Time: 17.125 Model Time: 0.057 Data Time: 0.724 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 8.0 | [2560/8000 (32%)]	Loss: 5.232781                         LR: 0.0003  Feat: 0.980 Epoch Time: 18.610 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 8.0 | [5120/8000 (65%)]	Loss: 5.175205                         LR: 0.0003  Feat: 0.984 Epoch Time: 20.088 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 8.0 | [7680/8000 (97%)]	Loss: 5.173556                         LR: 0.0003  Feat: 0.984 Epoch Time: 21.568 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 9.0 | [0/8000 (0%)]	Loss: 5.183854                         LR: 0.0003  Feat: 0.985 Epoch Time: 22.593 Model Time: 0.056 Data Time: 0.822 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 9.0 | [2560/8000 (32%)]	Loss: 5.135022                         LR: 0.0003  Feat: 0.982 Epoch Time: 24.072 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 9.0 | [5120/8000 (65%)]	Loss: 5.154238                         LR: 0.0003  Feat: 0.979 Epoch Time: 25.550 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 1 | Local Epoch : 9.0 | [7680/8000 (97%)]	Loss: 5.144090                         LR: 0.0003  Feat: 0.984 Epoch Time: 27.031 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 5.0 | [0/8000 (0%)]	Loss: 5.490788                         LR: 0.0003  Feat: 0.989 Epoch Time: 0.865 Model Time: 0.057 Data Time: 0.714 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 5.0 | [2560/8000 (32%)]	Loss: 5.300059                         LR: 0.0003  Feat: 0.973 Epoch Time: 2.346 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 5.0 | [5120/8000 (65%)]	Loss: 5.334683                         LR: 0.0003  Feat: 0.967 Epoch Time: 3.824 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 5.0 | [7680/8000 (97%)]	Loss: 5.317554                         LR: 0.0003  Feat: 0.966 Epoch Time: 5.301 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 6.0 | [0/8000 (0%)]	Loss: 5.239291                         LR: 0.0003  Feat: 0.972 Epoch Time: 6.232 Model Time: 0.056 Data Time: 0.735 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 6.0 | [2560/8000 (32%)]	Loss: 5.210024                         LR: 0.0003  Feat: 0.974 Epoch Time: 7.712 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 6.0 | [5120/8000 (65%)]	Loss: 5.214712                         LR: 0.0003  Feat: 0.974 Epoch Time: 9.192 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 6.0 | [7680/8000 (97%)]	Loss: 5.224760                         LR: 0.0003  Feat: 0.978 Epoch Time: 10.670 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 7.0 | [0/8000 (0%)]	Loss: 5.232063                         LR: 0.0003  Feat: 0.977 Epoch Time: 11.576 Model Time: 0.055 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 7.0 | [2560/8000 (32%)]	Loss: 5.247813                         LR: 0.0003  Feat: 0.977 Epoch Time: 13.061 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 7.0 | [5120/8000 (65%)]	Loss: 5.156398                         LR: 0.0003  Feat: 0.978 Epoch Time: 14.543 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 7.0 | [7680/8000 (97%)]	Loss: 5.263674                         LR: 0.0003  Feat: 0.976 Epoch Time: 16.023 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 8.0 | [0/8000 (0%)]	Loss: 5.182920                         LR: 0.0003  Feat: 0.975 Epoch Time: 16.915 Model Time: 0.059 Data Time: 0.686 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 8.0 | [2560/8000 (32%)]	Loss: 5.239091                         LR: 0.0003  Feat: 0.975 Epoch Time: 18.397 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 8.0 | [5120/8000 (65%)]	Loss: 5.227897                         LR: 0.0003  Feat: 0.978 Epoch Time: 19.887 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 8.0 | [7680/8000 (97%)]	Loss: 5.217150                         LR: 0.0003  Feat: 0.976 Epoch Time: 21.367 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 9.0 | [0/8000 (0%)]	Loss: 5.200212                         LR: 0.0003  Feat: 0.978 Epoch Time: 22.249 Model Time: 0.061 Data Time: 0.686 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 9.0 | [2560/8000 (32%)]	Loss: 5.148721                         LR: 0.0003  Feat: 0.978 Epoch Time: 23.729 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 9.0 | [5120/8000 (65%)]	Loss: 5.183939                         LR: 0.0003  Feat: 0.976 Epoch Time: 25.209 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 1 | Local Epoch : 9.0 | [7680/8000 (97%)]	Loss: 5.163891                         LR: 0.0003  Feat: 0.984 Epoch Time: 26.689 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 5.0 | [0/8000 (0%)]	Loss: 5.398850                         LR: 0.0003  Feat: 0.986 Epoch Time: 0.839 Model Time: 0.056 Data Time: 0.689 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 5.0 | [2560/8000 (32%)]	Loss: 5.339036                         LR: 0.0003  Feat: 0.979 Epoch Time: 2.324 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 5.0 | [5120/8000 (65%)]	Loss: 5.303454                         LR: 0.0003  Feat: 0.975 Epoch Time: 3.805 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 5.0 | [7680/8000 (97%)]	Loss: 5.246839                         LR: 0.0003  Feat: 0.968 Epoch Time: 5.286 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 6.0 | [0/8000 (0%)]	Loss: 5.319031                         LR: 0.0003  Feat: 0.971 Epoch Time: 6.179 Model Time: 0.055 Data Time: 0.694 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 6.0 | [2560/8000 (32%)]	Loss: 5.270091                         LR: 0.0003  Feat: 0.977 Epoch Time: 7.661 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 6.0 | [5120/8000 (65%)]	Loss: 5.263552                         LR: 0.0003  Feat: 0.982 Epoch Time: 9.144 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 6.0 | [7680/8000 (97%)]	Loss: 5.272126                         LR: 0.0003  Feat: 0.981 Epoch Time: 10.624 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 7.0 | [0/8000 (0%)]	Loss: 5.190804                         LR: 0.0003  Feat: 0.980 Epoch Time: 11.556 Model Time: 0.058 Data Time: 0.734 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 7.0 | [2560/8000 (32%)]	Loss: 5.221576                         LR: 0.0003  Feat: 0.981 Epoch Time: 13.038 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 7.0 | [5120/8000 (65%)]	Loss: 5.208793                         LR: 0.0003  Feat: 0.981 Epoch Time: 14.519 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 7.0 | [7680/8000 (97%)]	Loss: 5.139519                         LR: 0.0003  Feat: 0.981 Epoch Time: 15.999 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 8.0 | [0/8000 (0%)]	Loss: 5.216942                         LR: 0.0003  Feat: 0.979 Epoch Time: 16.886 Model Time: 0.056 Data Time: 0.691 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 8.0 | [2560/8000 (32%)]	Loss: 5.208859                         LR: 0.0003  Feat: 0.982 Epoch Time: 18.375 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 8.0 | [5120/8000 (65%)]	Loss: 5.191374                         LR: 0.0003  Feat: 0.980 Epoch Time: 19.856 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 8.0 | [7680/8000 (97%)]	Loss: 5.116624                         LR: 0.0003  Feat: 0.981 Epoch Time: 21.339 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 9.0 | [0/8000 (0%)]	Loss: 5.271000                         LR: 0.0003  Feat: 0.979 Epoch Time: 22.260 Model Time: 0.055 Data Time: 0.721 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 9.0 | [2560/8000 (32%)]	Loss: 5.193479                         LR: 0.0003  Feat: 0.982 Epoch Time: 23.744 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 9.0 | [5120/8000 (65%)]	Loss: 5.250722                         LR: 0.0003  Feat: 0.977 Epoch Time: 25.225 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 1 | Local Epoch : 9.0 | [7680/8000 (97%)]	Loss: 5.138434                         LR: 0.0003  Feat: 0.976 Epoch Time: 26.707 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Federated averaging

 | Global Training Round : 3 | Model : 05-18_04:53_3082253

Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 10.0 | [0/8000 (0%)]	Loss: 5.133018                         LR: 0.0003  Feat: 0.982 Epoch Time: 0.842 Model Time: 0.058 Data Time: 0.691 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 10.0 | [2560/8000 (32%)]	Loss: 5.157942                         LR: 0.0003  Feat: 0.979 Epoch Time: 2.324 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 10.0 | [5120/8000 (65%)]	Loss: 5.135880                         LR: 0.0003  Feat: 0.980 Epoch Time: 3.940 Model Time: 0.189 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 10.0 | [7680/8000 (97%)]	Loss: 5.084487                         LR: 0.0003  Feat: 0.981 Epoch Time: 5.422 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 11.0 | [0/8000 (0%)]	Loss: 5.168244                         LR: 0.0003  Feat: 0.983 Epoch Time: 6.304 Model Time: 0.058 Data Time: 0.682 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 11.0 | [2560/8000 (32%)]	Loss: 5.175940                         LR: 0.0003  Feat: 0.980 Epoch Time: 7.790 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 11.0 | [5120/8000 (65%)]	Loss: 5.135006                         LR: 0.0003  Feat: 0.982 Epoch Time: 9.276 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 11.0 | [7680/8000 (97%)]	Loss: 5.036761                         LR: 0.0003  Feat: 0.983 Epoch Time: 10.757 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 12.0 | [0/8000 (0%)]	Loss: 5.192675                         LR: 0.0003  Feat: 0.981 Epoch Time: 11.678 Model Time: 0.055 Data Time: 0.726 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 12.0 | [2560/8000 (32%)]	Loss: 5.100170                         LR: 0.0003  Feat: 0.980 Epoch Time: 13.161 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 12.0 | [5120/8000 (65%)]	Loss: 5.136192                         LR: 0.0003  Feat: 0.980 Epoch Time: 14.844 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 12.0 | [7680/8000 (97%)]	Loss: 5.114478                         LR: 0.0003  Feat: 0.980 Epoch Time: 16.325 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 13.0 | [0/8000 (0%)]	Loss: 5.133132                         LR: 0.0003  Feat: 0.982 Epoch Time: 17.212 Model Time: 0.055 Data Time: 0.690 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 13.0 | [2560/8000 (32%)]	Loss: 5.130073                         LR: 0.0003  Feat: 0.986 Epoch Time: 18.693 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 13.0 | [5120/8000 (65%)]	Loss: 5.128305                         LR: 0.0003  Feat: 0.982 Epoch Time: 20.173 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 13.0 | [7680/8000 (97%)]	Loss: 5.136867                         LR: 0.0003  Feat: 0.985 Epoch Time: 21.654 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 14.0 | [0/8000 (0%)]	Loss: 5.101349                         LR: 0.0003  Feat: 0.985 Epoch Time: 22.547 Model Time: 0.063 Data Time: 0.697 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 14.0 | [2560/8000 (32%)]	Loss: 5.117372                         LR: 0.0003  Feat: 0.983 Epoch Time: 24.029 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 14.0 | [5120/8000 (65%)]	Loss: 5.117793                         LR: 0.0003  Feat: 0.985 Epoch Time: 25.518 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 2 | Local Epoch : 14.0 | [7680/8000 (97%)]	Loss: 5.080113                         LR: 0.0003  Feat: 0.983 Epoch Time: 26.998 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 10.0 | [0/8000 (0%)]	Loss: 5.137319                         LR: 0.0003  Feat: 0.982 Epoch Time: 0.824 Model Time: 0.056 Data Time: 0.674 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 10.0 | [2560/8000 (32%)]	Loss: 5.186593                         LR: 0.0003  Feat: 0.984 Epoch Time: 2.313 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 10.0 | [5120/8000 (65%)]	Loss: 5.153049                         LR: 0.0003  Feat: 0.991 Epoch Time: 3.797 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 10.0 | [7680/8000 (97%)]	Loss: 5.214858                         LR: 0.0003  Feat: 0.991 Epoch Time: 5.280 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 11.0 | [0/8000 (0%)]	Loss: 5.161501                         LR: 0.0003  Feat: 0.987 Epoch Time: 6.170 Model Time: 0.055 Data Time: 0.693 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 11.0 | [2560/8000 (32%)]	Loss: 5.134198                         LR: 0.0003  Feat: 0.985 Epoch Time: 7.655 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 11.0 | [5120/8000 (65%)]	Loss: 5.132582                         LR: 0.0003  Feat: 0.985 Epoch Time: 9.136 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 11.0 | [7680/8000 (97%)]	Loss: 5.101814                         LR: 0.0003  Feat: 0.987 Epoch Time: 10.617 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 12.0 | [0/8000 (0%)]	Loss: 5.179595                         LR: 0.0003  Feat: 0.985 Epoch Time: 11.482 Model Time: 0.056 Data Time: 0.672 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 12.0 | [2560/8000 (32%)]	Loss: 5.062671                         LR: 0.0003  Feat: 0.984 Epoch Time: 12.969 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 12.0 | [5120/8000 (65%)]	Loss: 5.097135                         LR: 0.0003  Feat: 0.984 Epoch Time: 14.451 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 12.0 | [7680/8000 (97%)]	Loss: 5.123507                         LR: 0.0003  Feat: 0.984 Epoch Time: 15.932 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 13.0 | [0/8000 (0%)]	Loss: 5.080568                         LR: 0.0003  Feat: 0.984 Epoch Time: 16.812 Model Time: 0.055 Data Time: 0.685 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 13.0 | [2560/8000 (32%)]	Loss: 5.113011                         LR: 0.0003  Feat: 0.984 Epoch Time: 18.294 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 13.0 | [5120/8000 (65%)]	Loss: 5.110386                         LR: 0.0003  Feat: 0.984 Epoch Time: 19.775 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 13.0 | [7680/8000 (97%)]	Loss: 5.124894                         LR: 0.0003  Feat: 0.983 Epoch Time: 21.255 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 14.0 | [0/8000 (0%)]	Loss: 5.064738                         LR: 0.0003  Feat: 0.980 Epoch Time: 22.134 Model Time: 0.054 Data Time: 0.687 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 14.0 | [2560/8000 (32%)]	Loss: 5.065976                         LR: 0.0003  Feat: 0.985 Epoch Time: 23.614 Model Time: 0.051 Data Time: 0.094 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 14.0 | [5120/8000 (65%)]	Loss: 5.091619                         LR: 0.0003  Feat: 0.984 Epoch Time: 25.096 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 2 | Local Epoch : 14.0 | [7680/8000 (97%)]	Loss: 5.070902                         LR: 0.0003  Feat: 0.984 Epoch Time: 26.574 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 10.0 | [0/8000 (0%)]	Loss: 5.158966                         LR: 0.0003  Feat: 0.985 Epoch Time: 0.864 Model Time: 0.055 Data Time: 0.714 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 10.0 | [2560/8000 (32%)]	Loss: 5.216392                         LR: 0.0003  Feat: 0.988 Epoch Time: 2.351 Model Time: 0.056 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 10.0 | [5120/8000 (65%)]	Loss: 5.134933                         LR: 0.0003  Feat: 0.981 Epoch Time: 3.835 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 10.0 | [7680/8000 (97%)]	Loss: 5.153580                         LR: 0.0003  Feat: 0.983 Epoch Time: 5.316 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 11.0 | [0/8000 (0%)]	Loss: 5.188213                         LR: 0.0003  Feat: 0.981 Epoch Time: 6.341 Model Time: 0.056 Data Time: 0.816 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 11.0 | [2560/8000 (32%)]	Loss: 5.122789                         LR: 0.0003  Feat: 0.987 Epoch Time: 7.824 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 11.0 | [5120/8000 (65%)]	Loss: 5.161376                         LR: 0.0003  Feat: 0.986 Epoch Time: 9.305 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 11.0 | [7680/8000 (97%)]	Loss: 5.080121                         LR: 0.0003  Feat: 0.985 Epoch Time: 10.784 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 12.0 | [0/8000 (0%)]	Loss: 5.099105                         LR: 0.0003  Feat: 0.984 Epoch Time: 11.783 Model Time: 0.058 Data Time: 0.798 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 12.0 | [2560/8000 (32%)]	Loss: 5.116479                         LR: 0.0003  Feat: 0.987 Epoch Time: 13.269 Model Time: 0.053 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 12.0 | [5120/8000 (65%)]	Loss: 5.107402                         LR: 0.0003  Feat: 0.985 Epoch Time: 14.752 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 12.0 | [7680/8000 (97%)]	Loss: 5.019082                         LR: 0.0003  Feat: 0.984 Epoch Time: 16.233 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 13.0 | [0/8000 (0%)]	Loss: 5.126088                         LR: 0.0003  Feat: 0.984 Epoch Time: 17.139 Model Time: 0.056 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 13.0 | [2560/8000 (32%)]	Loss: 5.121035                         LR: 0.0003  Feat: 0.983 Epoch Time: 18.620 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 13.0 | [5120/8000 (65%)]	Loss: 5.114958                         LR: 0.0003  Feat: 0.987 Epoch Time: 20.100 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 13.0 | [7680/8000 (97%)]	Loss: 5.116693                         LR: 0.0003  Feat: 0.984 Epoch Time: 21.581 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 14.0 | [0/8000 (0%)]	Loss: 5.095006                         LR: 0.0003  Feat: 0.983 Epoch Time: 22.478 Model Time: 0.055 Data Time: 0.703 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 14.0 | [2560/8000 (32%)]	Loss: 5.144448                         LR: 0.0003  Feat: 0.985 Epoch Time: 23.958 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 14.0 | [5120/8000 (65%)]	Loss: 5.086922                         LR: 0.0003  Feat: 0.989 Epoch Time: 25.437 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 2 | Local Epoch : 14.0 | [7680/8000 (97%)]	Loss: 5.061455                         LR: 0.0003  Feat: 0.986 Epoch Time: 26.916 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 10.0 | [0/8000 (0%)]	Loss: 5.126731                         LR: 0.0003  Feat: 0.985 Epoch Time: 0.840 Model Time: 0.058 Data Time: 0.690 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 10.0 | [2560/8000 (32%)]	Loss: 5.227162                         LR: 0.0003  Feat: 0.983 Epoch Time: 2.321 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 10.0 | [5120/8000 (65%)]	Loss: 5.087697                         LR: 0.0003  Feat: 0.980 Epoch Time: 3.800 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 10.0 | [7680/8000 (97%)]	Loss: 5.200465                         LR: 0.0003  Feat: 0.985 Epoch Time: 5.281 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 11.0 | [0/8000 (0%)]	Loss: 5.165542                         LR: 0.0003  Feat: 0.984 Epoch Time: 6.173 Model Time: 0.056 Data Time: 0.697 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 11.0 | [2560/8000 (32%)]	Loss: 5.143521                         LR: 0.0003  Feat: 0.981 Epoch Time: 7.654 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 11.0 | [5120/8000 (65%)]	Loss: 5.134443                         LR: 0.0003  Feat: 0.985 Epoch Time: 9.135 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 11.0 | [7680/8000 (97%)]	Loss: 5.131721                         LR: 0.0003  Feat: 0.978 Epoch Time: 10.615 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 12.0 | [0/8000 (0%)]	Loss: 5.087859                         LR: 0.0003  Feat: 0.980 Epoch Time: 11.518 Model Time: 0.054 Data Time: 0.712 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 12.0 | [2560/8000 (32%)]	Loss: 5.144117                         LR: 0.0003  Feat: 0.984 Epoch Time: 12.998 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 12.0 | [5120/8000 (65%)]	Loss: 5.073664                         LR: 0.0003  Feat: 0.989 Epoch Time: 14.478 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 12.0 | [7680/8000 (97%)]	Loss: 5.128792                         LR: 0.0003  Feat: 0.985 Epoch Time: 15.958 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 13.0 | [0/8000 (0%)]	Loss: 5.111006                         LR: 0.0003  Feat: 0.984 Epoch Time: 16.858 Model Time: 0.055 Data Time: 0.706 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 13.0 | [2560/8000 (32%)]	Loss: 5.116990                         LR: 0.0003  Feat: 0.982 Epoch Time: 18.341 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 13.0 | [5120/8000 (65%)]	Loss: 5.148790                         LR: 0.0003  Feat: 0.983 Epoch Time: 19.821 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 13.0 | [7680/8000 (97%)]	Loss: 5.104512                         LR: 0.0003  Feat: 0.984 Epoch Time: 21.300 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 14.0 | [0/8000 (0%)]	Loss: 5.064329                         LR: 0.0003  Feat: 0.982 Epoch Time: 22.201 Model Time: 0.059 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 14.0 | [2560/8000 (32%)]	Loss: 5.153600                         LR: 0.0003  Feat: 0.980 Epoch Time: 23.682 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 14.0 | [5120/8000 (65%)]	Loss: 5.121278                         LR: 0.0003  Feat: 0.988 Epoch Time: 25.162 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 2 | Local Epoch : 14.0 | [7680/8000 (97%)]	Loss: 5.097245                         LR: 0.0003  Feat: 0.979 Epoch Time: 26.642 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 10.0 | [0/8000 (0%)]	Loss: 5.066434                         LR: 0.0003  Feat: 0.981 Epoch Time: 0.840 Model Time: 0.057 Data Time: 0.690 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 10.0 | [2560/8000 (32%)]	Loss: 5.179441                         LR: 0.0003  Feat: 0.984 Epoch Time: 2.322 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 10.0 | [5120/8000 (65%)]	Loss: 5.111026                         LR: 0.0003  Feat: 0.983 Epoch Time: 3.802 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 10.0 | [7680/8000 (97%)]	Loss: 5.182364                         LR: 0.0003  Feat: 0.982 Epoch Time: 5.282 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 11.0 | [0/8000 (0%)]	Loss: 5.178267                         LR: 0.0003  Feat: 0.981 Epoch Time: 6.159 Model Time: 0.055 Data Time: 0.685 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 11.0 | [2560/8000 (32%)]	Loss: 5.180856                         LR: 0.0003  Feat: 0.984 Epoch Time: 7.646 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 11.0 | [5120/8000 (65%)]	Loss: 5.112036                         LR: 0.0003  Feat: 0.981 Epoch Time: 9.126 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 11.0 | [7680/8000 (97%)]	Loss: 5.094481                         LR: 0.0003  Feat: 0.981 Epoch Time: 10.605 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 12.0 | [0/8000 (0%)]	Loss: 5.115602                         LR: 0.0003  Feat: 0.983 Epoch Time: 11.470 Model Time: 0.057 Data Time: 0.670 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 12.0 | [2560/8000 (32%)]	Loss: 5.108533                         LR: 0.0003  Feat: 0.984 Epoch Time: 12.953 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 12.0 | [5120/8000 (65%)]	Loss: 5.120546                         LR: 0.0003  Feat: 0.984 Epoch Time: 14.433 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 12.0 | [7680/8000 (97%)]	Loss: 5.125237                         LR: 0.0003  Feat: 0.984 Epoch Time: 16.035 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 13.0 | [0/8000 (0%)]	Loss: 5.091363                         LR: 0.0003  Feat: 0.985 Epoch Time: 16.906 Model Time: 0.059 Data Time: 0.679 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 13.0 | [2560/8000 (32%)]	Loss: 5.083917                         LR: 0.0003  Feat: 0.992 Epoch Time: 18.385 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 13.0 | [5120/8000 (65%)]	Loss: 5.066767                         LR: 0.0003  Feat: 0.987 Epoch Time: 19.864 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 13.0 | [7680/8000 (97%)]	Loss: 5.124295                         LR: 0.0003  Feat: 0.988 Epoch Time: 21.344 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 14.0 | [0/8000 (0%)]	Loss: 5.087138                         LR: 0.0003  Feat: 0.988 Epoch Time: 22.212 Model Time: 0.058 Data Time: 0.673 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 14.0 | [2560/8000 (32%)]	Loss: 5.099893                         LR: 0.0003  Feat: 0.984 Epoch Time: 23.698 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 14.0 | [5120/8000 (65%)]	Loss: 5.123955                         LR: 0.0003  Feat: 0.982 Epoch Time: 25.180 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 2 | Local Epoch : 14.0 | [7680/8000 (97%)]	Loss: 5.108353                         LR: 0.0003  Feat: 0.982 Epoch Time: 26.662 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Federated averaging

 | Global Training Round : 4 | Model : 05-18_04:53_3082253

Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 15.0 | [0/8000 (0%)]	Loss: 5.108836                         LR: 0.0001  Feat: 0.986 Epoch Time: 0.827 Model Time: 0.057 Data Time: 0.677 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 15.0 | [2560/8000 (32%)]	Loss: 5.059277                         LR: 0.0001  Feat: 0.988 Epoch Time: 2.310 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 15.0 | [5120/8000 (65%)]	Loss: 5.057054                         LR: 0.0001  Feat: 0.985 Epoch Time: 3.792 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 15.0 | [7680/8000 (97%)]	Loss: 5.038153                         LR: 0.0001  Feat: 0.985 Epoch Time: 5.273 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 16.0 | [0/8000 (0%)]	Loss: 5.096203                         LR: 0.0001  Feat: 0.985 Epoch Time: 6.174 Model Time: 0.055 Data Time: 0.703 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 16.0 | [2560/8000 (32%)]	Loss: 5.074285                         LR: 0.0001  Feat: 0.984 Epoch Time: 7.658 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 16.0 | [5120/8000 (65%)]	Loss: 5.083234                         LR: 0.0001  Feat: 0.985 Epoch Time: 9.138 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 16.0 | [7680/8000 (97%)]	Loss: 5.027933                         LR: 0.0001  Feat: 0.983 Epoch Time: 10.619 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 17.0 | [0/8000 (0%)]	Loss: 5.067615                         LR: 0.0001  Feat: 0.984 Epoch Time: 11.500 Model Time: 0.061 Data Time: 0.684 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 17.0 | [2560/8000 (32%)]	Loss: 5.011475                         LR: 0.0001  Feat: 0.983 Epoch Time: 12.982 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 17.0 | [5120/8000 (65%)]	Loss: 5.043190                         LR: 0.0001  Feat: 0.983 Epoch Time: 14.463 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 17.0 | [7680/8000 (97%)]	Loss: 5.044413                         LR: 0.0001  Feat: 0.984 Epoch Time: 15.944 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 18.0 | [0/8000 (0%)]	Loss: 5.052914                         LR: 0.0001  Feat: 0.986 Epoch Time: 16.827 Model Time: 0.057 Data Time: 0.682 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 18.0 | [2560/8000 (32%)]	Loss: 5.045825                         LR: 0.0001  Feat: 0.983 Epoch Time: 18.309 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 18.0 | [5120/8000 (65%)]	Loss: 5.025841                         LR: 0.0001  Feat: 0.983 Epoch Time: 19.790 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 18.0 | [7680/8000 (97%)]	Loss: 5.004614                         LR: 0.0001  Feat: 0.980 Epoch Time: 21.270 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 19.0 | [0/8000 (0%)]	Loss: 4.985129                         LR: 0.0001  Feat: 0.979 Epoch Time: 22.407 Model Time: 0.057 Data Time: 0.914 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 19.0 | [2560/8000 (32%)]	Loss: 5.045799                         LR: 0.0001  Feat: 0.980 Epoch Time: 23.891 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 19.0 | [5120/8000 (65%)]	Loss: 5.059793                         LR: 0.0001  Feat: 0.981 Epoch Time: 25.370 Model Time: 0.050 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 3 | Local Epoch : 19.0 | [7680/8000 (97%)]	Loss: 5.084051                         LR: 0.0001  Feat: 0.981 Epoch Time: 26.850 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 15.0 | [0/8000 (0%)]	Loss: 5.050998                         LR: 0.0001  Feat: 0.985 Epoch Time: 0.875 Model Time: 0.056 Data Time: 0.726 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 15.0 | [2560/8000 (32%)]	Loss: 5.098763                         LR: 0.0001  Feat: 0.984 Epoch Time: 2.356 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 15.0 | [5120/8000 (65%)]	Loss: 5.108822                         LR: 0.0001  Feat: 0.985 Epoch Time: 3.834 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 15.0 | [7680/8000 (97%)]	Loss: 5.077480                         LR: 0.0001  Feat: 0.985 Epoch Time: 5.312 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 16.0 | [0/8000 (0%)]	Loss: 5.055012                         LR: 0.0001  Feat: 0.983 Epoch Time: 6.366 Model Time: 0.056 Data Time: 0.845 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 16.0 | [2560/8000 (32%)]	Loss: 5.043247                         LR: 0.0001  Feat: 0.984 Epoch Time: 7.846 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 16.0 | [5120/8000 (65%)]	Loss: 5.018472                         LR: 0.0001  Feat: 0.983 Epoch Time: 9.325 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 16.0 | [7680/8000 (97%)]	Loss: 5.042044                         LR: 0.0001  Feat: 0.982 Epoch Time: 10.805 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 17.0 | [0/8000 (0%)]	Loss: 5.019695                         LR: 0.0001  Feat: 0.983 Epoch Time: 11.680 Model Time: 0.058 Data Time: 0.677 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 17.0 | [2560/8000 (32%)]	Loss: 5.110849                         LR: 0.0001  Feat: 0.983 Epoch Time: 13.162 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 17.0 | [5120/8000 (65%)]	Loss: 5.092742                         LR: 0.0001  Feat: 0.983 Epoch Time: 14.642 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 17.0 | [7680/8000 (97%)]	Loss: 5.065451                         LR: 0.0001  Feat: 0.981 Epoch Time: 16.125 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 18.0 | [0/8000 (0%)]	Loss: 5.058900                         LR: 0.0001  Feat: 0.979 Epoch Time: 17.012 Model Time: 0.059 Data Time: 0.688 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 18.0 | [2560/8000 (32%)]	Loss: 5.064864                         LR: 0.0001  Feat: 0.981 Epoch Time: 18.497 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 18.0 | [5120/8000 (65%)]	Loss: 5.019310                         LR: 0.0001  Feat: 0.980 Epoch Time: 19.977 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 18.0 | [7680/8000 (97%)]	Loss: 4.990741                         LR: 0.0001  Feat: 0.983 Epoch Time: 21.456 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 19.0 | [0/8000 (0%)]	Loss: 5.013731                         LR: 0.0001  Feat: 0.980 Epoch Time: 22.506 Model Time: 0.055 Data Time: 0.843 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 19.0 | [2560/8000 (32%)]	Loss: 5.040688                         LR: 0.0001  Feat: 0.980 Epoch Time: 23.987 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 19.0 | [5120/8000 (65%)]	Loss: 5.018892                         LR: 0.0001  Feat: 0.982 Epoch Time: 25.466 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 3 | Local Epoch : 19.0 | [7680/8000 (97%)]	Loss: 5.011588                         LR: 0.0001  Feat: 0.979 Epoch Time: 26.946 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 15.0 | [0/8000 (0%)]	Loss: 5.055453                         LR: 0.0001  Feat: 0.987 Epoch Time: 0.850 Model Time: 0.057 Data Time: 0.699 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 15.0 | [2560/8000 (32%)]	Loss: 5.118411                         LR: 0.0001  Feat: 0.983 Epoch Time: 2.328 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 15.0 | [5120/8000 (65%)]	Loss: 5.092273                         LR: 0.0001  Feat: 0.988 Epoch Time: 3.806 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 15.0 | [7680/8000 (97%)]	Loss: 5.089453                         LR: 0.0001  Feat: 0.985 Epoch Time: 5.284 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 16.0 | [0/8000 (0%)]	Loss: 5.094786                         LR: 0.0001  Feat: 0.985 Epoch Time: 6.178 Model Time: 0.055 Data Time: 0.698 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 16.0 | [2560/8000 (32%)]	Loss: 5.016031                         LR: 0.0001  Feat: 0.988 Epoch Time: 7.661 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 16.0 | [5120/8000 (65%)]	Loss: 5.053337                         LR: 0.0001  Feat: 0.982 Epoch Time: 9.139 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 16.0 | [7680/8000 (97%)]	Loss: 4.985948                         LR: 0.0001  Feat: 0.980 Epoch Time: 10.617 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 17.0 | [0/8000 (0%)]	Loss: 5.084549                         LR: 0.0001  Feat: 0.981 Epoch Time: 11.504 Model Time: 0.056 Data Time: 0.692 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 17.0 | [2560/8000 (32%)]	Loss: 4.991328                         LR: 0.0001  Feat: 0.985 Epoch Time: 12.988 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 17.0 | [5120/8000 (65%)]	Loss: 4.995338                         LR: 0.0001  Feat: 0.984 Epoch Time: 14.466 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 17.0 | [7680/8000 (97%)]	Loss: 5.051321                         LR: 0.0001  Feat: 0.978 Epoch Time: 15.944 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 18.0 | [0/8000 (0%)]	Loss: 4.975437                         LR: 0.0001  Feat: 0.981 Epoch Time: 16.835 Model Time: 0.057 Data Time: 0.692 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 18.0 | [2560/8000 (32%)]	Loss: 5.090340                         LR: 0.0001  Feat: 0.983 Epoch Time: 18.314 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 18.0 | [5120/8000 (65%)]	Loss: 4.979352                         LR: 0.0001  Feat: 0.985 Epoch Time: 19.794 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 18.0 | [7680/8000 (97%)]	Loss: 5.059629                         LR: 0.0001  Feat: 0.980 Epoch Time: 21.272 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 19.0 | [0/8000 (0%)]	Loss: 4.981189                         LR: 0.0001  Feat: 0.981 Epoch Time: 22.172 Model Time: 0.060 Data Time: 0.703 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 19.0 | [2560/8000 (32%)]	Loss: 5.078384                         LR: 0.0001  Feat: 0.981 Epoch Time: 23.651 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 19.0 | [5120/8000 (65%)]	Loss: 5.056882                         LR: 0.0001  Feat: 0.979 Epoch Time: 25.133 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 3 | Local Epoch : 19.0 | [7680/8000 (97%)]	Loss: 5.045713                         LR: 0.0001  Feat: 0.980 Epoch Time: 26.612 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 15.0 | [0/8000 (0%)]	Loss: 5.042678                         LR: 0.0001  Feat: 0.986 Epoch Time: 0.842 Model Time: 0.056 Data Time: 0.692 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 15.0 | [2560/8000 (32%)]	Loss: 5.064789                         LR: 0.0001  Feat: 0.983 Epoch Time: 2.322 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 15.0 | [5120/8000 (65%)]	Loss: 5.090087                         LR: 0.0001  Feat: 0.985 Epoch Time: 3.802 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 15.0 | [7680/8000 (97%)]	Loss: 5.075439                         LR: 0.0001  Feat: 0.981 Epoch Time: 5.281 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 16.0 | [0/8000 (0%)]	Loss: 5.072859                         LR: 0.0001  Feat: 0.985 Epoch Time: 6.173 Model Time: 0.054 Data Time: 0.698 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 16.0 | [2560/8000 (32%)]	Loss: 5.071104                         LR: 0.0001  Feat: 0.984 Epoch Time: 7.654 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 16.0 | [5120/8000 (65%)]	Loss: 5.023020                         LR: 0.0001  Feat: 0.984 Epoch Time: 9.132 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 16.0 | [7680/8000 (97%)]	Loss: 5.008834                         LR: 0.0001  Feat: 0.982 Epoch Time: 10.611 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 17.0 | [0/8000 (0%)]	Loss: 5.058908                         LR: 0.0001  Feat: 0.982 Epoch Time: 11.490 Model Time: 0.059 Data Time: 0.685 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 17.0 | [2560/8000 (32%)]	Loss: 4.994708                         LR: 0.0001  Feat: 0.984 Epoch Time: 12.972 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 17.0 | [5120/8000 (65%)]	Loss: 5.017436                         LR: 0.0001  Feat: 0.982 Epoch Time: 14.451 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 17.0 | [7680/8000 (97%)]	Loss: 5.049766                         LR: 0.0001  Feat: 0.984 Epoch Time: 15.931 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 18.0 | [0/8000 (0%)]	Loss: 5.017652                         LR: 0.0001  Feat: 0.980 Epoch Time: 16.973 Model Time: 0.057 Data Time: 0.835 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 18.0 | [2560/8000 (32%)]	Loss: 5.012486                         LR: 0.0001  Feat: 0.984 Epoch Time: 18.453 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 18.0 | [5120/8000 (65%)]	Loss: 5.055029                         LR: 0.0001  Feat: 0.978 Epoch Time: 19.933 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 18.0 | [7680/8000 (97%)]	Loss: 5.045739                         LR: 0.0001  Feat: 0.979 Epoch Time: 21.412 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 19.0 | [0/8000 (0%)]	Loss: 5.008480                         LR: 0.0001  Feat: 0.981 Epoch Time: 22.318 Model Time: 0.055 Data Time: 0.710 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 19.0 | [2560/8000 (32%)]	Loss: 5.077446                         LR: 0.0001  Feat: 0.980 Epoch Time: 23.800 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 19.0 | [5120/8000 (65%)]	Loss: 5.108568                         LR: 0.0001  Feat: 0.979 Epoch Time: 25.278 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 3 | Local Epoch : 19.0 | [7680/8000 (97%)]	Loss: 5.015046                         LR: 0.0001  Feat: 0.980 Epoch Time: 26.758 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 15.0 | [0/8000 (0%)]	Loss: 5.108261                         LR: 0.0001  Feat: 0.988 Epoch Time: 0.834 Model Time: 0.057 Data Time: 0.684 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 15.0 | [2560/8000 (32%)]	Loss: 5.085569                         LR: 0.0001  Feat: 0.986 Epoch Time: 2.316 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 15.0 | [5120/8000 (65%)]	Loss: 5.045776                         LR: 0.0001  Feat: 0.989 Epoch Time: 3.795 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 15.0 | [7680/8000 (97%)]	Loss: 5.073112                         LR: 0.0001  Feat: 0.983 Epoch Time: 5.275 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 16.0 | [0/8000 (0%)]	Loss: 5.063585                         LR: 0.0001  Feat: 0.985 Epoch Time: 6.154 Model Time: 0.056 Data Time: 0.682 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 16.0 | [2560/8000 (32%)]	Loss: 5.089056                         LR: 0.0001  Feat: 0.984 Epoch Time: 7.634 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 16.0 | [5120/8000 (65%)]	Loss: 5.058172                         LR: 0.0001  Feat: 0.986 Epoch Time: 9.113 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 16.0 | [7680/8000 (97%)]	Loss: 5.042263                         LR: 0.0001  Feat: 0.987 Epoch Time: 10.593 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 17.0 | [0/8000 (0%)]	Loss: 5.075931                         LR: 0.0001  Feat: 0.986 Epoch Time: 11.489 Model Time: 0.055 Data Time: 0.698 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 17.0 | [2560/8000 (32%)]	Loss: 5.045270                         LR: 0.0001  Feat: 0.985 Epoch Time: 12.970 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 17.0 | [5120/8000 (65%)]	Loss: 5.055840                         LR: 0.0001  Feat: 0.984 Epoch Time: 14.448 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 17.0 | [7680/8000 (97%)]	Loss: 5.059493                         LR: 0.0001  Feat: 0.983 Epoch Time: 15.927 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 18.0 | [0/8000 (0%)]	Loss: 5.039024                         LR: 0.0001  Feat: 0.981 Epoch Time: 16.977 Model Time: 0.060 Data Time: 0.842 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 18.0 | [2560/8000 (32%)]	Loss: 5.063366                         LR: 0.0001  Feat: 0.981 Epoch Time: 18.458 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 18.0 | [5120/8000 (65%)]	Loss: 4.991021                         LR: 0.0001  Feat: 0.985 Epoch Time: 19.937 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 18.0 | [7680/8000 (97%)]	Loss: 5.055171                         LR: 0.0001  Feat: 0.980 Epoch Time: 21.417 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 19.0 | [0/8000 (0%)]	Loss: 4.993458                         LR: 0.0001  Feat: 0.984 Epoch Time: 22.353 Model Time: 0.060 Data Time: 0.740 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 19.0 | [2560/8000 (32%)]	Loss: 5.053971                         LR: 0.0001  Feat: 0.982 Epoch Time: 23.836 Model Time: 0.053 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 19.0 | [5120/8000 (65%)]	Loss: 5.050428                         LR: 0.0001  Feat: 0.979 Epoch Time: 25.318 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 3 | Local Epoch : 19.0 | [7680/8000 (97%)]	Loss: 5.032892                         LR: 0.0001  Feat: 0.981 Epoch Time: 26.800 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Federated averaging

 | Global Training Round : 5 | Model : 05-18_04:53_3082253

Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 20.0 | [0/8000 (0%)]	Loss: 5.057316                         LR: 0.0001  Feat: 0.979 Epoch Time: 0.877 Model Time: 0.056 Data Time: 0.728 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 20.0 | [2560/8000 (32%)]	Loss: 5.007823                         LR: 0.0001  Feat: 0.981 Epoch Time: 2.361 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 20.0 | [5120/8000 (65%)]	Loss: 5.039266                         LR: 0.0001  Feat: 0.979 Epoch Time: 3.844 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 20.0 | [7680/8000 (97%)]	Loss: 5.016882                         LR: 0.0001  Feat: 0.978 Epoch Time: 5.326 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 21.0 | [0/8000 (0%)]	Loss: 5.046694                         LR: 0.0001  Feat: 0.977 Epoch Time: 6.225 Model Time: 0.061 Data Time: 0.698 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 21.0 | [2560/8000 (32%)]	Loss: 5.033854                         LR: 0.0001  Feat: 0.979 Epoch Time: 7.707 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 21.0 | [5120/8000 (65%)]	Loss: 5.015347                         LR: 0.0001  Feat: 0.978 Epoch Time: 9.189 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 21.0 | [7680/8000 (97%)]	Loss: 5.054579                         LR: 0.0001  Feat: 0.977 Epoch Time: 10.669 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 22.0 | [0/8000 (0%)]	Loss: 5.033525                         LR: 0.0001  Feat: 0.975 Epoch Time: 11.574 Model Time: 0.055 Data Time: 0.707 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 22.0 | [2560/8000 (32%)]	Loss: 4.984304                         LR: 0.0001  Feat: 0.976 Epoch Time: 13.058 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 22.0 | [5120/8000 (65%)]	Loss: 5.057729                         LR: 0.0001  Feat: 0.975 Epoch Time: 14.537 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 22.0 | [7680/8000 (97%)]	Loss: 4.991403                         LR: 0.0001  Feat: 0.977 Epoch Time: 16.017 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 23.0 | [0/8000 (0%)]	Loss: 5.031756                         LR: 0.0001  Feat: 0.977 Epoch Time: 16.922 Model Time: 0.055 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 23.0 | [2560/8000 (32%)]	Loss: 5.092365                         LR: 0.0001  Feat: 0.975 Epoch Time: 18.403 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 23.0 | [5120/8000 (65%)]	Loss: 4.997711                         LR: 0.0001  Feat: 0.974 Epoch Time: 19.883 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 23.0 | [7680/8000 (97%)]	Loss: 5.001473                         LR: 0.0001  Feat: 0.972 Epoch Time: 21.363 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 24.0 | [0/8000 (0%)]	Loss: 4.965761                         LR: 0.0001  Feat: 0.972 Epoch Time: 22.390 Model Time: 0.056 Data Time: 0.820 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 24.0 | [2560/8000 (32%)]	Loss: 5.006395                         LR: 0.0001  Feat: 0.971 Epoch Time: 23.871 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 24.0 | [5120/8000 (65%)]	Loss: 5.008671                         LR: 0.0001  Feat: 0.974 Epoch Time: 25.349 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 4 | Local Epoch : 24.0 | [7680/8000 (97%)]	Loss: 5.021949                         LR: 0.0001  Feat: 0.974 Epoch Time: 26.829 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 20.0 | [0/8000 (0%)]	Loss: 5.108862                         LR: 0.0001  Feat: 0.980 Epoch Time: 0.944 Model Time: 0.057 Data Time: 0.793 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 20.0 | [2560/8000 (32%)]	Loss: 5.104996                         LR: 0.0001  Feat: 0.980 Epoch Time: 2.425 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 20.0 | [5120/8000 (65%)]	Loss: 5.080025                         LR: 0.0001  Feat: 0.976 Epoch Time: 3.903 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 20.0 | [7680/8000 (97%)]	Loss: 5.004733                         LR: 0.0001  Feat: 0.976 Epoch Time: 5.383 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 21.0 | [0/8000 (0%)]	Loss: 5.032834                         LR: 0.0001  Feat: 0.978 Epoch Time: 6.345 Model Time: 0.056 Data Time: 0.763 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 21.0 | [2560/8000 (32%)]	Loss: 5.005107                         LR: 0.0001  Feat: 0.978 Epoch Time: 7.824 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 21.0 | [5120/8000 (65%)]	Loss: 5.059920                         LR: 0.0001  Feat: 0.976 Epoch Time: 9.303 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 21.0 | [7680/8000 (97%)]	Loss: 4.985177                         LR: 0.0001  Feat: 0.976 Epoch Time: 10.781 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 22.0 | [0/8000 (0%)]	Loss: 5.031845                         LR: 0.0001  Feat: 0.977 Epoch Time: 11.723 Model Time: 0.056 Data Time: 0.741 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 22.0 | [2560/8000 (32%)]	Loss: 4.993184                         LR: 0.0001  Feat: 0.975 Epoch Time: 13.203 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 22.0 | [5120/8000 (65%)]	Loss: 5.003396                         LR: 0.0001  Feat: 0.972 Epoch Time: 14.682 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 22.0 | [7680/8000 (97%)]	Loss: 5.004263                         LR: 0.0001  Feat: 0.974 Epoch Time: 16.162 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 23.0 | [0/8000 (0%)]	Loss: 4.993680                         LR: 0.0001  Feat: 0.977 Epoch Time: 17.095 Model Time: 0.055 Data Time: 0.739 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 23.0 | [2560/8000 (32%)]	Loss: 5.016834                         LR: 0.0001  Feat: 0.978 Epoch Time: 18.575 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 23.0 | [5120/8000 (65%)]	Loss: 5.007985                         LR: 0.0001  Feat: 0.978 Epoch Time: 20.056 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 23.0 | [7680/8000 (97%)]	Loss: 4.980870                         LR: 0.0001  Feat: 0.977 Epoch Time: 21.536 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 24.0 | [0/8000 (0%)]	Loss: 4.972904                         LR: 0.0001  Feat: 0.978 Epoch Time: 22.423 Model Time: 0.058 Data Time: 0.693 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 24.0 | [2560/8000 (32%)]	Loss: 4.996377                         LR: 0.0001  Feat: 0.973 Epoch Time: 23.905 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 24.0 | [5120/8000 (65%)]	Loss: 5.019569                         LR: 0.0001  Feat: 0.973 Epoch Time: 25.386 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 4 | Local Epoch : 24.0 | [7680/8000 (97%)]	Loss: 4.980283                         LR: 0.0001  Feat: 0.975 Epoch Time: 26.872 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 20.0 | [0/8000 (0%)]	Loss: 5.060636                         LR: 0.0001  Feat: 0.981 Epoch Time: 0.883 Model Time: 0.057 Data Time: 0.733 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 20.0 | [2560/8000 (32%)]	Loss: 5.086874                         LR: 0.0001  Feat: 0.977 Epoch Time: 2.365 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 20.0 | [5120/8000 (65%)]	Loss: 5.019394                         LR: 0.0001  Feat: 0.978 Epoch Time: 3.843 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 20.0 | [7680/8000 (97%)]	Loss: 5.028506                         LR: 0.0001  Feat: 0.978 Epoch Time: 5.322 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 21.0 | [0/8000 (0%)]	Loss: 5.042636                         LR: 0.0001  Feat: 0.978 Epoch Time: 6.218 Model Time: 0.056 Data Time: 0.699 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 21.0 | [2560/8000 (32%)]	Loss: 5.005604                         LR: 0.0001  Feat: 0.976 Epoch Time: 7.698 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 21.0 | [5120/8000 (65%)]	Loss: 5.019657                         LR: 0.0001  Feat: 0.978 Epoch Time: 9.178 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 21.0 | [7680/8000 (97%)]	Loss: 5.060541                         LR: 0.0001  Feat: 0.977 Epoch Time: 10.672 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 22.0 | [0/8000 (0%)]	Loss: 5.023421                         LR: 0.0001  Feat: 0.978 Epoch Time: 11.588 Model Time: 0.054 Data Time: 0.724 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 22.0 | [2560/8000 (32%)]	Loss: 5.037762                         LR: 0.0001  Feat: 0.977 Epoch Time: 13.068 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 22.0 | [5120/8000 (65%)]	Loss: 5.027523                         LR: 0.0001  Feat: 0.977 Epoch Time: 14.547 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 22.0 | [7680/8000 (97%)]	Loss: 5.080549                         LR: 0.0001  Feat: 0.974 Epoch Time: 16.026 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 23.0 | [0/8000 (0%)]	Loss: 5.049328                         LR: 0.0001  Feat: 0.974 Epoch Time: 16.910 Model Time: 0.059 Data Time: 0.685 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 23.0 | [2560/8000 (32%)]	Loss: 5.042915                         LR: 0.0001  Feat: 0.975 Epoch Time: 18.392 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 23.0 | [5120/8000 (65%)]	Loss: 5.041150                         LR: 0.0001  Feat: 0.975 Epoch Time: 19.870 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 23.0 | [7680/8000 (97%)]	Loss: 5.008023                         LR: 0.0001  Feat: 0.975 Epoch Time: 21.348 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 24.0 | [0/8000 (0%)]	Loss: 5.017980                         LR: 0.0001  Feat: 0.974 Epoch Time: 22.233 Model Time: 0.057 Data Time: 0.689 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 24.0 | [2560/8000 (32%)]	Loss: 5.013248                         LR: 0.0001  Feat: 0.976 Epoch Time: 23.722 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 24.0 | [5120/8000 (65%)]	Loss: 5.057881                         LR: 0.0001  Feat: 0.971 Epoch Time: 25.209 Model Time: 0.053 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 4 | Local Epoch : 24.0 | [7680/8000 (97%)]	Loss: 4.961310                         LR: 0.0001  Feat: 0.973 Epoch Time: 26.690 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 20.0 | [0/8000 (0%)]	Loss: 5.019411                         LR: 0.0001  Feat: 0.982 Epoch Time: 0.841 Model Time: 0.057 Data Time: 0.691 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 20.0 | [2560/8000 (32%)]	Loss: 5.046898                         LR: 0.0001  Feat: 0.983 Epoch Time: 2.321 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 20.0 | [5120/8000 (65%)]	Loss: 5.065982                         LR: 0.0001  Feat: 0.982 Epoch Time: 3.801 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 20.0 | [7680/8000 (97%)]	Loss: 5.054523                         LR: 0.0001  Feat: 0.984 Epoch Time: 5.285 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 21.0 | [0/8000 (0%)]	Loss: 5.029922                         LR: 0.0001  Feat: 0.977 Epoch Time: 6.175 Model Time: 0.055 Data Time: 0.696 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 21.0 | [2560/8000 (32%)]	Loss: 5.012935                         LR: 0.0001  Feat: 0.977 Epoch Time: 7.656 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 21.0 | [5120/8000 (65%)]	Loss: 5.045428                         LR: 0.0001  Feat: 0.979 Epoch Time: 9.138 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 21.0 | [7680/8000 (97%)]	Loss: 4.997774                         LR: 0.0001  Feat: 0.977 Epoch Time: 10.621 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 22.0 | [0/8000 (0%)]	Loss: 5.014562                         LR: 0.0001  Feat: 0.979 Epoch Time: 11.500 Model Time: 0.056 Data Time: 0.681 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 22.0 | [2560/8000 (32%)]	Loss: 5.089720                         LR: 0.0001  Feat: 0.976 Epoch Time: 12.981 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 22.0 | [5120/8000 (65%)]	Loss: 5.023807                         LR: 0.0001  Feat: 0.980 Epoch Time: 14.459 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 22.0 | [7680/8000 (97%)]	Loss: 4.983256                         LR: 0.0001  Feat: 0.982 Epoch Time: 15.937 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 23.0 | [0/8000 (0%)]	Loss: 4.995511                         LR: 0.0001  Feat: 0.981 Epoch Time: 16.849 Model Time: 0.055 Data Time: 0.716 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 23.0 | [2560/8000 (32%)]	Loss: 4.957165                         LR: 0.0001  Feat: 0.978 Epoch Time: 18.328 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 23.0 | [5120/8000 (65%)]	Loss: 5.060452                         LR: 0.0001  Feat: 0.977 Epoch Time: 19.808 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 23.0 | [7680/8000 (97%)]	Loss: 5.067917                         LR: 0.0001  Feat: 0.974 Epoch Time: 21.286 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 24.0 | [0/8000 (0%)]	Loss: 5.035845                         LR: 0.0001  Feat: 0.975 Epoch Time: 22.201 Model Time: 0.059 Data Time: 0.718 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 24.0 | [2560/8000 (32%)]	Loss: 5.016568                         LR: 0.0001  Feat: 0.972 Epoch Time: 23.682 Model Time: 0.050 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 24.0 | [5120/8000 (65%)]	Loss: 5.015759                         LR: 0.0001  Feat: 0.974 Epoch Time: 25.162 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 4 | Local Epoch : 24.0 | [7680/8000 (97%)]	Loss: 5.044978                         LR: 0.0001  Feat: 0.975 Epoch Time: 26.641 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 20.0 | [0/8000 (0%)]	Loss: 5.000339                         LR: 0.0001  Feat: 0.980 Epoch Time: 0.830 Model Time: 0.056 Data Time: 0.681 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 20.0 | [2560/8000 (32%)]	Loss: 5.021372                         LR: 0.0001  Feat: 0.981 Epoch Time: 2.312 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 20.0 | [5120/8000 (65%)]	Loss: 5.042536                         LR: 0.0001  Feat: 0.983 Epoch Time: 3.789 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 20.0 | [7680/8000 (97%)]	Loss: 4.985329                         LR: 0.0001  Feat: 0.978 Epoch Time: 5.269 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 21.0 | [0/8000 (0%)]	Loss: 5.079571                         LR: 0.0001  Feat: 0.979 Epoch Time: 6.178 Model Time: 0.054 Data Time: 0.715 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 21.0 | [2560/8000 (32%)]	Loss: 4.998612                         LR: 0.0001  Feat: 0.978 Epoch Time: 7.660 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 21.0 | [5120/8000 (65%)]	Loss: 5.002724                         LR: 0.0001  Feat: 0.979 Epoch Time: 9.139 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 21.0 | [7680/8000 (97%)]	Loss: 5.022292                         LR: 0.0001  Feat: 0.979 Epoch Time: 10.618 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 22.0 | [0/8000 (0%)]	Loss: 5.034211                         LR: 0.0001  Feat: 0.979 Epoch Time: 11.493 Model Time: 0.057 Data Time: 0.680 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 22.0 | [2560/8000 (32%)]	Loss: 5.023681                         LR: 0.0001  Feat: 0.977 Epoch Time: 12.974 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 22.0 | [5120/8000 (65%)]	Loss: 5.038807                         LR: 0.0001  Feat: 0.979 Epoch Time: 14.454 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 22.0 | [7680/8000 (97%)]	Loss: 5.060513                         LR: 0.0001  Feat: 0.978 Epoch Time: 15.933 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 23.0 | [0/8000 (0%)]	Loss: 4.996453                         LR: 0.0001  Feat: 0.978 Epoch Time: 16.848 Model Time: 0.055 Data Time: 0.717 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 23.0 | [2560/8000 (32%)]	Loss: 5.001119                         LR: 0.0001  Feat: 0.978 Epoch Time: 18.329 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 23.0 | [5120/8000 (65%)]	Loss: 5.037352                         LR: 0.0001  Feat: 0.975 Epoch Time: 19.808 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 23.0 | [7680/8000 (97%)]	Loss: 5.014027                         LR: 0.0001  Feat: 0.976 Epoch Time: 21.286 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 24.0 | [0/8000 (0%)]	Loss: 5.047537                         LR: 0.0001  Feat: 0.977 Epoch Time: 22.189 Model Time: 0.055 Data Time: 0.707 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 24.0 | [2560/8000 (32%)]	Loss: 5.079519                         LR: 0.0001  Feat: 0.975 Epoch Time: 23.670 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 24.0 | [5120/8000 (65%)]	Loss: 5.017397                         LR: 0.0001  Feat: 0.977 Epoch Time: 25.150 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 4 | Local Epoch : 24.0 | [7680/8000 (97%)]	Loss: 4.982291                         LR: 0.0001  Feat: 0.978 Epoch Time: 26.629 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Federated averaging

 | Global Training Round : 6 | Model : 05-18_04:53_3082253

Updating local model for agent:  1
Updating local model for agent:  1
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 25.0 | [0/8000 (0%)]	Loss: 4.977270                         LR: 0.0000  Feat: 0.976 Epoch Time: 0.857 Model Time: 0.056 Data Time: 0.708 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 25.0 | [2560/8000 (32%)]	Loss: 5.063711                         LR: 0.0000  Feat: 0.975 Epoch Time: 2.341 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 25.0 | [5120/8000 (65%)]	Loss: 5.023589                         LR: 0.0000  Feat: 0.973 Epoch Time: 3.821 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 25.0 | [7680/8000 (97%)]	Loss: 5.026040                         LR: 0.0000  Feat: 0.972 Epoch Time: 5.301 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 26.0 | [0/8000 (0%)]	Loss: 5.062784                         LR: 0.0000  Feat: 0.973 Epoch Time: 6.199 Model Time: 0.054 Data Time: 0.702 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 26.0 | [2560/8000 (32%)]	Loss: 4.960221                         LR: 0.0000  Feat: 0.972 Epoch Time: 7.681 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 26.0 | [5120/8000 (65%)]	Loss: 4.992563                         LR: 0.0000  Feat: 0.973 Epoch Time: 9.161 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 26.0 | [7680/8000 (97%)]	Loss: 5.052984                         LR: 0.0000  Feat: 0.972 Epoch Time: 10.640 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 27.0 | [0/8000 (0%)]	Loss: 4.960013                         LR: 0.0000  Feat: 0.974 Epoch Time: 11.533 Model Time: 0.056 Data Time: 0.696 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 27.0 | [2560/8000 (32%)]	Loss: 4.990335                         LR: 0.0000  Feat: 0.972 Epoch Time: 13.015 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 27.0 | [5120/8000 (65%)]	Loss: 5.042954                         LR: 0.0000  Feat: 0.974 Epoch Time: 14.495 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 27.0 | [7680/8000 (97%)]	Loss: 4.955921                         LR: 0.0000  Feat: 0.973 Epoch Time: 15.974 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 28.0 | [0/8000 (0%)]	Loss: 4.995483                         LR: 0.0000  Feat: 0.972 Epoch Time: 16.869 Model Time: 0.055 Data Time: 0.698 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 28.0 | [2560/8000 (32%)]	Loss: 4.991009                         LR: 0.0000  Feat: 0.972 Epoch Time: 18.351 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 28.0 | [5120/8000 (65%)]	Loss: 5.033637                         LR: 0.0000  Feat: 0.969 Epoch Time: 19.830 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 28.0 | [7680/8000 (97%)]	Loss: 4.985905                         LR: 0.0000  Feat: 0.971 Epoch Time: 21.311 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 29.0 | [0/8000 (0%)]	Loss: 5.021668                         LR: 0.0000  Feat: 0.968 Epoch Time: 22.361 Model Time: 0.062 Data Time: 0.833 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 29.0 | [2560/8000 (32%)]	Loss: 4.901909                         LR: 0.0000  Feat: 0.972 Epoch Time: 23.843 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 29.0 | [5120/8000 (65%)]	Loss: 5.013954                         LR: 0.0000  Feat: 0.972 Epoch Time: 25.324 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 1 | Global Round : 5 | Local Epoch : 29.0 | [7680/8000 (97%)]	Loss: 5.003967                         LR: 0.0000  Feat: 0.973 Epoch Time: 26.804 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  0
Updating local model for agent:  0
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 25.0 | [0/8000 (0%)]	Loss: 5.013049                         LR: 0.0000  Feat: 0.974 Epoch Time: 0.905 Model Time: 0.058 Data Time: 0.754 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 25.0 | [2560/8000 (32%)]	Loss: 4.961651                         LR: 0.0000  Feat: 0.971 Epoch Time: 2.387 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 25.0 | [5120/8000 (65%)]	Loss: 4.997705                         LR: 0.0000  Feat: 0.969 Epoch Time: 3.867 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 25.0 | [7680/8000 (97%)]	Loss: 5.007866                         LR: 0.0000  Feat: 0.973 Epoch Time: 5.347 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 26.0 | [0/8000 (0%)]	Loss: 5.079489                         LR: 0.0000  Feat: 0.971 Epoch Time: 6.311 Model Time: 0.060 Data Time: 0.760 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 26.0 | [2560/8000 (32%)]	Loss: 5.071350                         LR: 0.0000  Feat: 0.973 Epoch Time: 7.794 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 26.0 | [5120/8000 (65%)]	Loss: 5.020759                         LR: 0.0000  Feat: 0.972 Epoch Time: 9.273 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 26.0 | [7680/8000 (97%)]	Loss: 4.963154                         LR: 0.0000  Feat: 0.974 Epoch Time: 10.751 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 27.0 | [0/8000 (0%)]	Loss: 5.031510                         LR: 0.0000  Feat: 0.972 Epoch Time: 11.703 Model Time: 0.060 Data Time: 0.754 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 27.0 | [2560/8000 (32%)]	Loss: 5.045815                         LR: 0.0000  Feat: 0.972 Epoch Time: 13.188 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 27.0 | [5120/8000 (65%)]	Loss: 5.002531                         LR: 0.0000  Feat: 0.971 Epoch Time: 14.670 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 27.0 | [7680/8000 (97%)]	Loss: 4.971353                         LR: 0.0000  Feat: 0.974 Epoch Time: 16.153 Model Time: 0.055 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 28.0 | [0/8000 (0%)]	Loss: 5.018392                         LR: 0.0000  Feat: 0.970 Epoch Time: 17.080 Model Time: 0.055 Data Time: 0.731 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 28.0 | [2560/8000 (32%)]	Loss: 4.978603                         LR: 0.0000  Feat: 0.973 Epoch Time: 18.564 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 28.0 | [5120/8000 (65%)]	Loss: 4.991103                         LR: 0.0000  Feat: 0.968 Epoch Time: 20.045 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 28.0 | [7680/8000 (97%)]	Loss: 5.059216                         LR: 0.0000  Feat: 0.968 Epoch Time: 21.526 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 29.0 | [0/8000 (0%)]	Loss: 4.982662                         LR: 0.0000  Feat: 0.972 Epoch Time: 22.467 Model Time: 0.055 Data Time: 0.745 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 29.0 | [2560/8000 (32%)]	Loss: 5.010964                         LR: 0.0000  Feat: 0.970 Epoch Time: 23.949 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 29.0 | [5120/8000 (65%)]	Loss: 5.002401                         LR: 0.0000  Feat: 0.971 Epoch Time: 25.429 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 0 | Global Round : 5 | Local Epoch : 29.0 | [7680/8000 (97%)]	Loss: 5.018744                         LR: 0.0000  Feat: 0.969 Epoch Time: 26.912 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  4
Updating local model for agent:  4
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 25.0 | [0/8000 (0%)]	Loss: 5.002234                         LR: 0.0000  Feat: 0.974 Epoch Time: 0.886 Model Time: 0.057 Data Time: 0.736 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 25.0 | [2560/8000 (32%)]	Loss: 5.044193                         LR: 0.0000  Feat: 0.972 Epoch Time: 2.368 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 25.0 | [5120/8000 (65%)]	Loss: 4.997587                         LR: 0.0000  Feat: 0.973 Epoch Time: 3.848 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 25.0 | [7680/8000 (97%)]	Loss: 4.990137                         LR: 0.0000  Feat: 0.972 Epoch Time: 5.329 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 26.0 | [0/8000 (0%)]	Loss: 4.956851                         LR: 0.0000  Feat: 0.972 Epoch Time: 6.257 Model Time: 0.055 Data Time: 0.731 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 26.0 | [2560/8000 (32%)]	Loss: 5.001884                         LR: 0.0000  Feat: 0.972 Epoch Time: 7.736 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 26.0 | [5120/8000 (65%)]	Loss: 5.015786                         LR: 0.0000  Feat: 0.971 Epoch Time: 9.214 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 26.0 | [7680/8000 (97%)]	Loss: 4.991256                         LR: 0.0000  Feat: 0.971 Epoch Time: 10.693 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 27.0 | [0/8000 (0%)]	Loss: 5.029260                         LR: 0.0000  Feat: 0.973 Epoch Time: 11.612 Model Time: 0.055 Data Time: 0.720 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 27.0 | [2560/8000 (32%)]	Loss: 5.006548                         LR: 0.0000  Feat: 0.972 Epoch Time: 13.092 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 27.0 | [5120/8000 (65%)]	Loss: 4.932570                         LR: 0.0000  Feat: 0.973 Epoch Time: 14.571 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 27.0 | [7680/8000 (97%)]	Loss: 4.979543                         LR: 0.0000  Feat: 0.973 Epoch Time: 16.049 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 28.0 | [0/8000 (0%)]	Loss: 4.999074                         LR: 0.0000  Feat: 0.974 Epoch Time: 16.960 Model Time: 0.054 Data Time: 0.714 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 28.0 | [2560/8000 (32%)]	Loss: 4.997727                         LR: 0.0000  Feat: 0.970 Epoch Time: 18.441 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 28.0 | [5120/8000 (65%)]	Loss: 5.048803                         LR: 0.0000  Feat: 0.971 Epoch Time: 19.920 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 28.0 | [7680/8000 (97%)]	Loss: 5.003440                         LR: 0.0000  Feat: 0.969 Epoch Time: 21.400 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 29.0 | [0/8000 (0%)]	Loss: 5.019690                         LR: 0.0000  Feat: 0.970 Epoch Time: 22.309 Model Time: 0.059 Data Time: 0.714 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 29.0 | [2560/8000 (32%)]	Loss: 4.963296                         LR: 0.0000  Feat: 0.971 Epoch Time: 23.788 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 29.0 | [5120/8000 (65%)]	Loss: 4.968318                         LR: 0.0000  Feat: 0.972 Epoch Time: 25.266 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 4 | Global Round : 5 | Local Epoch : 29.0 | [7680/8000 (97%)]	Loss: 4.971390                         LR: 0.0000  Feat: 0.973 Epoch Time: 26.744 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Updating local model for agent:  3
Updating local model for agent:  3
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 25.0 | [0/8000 (0%)]	Loss: 5.014535                         LR: 0.0000  Feat: 0.976 Epoch Time: 0.846 Model Time: 0.057 Data Time: 0.695 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 25.0 | [2560/8000 (32%)]	Loss: 5.011876                         LR: 0.0000  Feat: 0.976 Epoch Time: 2.329 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 25.0 | [5120/8000 (65%)]	Loss: 4.970879                         LR: 0.0000  Feat: 0.973 Epoch Time: 3.808 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 25.0 | [7680/8000 (97%)]	Loss: 4.954823                         LR: 0.0000  Feat: 0.973 Epoch Time: 5.287 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 26.0 | [0/8000 (0%)]	Loss: 4.976593                         LR: 0.0000  Feat: 0.973 Epoch Time: 6.173 Model Time: 0.055 Data Time: 0.687 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 26.0 | [2560/8000 (32%)]	Loss: 4.992292                         LR: 0.0000  Feat: 0.973 Epoch Time: 7.654 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 26.0 | [5120/8000 (65%)]	Loss: 5.013245                         LR: 0.0000  Feat: 0.972 Epoch Time: 9.134 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 26.0 | [7680/8000 (97%)]	Loss: 5.018021                         LR: 0.0000  Feat: 0.973 Epoch Time: 10.613 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 27.0 | [0/8000 (0%)]	Loss: 4.956079                         LR: 0.0000  Feat: 0.971 Epoch Time: 11.495 Model Time: 0.056 Data Time: 0.688 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 27.0 | [2560/8000 (32%)]	Loss: 5.009906                         LR: 0.0000  Feat: 0.973 Epoch Time: 12.977 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 27.0 | [5120/8000 (65%)]	Loss: 5.018697                         LR: 0.0000  Feat: 0.971 Epoch Time: 14.456 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 27.0 | [7680/8000 (97%)]	Loss: 4.976757                         LR: 0.0000  Feat: 0.971 Epoch Time: 15.936 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 28.0 | [0/8000 (0%)]	Loss: 4.964463                         LR: 0.0000  Feat: 0.971 Epoch Time: 16.965 Model Time: 0.057 Data Time: 0.822 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 28.0 | [2560/8000 (32%)]	Loss: 5.094462                         LR: 0.0000  Feat: 0.972 Epoch Time: 18.447 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 28.0 | [5120/8000 (65%)]	Loss: 4.971812                         LR: 0.0000  Feat: 0.973 Epoch Time: 19.926 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 28.0 | [7680/8000 (97%)]	Loss: 4.972837                         LR: 0.0000  Feat: 0.969 Epoch Time: 21.406 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 29.0 | [0/8000 (0%)]	Loss: 5.004621                         LR: 0.0000  Feat: 0.970 Epoch Time: 22.380 Model Time: 0.057 Data Time: 0.771 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 29.0 | [2560/8000 (32%)]	Loss: 5.000242                         LR: 0.0000  Feat: 0.971 Epoch Time: 23.863 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 29.0 | [5120/8000 (65%)]	Loss: 5.028690                         LR: 0.0000  Feat: 0.971 Epoch Time: 25.343 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 3 | Global Round : 5 | Local Epoch : 29.0 | [7680/8000 (97%)]	Loss: 5.032910                         LR: 0.0000  Feat: 0.969 Epoch Time: 26.823 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Updating local model for agent:  2
Updating local model for agent:  2
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 25.0 | [0/8000 (0%)]	Loss: 5.016467                         LR: 0.0000  Feat: 0.975 Epoch Time: 0.926 Model Time: 0.058 Data Time: 0.775 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 25.0 | [2560/8000 (32%)]	Loss: 4.963256                         LR: 0.0000  Feat: 0.975 Epoch Time: 2.408 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 25.0 | [5120/8000 (65%)]	Loss: 5.001883                         LR: 0.0000  Feat: 0.974 Epoch Time: 3.887 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 25.0 | [7680/8000 (97%)]	Loss: 4.988350                         LR: 0.0000  Feat: 0.973 Epoch Time: 5.366 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 26.0 | [0/8000 (0%)]	Loss: 5.001626                         LR: 0.0000  Feat: 0.975 Epoch Time: 6.324 Model Time: 0.055 Data Time: 0.759 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 26.0 | [2560/8000 (32%)]	Loss: 4.995111                         LR: 0.0000  Feat: 0.972 Epoch Time: 7.808 Model Time: 0.051 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 26.0 | [5120/8000 (65%)]	Loss: 4.971930                         LR: 0.0000  Feat: 0.972 Epoch Time: 9.287 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 26.0 | [7680/8000 (97%)]	Loss: 5.012622                         LR: 0.0000  Feat: 0.972 Epoch Time: 10.767 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 27.0 | [0/8000 (0%)]	Loss: 5.039254                         LR: 0.0000  Feat: 0.972 Epoch Time: 11.664 Model Time: 0.056 Data Time: 0.701 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 27.0 | [2560/8000 (32%)]	Loss: 5.008436                         LR: 0.0000  Feat: 0.974 Epoch Time: 13.148 Model Time: 0.052 Data Time: 0.096 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 27.0 | [5120/8000 (65%)]	Loss: 5.024944                         LR: 0.0000  Feat: 0.970 Epoch Time: 14.627 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 27.0 | [7680/8000 (97%)]	Loss: 5.006331                         LR: 0.0000  Feat: 0.973 Epoch Time: 16.106 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 28.0 | [0/8000 (0%)]	Loss: 4.980306                         LR: 0.0000  Feat: 0.970 Epoch Time: 17.005 Model Time: 0.058 Data Time: 0.700 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 28.0 | [2560/8000 (32%)]	Loss: 5.029741                         LR: 0.0000  Feat: 0.971 Epoch Time: 18.485 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 28.0 | [5120/8000 (65%)]	Loss: 5.034456                         LR: 0.0000  Feat: 0.972 Epoch Time: 19.962 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 28.0 | [7680/8000 (97%)]	Loss: 4.976218                         LR: 0.0000  Feat: 0.969 Epoch Time: 21.441 Model Time: 0.052 Data Time: 0.098 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 29.0 | [0/8000 (0%)]	Loss: 4.996649                         LR: 0.0000  Feat: 0.971 Epoch Time: 22.332 Model Time: 0.054 Data Time: 0.697 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 29.0 | [2560/8000 (32%)]	Loss: 4.988780                         LR: 0.0000  Feat: 0.973 Epoch Time: 23.815 Model Time: 0.052 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 29.0 | [5120/8000 (65%)]	Loss: 5.023676                         LR: 0.0000  Feat: 0.973 Epoch Time: 25.297 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Update SSL || User : 2 | Global Round : 5 | Local Epoch : 29.0 | [7680/8000 (97%)]	Loss: 4.988732                         LR: 0.0000  Feat: 0.968 Epoch Time: 26.777 Model Time: 0.051 Data Time: 0.097 Model: 05-18_04:53_3082253
Federated averaging
evaluating representations:  save/05-18_04:53_3082253iid_dec-False_ED-False_pe1_a5_e30_le5.0
Training a classifier on the federated global model
Training classifier
sampling for dataset: mnist
Files already downloaded and verified
dataset sample num: (50000, 32, 32, 3)
Files already downloaded and verified
Files already downloaded and verified
get dataset time: 2.064
Use i.i.d. sampling
sample dataset time: 0.027
user data samples: [10000, 10000, 10000, 10000, 10000]
begin training classifier...
Downstream Train Epoch: 1 [0/50000 (0%)]	Loss: 2.644105
Model None Epoch 1 Batch 0: Loss 2.6441051959991455
Model None Epoch 1 Batch 1: Loss 2.4079737663269043
Model None Epoch 1 Batch 2: Loss 2.24428391456604
Model None Epoch 1 Batch 3: Loss 2.1388227939605713
Model None Epoch 1 Batch 4: Loss 2.099318027496338
Model None Epoch 1 Batch 5: Loss 2.031827211380005
Model None Epoch 1 Batch 6: Loss 1.9204223155975342
Model None Epoch 1 Batch 7: Loss 1.92332923412323
Model None Epoch 1 Batch 8: Loss 1.8437004089355469
Model None Epoch 1 Batch 9: Loss 1.7591673135757446
Model None Epoch 1 Batch 10: Loss 1.7757693529129028
Model None Epoch 1 Batch 11: Loss 1.7464964389801025
Model None Epoch 1 Batch 12: Loss 1.6635464429855347
Model None Epoch 1 Batch 13: Loss 1.7403769493103027
Model None Epoch 1 Batch 14: Loss 1.7808932065963745
Model None Epoch 1 Batch 15: Loss 1.6622718572616577
Model None Epoch 1 Batch 16: Loss 1.6107994318008423
Model None Epoch 1 Batch 17: Loss 1.6844474077224731
Model None Epoch 1 Batch 18: Loss 1.7074120044708252
Model None Epoch 1 Batch 19: Loss 1.7948144674301147
Model None Epoch 1 Batch 20: Loss 1.6606838703155518
Model None Epoch 1 Batch 21: Loss 1.5164892673492432
Model None Epoch 1 Batch 22: Loss 1.726507544517517
Model None Epoch 1 Batch 23: Loss 1.683110237121582
Model None Epoch 1 Batch 24: Loss 1.6944544315338135
Model None Epoch 1 Batch 25: Loss 1.6359072923660278
Model None Epoch 1 Batch 26: Loss 1.7126960754394531
Model None Epoch 1 Batch 27: Loss 1.5679354667663574
Model None Epoch 1 Batch 28: Loss 1.6842886209487915
Model None Epoch 1 Batch 29: Loss 1.7005650997161865
Model None Epoch 1 Batch 30: Loss 1.5547802448272705
Model None Epoch 1 Batch 31: Loss 1.4885468482971191
Model None Epoch 1 Batch 32: Loss 1.5466113090515137
Model None Epoch 1 Batch 33: Loss 1.6092376708984375
Model None Epoch 1 Batch 34: Loss 1.661239743232727
Model None Epoch 1 Batch 35: Loss 1.668895959854126
Model None Epoch 1 Batch 36: Loss 1.5580638647079468
Model None Epoch 1 Batch 37: Loss 1.6097910404205322
Model None Epoch 1 Batch 38: Loss 1.679063081741333
Model None Epoch 1 Batch 39: Loss 1.4796034097671509
Model None Epoch 1 Batch 40: Loss 1.6327922344207764
Model None Epoch 1 Batch 41: Loss 1.6003509759902954
Model None Epoch 1 Batch 42: Loss 1.5860520601272583
Model None Epoch 1 Batch 43: Loss 1.524570345878601
Model None Epoch 1 Batch 44: Loss 1.6467937231063843
Model None Epoch 1 Batch 45: Loss 1.6727221012115479
Model None Epoch 1 Batch 46: Loss 1.593157172203064
Model None Epoch 1 Batch 47: Loss 1.611512541770935
Model None Epoch 1 Batch 48: Loss 1.5435190200805664
Model None Epoch 1 Batch 49: Loss 1.6674917936325073
Downstream Train Epoch: 1 [12800/50000 (26%)]	Loss: 1.480614
Model None Epoch 1 Batch 50: Loss 1.4806139469146729
Model None Epoch 1 Batch 51: Loss 1.5361171960830688
Model None Epoch 1 Batch 52: Loss 1.635718822479248
Model None Epoch 1 Batch 53: Loss 1.665387511253357
Model None Epoch 1 Batch 54: Loss 1.5239224433898926
Model None Epoch 1 Batch 55: Loss 1.6421955823898315
Model None Epoch 1 Batch 56: Loss 1.6255251169204712
Model None Epoch 1 Batch 57: Loss 1.592909574508667
Model None Epoch 1 Batch 58: Loss 1.6865545511245728
Model None Epoch 1 Batch 59: Loss 1.5633524656295776
Model None Epoch 1 Batch 60: Loss 1.4769365787506104
Model None Epoch 1 Batch 61: Loss 1.558846116065979
Model None Epoch 1 Batch 62: Loss 1.6691244840621948
Model None Epoch 1 Batch 63: Loss 1.6519429683685303
Model None Epoch 1 Batch 64: Loss 1.5165566205978394
Model None Epoch 1 Batch 65: Loss 1.6377818584442139
Model None Epoch 1 Batch 66: Loss 1.547946572303772
Model None Epoch 1 Batch 67: Loss 1.4967159032821655
Model None Epoch 1 Batch 68: Loss 1.7361263036727905
Model None Epoch 1 Batch 69: Loss 1.4973523616790771
Model None Epoch 1 Batch 70: Loss 1.6056452989578247
Model None Epoch 1 Batch 71: Loss 1.5231952667236328
Model None Epoch 1 Batch 72: Loss 1.5499330759048462
Model None Epoch 1 Batch 73: Loss 1.6150814294815063
Model None Epoch 1 Batch 74: Loss 1.5890470743179321
Model None Epoch 1 Batch 75: Loss 1.5379929542541504
Model None Epoch 1 Batch 76: Loss 1.5463674068450928
Model None Epoch 1 Batch 77: Loss 1.5250147581100464
Model None Epoch 1 Batch 78: Loss 1.517856240272522
Model None Epoch 1 Batch 79: Loss 1.6253020763397217
Model None Epoch 1 Batch 80: Loss 1.6813522577285767
Model None Epoch 1 Batch 81: Loss 1.5127043724060059
Model None Epoch 1 Batch 82: Loss 1.6027332544326782
Model None Epoch 1 Batch 83: Loss 1.5986508131027222
Model None Epoch 1 Batch 84: Loss 1.5867035388946533
Model None Epoch 1 Batch 85: Loss 1.4886306524276733
Model None Epoch 1 Batch 86: Loss 1.5579586029052734
Model None Epoch 1 Batch 87: Loss 1.5393824577331543
Model None Epoch 1 Batch 88: Loss 1.5570759773254395
Model None Epoch 1 Batch 89: Loss 1.632472038269043
Model None Epoch 1 Batch 90: Loss 1.6080491542816162
Model None Epoch 1 Batch 91: Loss 1.5975780487060547
Model None Epoch 1 Batch 92: Loss 1.5552181005477905
Model None Epoch 1 Batch 93: Loss 1.5926413536071777
Model None Epoch 1 Batch 94: Loss 1.4844038486480713
Model None Epoch 1 Batch 95: Loss 1.6289085149765015
Model None Epoch 1 Batch 96: Loss 1.6405352354049683
Model None Epoch 1 Batch 97: Loss 1.5606940984725952
Model None Epoch 1 Batch 98: Loss 1.5268347263336182
Model None Epoch 1 Batch 99: Loss 1.5608103275299072
Downstream Train Epoch: 1 [25600/50000 (51%)]	Loss: 1.623971
Model None Epoch 1 Batch 100: Loss 1.6239705085754395
Model None Epoch 1 Batch 101: Loss 1.5325500965118408
Model None Epoch 1 Batch 102: Loss 1.615195393562317
Model None Epoch 1 Batch 103: Loss 1.5190848112106323
Model None Epoch 1 Batch 104: Loss 1.671210527420044
Model None Epoch 1 Batch 105: Loss 1.4815970659255981
Model None Epoch 1 Batch 106: Loss 1.5642794370651245
Model None Epoch 1 Batch 107: Loss 1.637436032295227
Model None Epoch 1 Batch 108: Loss 1.6760104894638062
Model None Epoch 1 Batch 109: Loss 1.5562677383422852
Model None Epoch 1 Batch 110: Loss 1.6203950643539429
Model None Epoch 1 Batch 111: Loss 1.555012583732605
Model None Epoch 1 Batch 112: Loss 1.5145927667617798
Model None Epoch 1 Batch 113: Loss 1.5598704814910889
Model None Epoch 1 Batch 114: Loss 1.4873857498168945
Model None Epoch 1 Batch 115: Loss 1.535723090171814
Model None Epoch 1 Batch 116: Loss 1.6135445833206177
Model None Epoch 1 Batch 117: Loss 1.5005544424057007
Model None Epoch 1 Batch 118: Loss 1.5788326263427734
Model None Epoch 1 Batch 119: Loss 1.5413614511489868
Model None Epoch 1 Batch 120: Loss 1.5620023012161255
Model None Epoch 1 Batch 121: Loss 1.4741172790527344
Model None Epoch 1 Batch 122: Loss 1.5522266626358032
Model None Epoch 1 Batch 123: Loss 1.5703449249267578
Model None Epoch 1 Batch 124: Loss 1.5716605186462402
Model None Epoch 1 Batch 125: Loss 1.5573211908340454
Model None Epoch 1 Batch 126: Loss 1.6897469758987427
Model None Epoch 1 Batch 127: Loss 1.432936191558838
Model None Epoch 1 Batch 128: Loss 1.5240434408187866
Model None Epoch 1 Batch 129: Loss 1.4851068258285522
Model None Epoch 1 Batch 130: Loss 1.5038671493530273
Model None Epoch 1 Batch 131: Loss 1.748427152633667
Model None Epoch 1 Batch 132: Loss 1.62692129611969
Model None Epoch 1 Batch 133: Loss 1.5588394403457642
Model None Epoch 1 Batch 134: Loss 1.6022224426269531
Model None Epoch 1 Batch 135: Loss 1.5062288045883179
Model None Epoch 1 Batch 136: Loss 1.5390081405639648
Model None Epoch 1 Batch 137: Loss 1.6967101097106934
Model None Epoch 1 Batch 138: Loss 1.702515721321106
Model None Epoch 1 Batch 139: Loss 1.5486823320388794
Model None Epoch 1 Batch 140: Loss 1.6019335985183716
Model None Epoch 1 Batch 141: Loss 1.5686885118484497
Model None Epoch 1 Batch 142: Loss 1.4476372003555298
Model None Epoch 1 Batch 143: Loss 1.703523874282837
Model None Epoch 1 Batch 144: Loss 1.4832470417022705
Model None Epoch 1 Batch 145: Loss 1.5660607814788818
Model None Epoch 1 Batch 146: Loss 1.4460358619689941
Model None Epoch 1 Batch 147: Loss 1.6159461736679077
Model None Epoch 1 Batch 148: Loss 1.6149789094924927
Model None Epoch 1 Batch 149: Loss 1.5579047203063965
Downstream Train Epoch: 1 [38400/50000 (77%)]	Loss: 1.668272
Model None Epoch 1 Batch 150: Loss 1.668271780014038
Model None Epoch 1 Batch 151: Loss 1.5166740417480469
Model None Epoch 1 Batch 152: Loss 1.4086767435073853
Model None Epoch 1 Batch 153: Loss 1.557366132736206
Model None Epoch 1 Batch 154: Loss 1.559558629989624
Model None Epoch 1 Batch 155: Loss 1.493028998374939
Model None Epoch 1 Batch 156: Loss 1.5649796724319458
Model None Epoch 1 Batch 157: Loss 1.6061512231826782
Model None Epoch 1 Batch 158: Loss 1.6171573400497437
Model None Epoch 1 Batch 159: Loss 1.582440733909607
Model None Epoch 1 Batch 160: Loss 1.533302664756775
Model None Epoch 1 Batch 161: Loss 1.5784293413162231
Model None Epoch 1 Batch 162: Loss 1.5565215349197388
Model None Epoch 1 Batch 163: Loss 1.6442292928695679
Model None Epoch 1 Batch 164: Loss 1.6090877056121826
Model None Epoch 1 Batch 165: Loss 1.4367570877075195
Model None Epoch 1 Batch 166: Loss 1.5083703994750977
Model None Epoch 1 Batch 167: Loss 1.5245945453643799
Model None Epoch 1 Batch 168: Loss 1.556343913078308
Model None Epoch 1 Batch 169: Loss 1.59305739402771
Model None Epoch 1 Batch 170: Loss 1.4551295042037964
Model None Epoch 1 Batch 171: Loss 1.5299190282821655
Model None Epoch 1 Batch 172: Loss 1.480358600616455
Model None Epoch 1 Batch 173: Loss 1.4817537069320679
Model None Epoch 1 Batch 174: Loss 1.6178879737854004
Model None Epoch 1 Batch 175: Loss 1.5862001180648804
Model None Epoch 1 Batch 176: Loss 1.6040899753570557
Model None Epoch 1 Batch 177: Loss 1.4634641408920288
Model None Epoch 1 Batch 178: Loss 1.4477486610412598
Model None Epoch 1 Batch 179: Loss 1.4791455268859863
Model None Epoch 1 Batch 180: Loss 1.6919151544570923
Model None Epoch 1 Batch 181: Loss 1.5983986854553223
Model None Epoch 1 Batch 182: Loss 1.458135962486267
Model None Epoch 1 Batch 183: Loss 1.5663435459136963
Model None Epoch 1 Batch 184: Loss 1.6361608505249023
Model None Epoch 1 Batch 185: Loss 1.603510856628418
Model None Epoch 1 Batch 186: Loss 1.4718087911605835
Model None Epoch 1 Batch 187: Loss 1.6135950088500977
Model None Epoch 1 Batch 188: Loss 1.5087560415267944
Model None Epoch 1 Batch 189: Loss 1.5216048955917358
Model None Epoch 1 Batch 190: Loss 1.5839042663574219
Model None Epoch 1 Batch 191: Loss 1.6088778972625732
Model None Epoch 1 Batch 192: Loss 1.4630932807922363
Model None Epoch 1 Batch 193: Loss 1.5862317085266113
Model None Epoch 1 Batch 194: Loss 1.5401709079742432
Model None Epoch 1 Batch 195: Loss 1.6092872619628906

 Downstream Train loss: 1.608948279400261 Acc: 0.4698
Downstream Train Epoch: 2 [0/50000 (0%)]	Loss: 1.536870
Model None Epoch 2 Batch 0: Loss 1.5368703603744507
Model None Epoch 2 Batch 1: Loss 1.5406230688095093
Model None Epoch 2 Batch 2: Loss 1.6063857078552246
Model None Epoch 2 Batch 3: Loss 1.5217643976211548
Model None Epoch 2 Batch 4: Loss 1.644500732421875
Model None Epoch 2 Batch 5: Loss 1.4815809726715088
Model None Epoch 2 Batch 6: Loss 1.544097661972046
Model None Epoch 2 Batch 7: Loss 1.6004443168640137
Model None Epoch 2 Batch 8: Loss 1.5019651651382446
Model None Epoch 2 Batch 9: Loss 1.5995310544967651
Model None Epoch 2 Batch 10: Loss 1.4954849481582642
Model None Epoch 2 Batch 11: Loss 1.5128930807113647
Model None Epoch 2 Batch 12: Loss 1.461496114730835
Model None Epoch 2 Batch 13: Loss 1.6200528144836426
Model None Epoch 2 Batch 14: Loss 1.5131603479385376
Model None Epoch 2 Batch 15: Loss 1.6323087215423584
Model None Epoch 2 Batch 16: Loss 1.5528193712234497
Model None Epoch 2 Batch 17: Loss 1.5168098211288452
Model None Epoch 2 Batch 18: Loss 1.5984580516815186
Model None Epoch 2 Batch 19: Loss 1.4470041990280151
Model None Epoch 2 Batch 20: Loss 1.4826539754867554
Model None Epoch 2 Batch 21: Loss 1.608604073524475
Model None Epoch 2 Batch 22: Loss 1.5453110933303833
Model None Epoch 2 Batch 23: Loss 1.5831788778305054
Model None Epoch 2 Batch 24: Loss 1.596973180770874
Model None Epoch 2 Batch 25: Loss 1.5088856220245361
Model None Epoch 2 Batch 26: Loss 1.5655511617660522
Model None Epoch 2 Batch 27: Loss 1.5575487613677979
Model None Epoch 2 Batch 28: Loss 1.5028843879699707
Model None Epoch 2 Batch 29: Loss 1.6067191362380981
Model None Epoch 2 Batch 30: Loss 1.6358039379119873
Model None Epoch 2 Batch 31: Loss 1.5454933643341064
Model None Epoch 2 Batch 32: Loss 1.5546479225158691
Model None Epoch 2 Batch 33: Loss 1.52069091796875
Model None Epoch 2 Batch 34: Loss 1.5273103713989258
Model None Epoch 2 Batch 35: Loss 1.452586054801941
Model None Epoch 2 Batch 36: Loss 1.5158398151397705
Model None Epoch 2 Batch 37: Loss 1.5808483362197876
Model None Epoch 2 Batch 38: Loss 1.5550609827041626
Model None Epoch 2 Batch 39: Loss 1.5522984266281128
Model None Epoch 2 Batch 40: Loss 1.588510513305664
Model None Epoch 2 Batch 41: Loss 1.4782233238220215
Model None Epoch 2 Batch 42: Loss 1.6070597171783447
Model None Epoch 2 Batch 43: Loss 1.6360992193222046
Model None Epoch 2 Batch 44: Loss 1.5763413906097412
Model None Epoch 2 Batch 45: Loss 1.5550793409347534
Model None Epoch 2 Batch 46: Loss 1.5190293788909912
Model None Epoch 2 Batch 47: Loss 1.5353952646255493
Model None Epoch 2 Batch 48: Loss 1.493999719619751
Model None Epoch 2 Batch 49: Loss 1.6200096607208252
Downstream Train Epoch: 2 [12800/50000 (26%)]	Loss: 1.530426
Model None Epoch 2 Batch 50: Loss 1.5304263830184937
Model None Epoch 2 Batch 51: Loss 1.4812428951263428
Model None Epoch 2 Batch 52: Loss 1.603865146636963
Model None Epoch 2 Batch 53: Loss 1.5676050186157227
Model None Epoch 2 Batch 54: Loss 1.5340546369552612
Model None Epoch 2 Batch 55: Loss 1.53916335105896
Model None Epoch 2 Batch 56: Loss 1.4778670072555542
Model None Epoch 2 Batch 57: Loss 1.499056339263916
Model None Epoch 2 Batch 58: Loss 1.5529332160949707
Model None Epoch 2 Batch 59: Loss 1.495688557624817
Model None Epoch 2 Batch 60: Loss 1.46202552318573
Model None Epoch 2 Batch 61: Loss 1.5219297409057617
Model None Epoch 2 Batch 62: Loss 1.580530047416687
Model None Epoch 2 Batch 63: Loss 1.4871217012405396
Model None Epoch 2 Batch 64: Loss 1.6016807556152344
Model None Epoch 2 Batch 65: Loss 1.4754078388214111
Model None Epoch 2 Batch 66: Loss 1.5660851001739502
Model None Epoch 2 Batch 67: Loss 1.5651938915252686
Model None Epoch 2 Batch 68: Loss 1.583783745765686
Model None Epoch 2 Batch 69: Loss 1.531136155128479
Model None Epoch 2 Batch 70: Loss 1.514639973640442
Model None Epoch 2 Batch 71: Loss 1.4252384901046753
Model None Epoch 2 Batch 72: Loss 1.613368034362793
Model None Epoch 2 Batch 73: Loss 1.533050775527954
Model None Epoch 2 Batch 74: Loss 1.4979681968688965
Model None Epoch 2 Batch 75: Loss 1.5531904697418213
Model None Epoch 2 Batch 76: Loss 1.4928539991378784
Model None Epoch 2 Batch 77: Loss 1.6302746534347534
Model None Epoch 2 Batch 78: Loss 1.4204884767532349
Model None Epoch 2 Batch 79: Loss 1.5068565607070923
Model None Epoch 2 Batch 80: Loss 1.4281665086746216
Model None Epoch 2 Batch 81: Loss 1.512300968170166
Model None Epoch 2 Batch 82: Loss 1.5555886030197144
Model None Epoch 2 Batch 83: Loss 1.4495735168457031
Model None Epoch 2 Batch 84: Loss 1.4868160486221313
Model None Epoch 2 Batch 85: Loss 1.5697418451309204
Model None Epoch 2 Batch 86: Loss 1.6259427070617676
Model None Epoch 2 Batch 87: Loss 1.438847541809082
Model None Epoch 2 Batch 88: Loss 1.5670857429504395
Model None Epoch 2 Batch 89: Loss 1.544150471687317
Model None Epoch 2 Batch 90: Loss 1.6420745849609375
Model None Epoch 2 Batch 91: Loss 1.4925298690795898
Model None Epoch 2 Batch 92: Loss 1.55422043800354
Model None Epoch 2 Batch 93: Loss 1.4653751850128174
Model None Epoch 2 Batch 94: Loss 1.6046206951141357
Model None Epoch 2 Batch 95: Loss 1.4363933801651
Model None Epoch 2 Batch 96: Loss 1.4749373197555542
Model None Epoch 2 Batch 97: Loss 1.5667656660079956
Model None Epoch 2 Batch 98: Loss 1.4483288526535034
Model None Epoch 2 Batch 99: Loss 1.5099481344223022
Downstream Train Epoch: 2 [25600/50000 (51%)]	Loss: 1.509361
Model None Epoch 2 Batch 100: Loss 1.509360909461975
Model None Epoch 2 Batch 101: Loss 1.4576963186264038
Model None Epoch 2 Batch 102: Loss 1.498510718345642
Model None Epoch 2 Batch 103: Loss 1.56571626663208
Model None Epoch 2 Batch 104: Loss 1.5387282371520996
Model None Epoch 2 Batch 105: Loss 1.526337742805481
Model None Epoch 2 Batch 106: Loss 1.5796589851379395
Model None Epoch 2 Batch 107: Loss 1.5717934370040894
Model None Epoch 2 Batch 108: Loss 1.43960440158844
Model None Epoch 2 Batch 109: Loss 1.4542245864868164
Model None Epoch 2 Batch 110: Loss 1.5677099227905273
Model None Epoch 2 Batch 111: Loss 1.4270060062408447
Model None Epoch 2 Batch 112: Loss 1.529719352722168
Model None Epoch 2 Batch 113: Loss 1.593546986579895
Model None Epoch 2 Batch 114: Loss 1.438572883605957
Model None Epoch 2 Batch 115: Loss 1.4545741081237793
Model None Epoch 2 Batch 116: Loss 1.4867581129074097
Model None Epoch 2 Batch 117: Loss 1.563677191734314
Model None Epoch 2 Batch 118: Loss 1.5240049362182617
Model None Epoch 2 Batch 119: Loss 1.4967716932296753
Model None Epoch 2 Batch 120: Loss 1.4200918674468994
Model None Epoch 2 Batch 121: Loss 1.4643610715866089
Model None Epoch 2 Batch 122: Loss 1.5185232162475586
Model None Epoch 2 Batch 123: Loss 1.574167013168335
Model None Epoch 2 Batch 124: Loss 1.5158966779708862
Model None Epoch 2 Batch 125: Loss 1.4187355041503906
Model None Epoch 2 Batch 126: Loss 1.5538538694381714
Model None Epoch 2 Batch 127: Loss 1.4591113328933716
Model None Epoch 2 Batch 128: Loss 1.4992821216583252
Model None Epoch 2 Batch 129: Loss 1.6294864416122437
Model None Epoch 2 Batch 130: Loss 1.562451720237732
Model None Epoch 2 Batch 131: Loss 1.589697241783142
Model None Epoch 2 Batch 132: Loss 1.4574602842330933
Model None Epoch 2 Batch 133: Loss 1.4892082214355469
Model None Epoch 2 Batch 134: Loss 1.4787027835845947
Model None Epoch 2 Batch 135: Loss 1.512269377708435
Model None Epoch 2 Batch 136: Loss 1.4945069551467896
Model None Epoch 2 Batch 137: Loss 1.4949469566345215
Model None Epoch 2 Batch 138: Loss 1.5954022407531738
Model None Epoch 2 Batch 139: Loss 1.549622893333435
Model None Epoch 2 Batch 140: Loss 1.6103804111480713
Model None Epoch 2 Batch 141: Loss 1.5712593793869019
Model None Epoch 2 Batch 142: Loss 1.5494191646575928
Model None Epoch 2 Batch 143: Loss 1.6163246631622314
Model None Epoch 2 Batch 144: Loss 1.7181494235992432
Model None Epoch 2 Batch 145: Loss 1.6101887226104736
Model None Epoch 2 Batch 146: Loss 1.6325827836990356
Model None Epoch 2 Batch 147: Loss 1.5584930181503296
Model None Epoch 2 Batch 148: Loss 1.6334916353225708
Model None Epoch 2 Batch 149: Loss 1.5883514881134033
Downstream Train Epoch: 2 [38400/50000 (77%)]	Loss: 1.512256
Model None Epoch 2 Batch 150: Loss 1.5122559070587158
Model None Epoch 2 Batch 151: Loss 1.415937066078186
Model None Epoch 2 Batch 152: Loss 1.4125615358352661
Model None Epoch 2 Batch 153: Loss 1.3889541625976562
Model None Epoch 2 Batch 154: Loss 1.585018277168274
Model None Epoch 2 Batch 155: Loss 1.415595293045044
Model None Epoch 2 Batch 156: Loss 1.5276089906692505
Model None Epoch 2 Batch 157: Loss 1.6045657396316528
Model None Epoch 2 Batch 158: Loss 1.487680196762085
Model None Epoch 2 Batch 159: Loss 1.6024452447891235
Model None Epoch 2 Batch 160: Loss 1.5400267839431763
Model None Epoch 2 Batch 161: Loss 1.4532471895217896
Model None Epoch 2 Batch 162: Loss 1.4902063608169556
Model None Epoch 2 Batch 163: Loss 1.5433841943740845
Model None Epoch 2 Batch 164: Loss 1.4945628643035889
Model None Epoch 2 Batch 165: Loss 1.4774633646011353
Model None Epoch 2 Batch 166: Loss 1.4854902029037476
Model None Epoch 2 Batch 167: Loss 1.4409384727478027
Model None Epoch 2 Batch 168: Loss 1.574021339416504
Model None Epoch 2 Batch 169: Loss 1.5239979028701782
Model None Epoch 2 Batch 170: Loss 1.4801136255264282
Model None Epoch 2 Batch 171: Loss 1.5049517154693604
Model None Epoch 2 Batch 172: Loss 1.5136733055114746
Model None Epoch 2 Batch 173: Loss 1.5012285709381104
Model None Epoch 2 Batch 174: Loss 1.4362305402755737
Model None Epoch 2 Batch 175: Loss 1.3610836267471313
Model None Epoch 2 Batch 176: Loss 1.5472440719604492
Model None Epoch 2 Batch 177: Loss 1.5471779108047485
Model None Epoch 2 Batch 178: Loss 1.443651795387268
Model None Epoch 2 Batch 179: Loss 1.4178767204284668
Model None Epoch 2 Batch 180: Loss 1.4538925886154175
Model None Epoch 2 Batch 181: Loss 1.4895813465118408
Model None Epoch 2 Batch 182: Loss 1.5669021606445312
Model None Epoch 2 Batch 183: Loss 1.5956119298934937
Model None Epoch 2 Batch 184: Loss 1.5314339399337769
Model None Epoch 2 Batch 185: Loss 1.436090111732483
Model None Epoch 2 Batch 186: Loss 1.4106401205062866
Model None Epoch 2 Batch 187: Loss 1.537551760673523
Model None Epoch 2 Batch 188: Loss 1.5338072776794434
Model None Epoch 2 Batch 189: Loss 1.5380462408065796
Model None Epoch 2 Batch 190: Loss 1.4588390588760376
Model None Epoch 2 Batch 191: Loss 1.500470519065857
Model None Epoch 2 Batch 192: Loss 1.559569239616394
Model None Epoch 2 Batch 193: Loss 1.532947301864624
Model None Epoch 2 Batch 194: Loss 1.5719530582427979
Model None Epoch 2 Batch 195: Loss 1.4456866979599

 Downstream Train loss: 1.5267226081721637 Acc: 0.5033
Downstream Train Epoch: 3 [0/50000 (0%)]	Loss: 1.603155
Model None Epoch 3 Batch 0: Loss 1.6031546592712402
Model None Epoch 3 Batch 1: Loss 1.4921847581863403
Model None Epoch 3 Batch 2: Loss 1.48909592628479
Model None Epoch 3 Batch 3: Loss 1.4481383562088013
Model None Epoch 3 Batch 4: Loss 1.5188660621643066
Model None Epoch 3 Batch 5: Loss 1.5486507415771484
Model None Epoch 3 Batch 6: Loss 1.5545175075531006
Model None Epoch 3 Batch 7: Loss 1.5559004545211792
Model None Epoch 3 Batch 8: Loss 1.433017611503601
Model None Epoch 3 Batch 9: Loss 1.6082919836044312
Model None Epoch 3 Batch 10: Loss 1.3856145143508911
Model None Epoch 3 Batch 11: Loss 1.5249325037002563
Model None Epoch 3 Batch 12: Loss 1.533359408378601
Model None Epoch 3 Batch 13: Loss 1.4470633268356323
Model None Epoch 3 Batch 14: Loss 1.6173131465911865
Model None Epoch 3 Batch 15: Loss 1.5079841613769531
Model None Epoch 3 Batch 16: Loss 1.4694619178771973
Model None Epoch 3 Batch 17: Loss 1.4865086078643799
Model None Epoch 3 Batch 18: Loss 1.4867194890975952
Model None Epoch 3 Batch 19: Loss 1.5922843217849731
Model None Epoch 3 Batch 20: Loss 1.4482592344284058
Model None Epoch 3 Batch 21: Loss 1.4278627634048462
Model None Epoch 3 Batch 22: Loss 1.5732524394989014
Model None Epoch 3 Batch 23: Loss 1.5201839208602905
Model None Epoch 3 Batch 24: Loss 1.5941895246505737
Model None Epoch 3 Batch 25: Loss 1.4413968324661255
Model None Epoch 3 Batch 26: Loss 1.6225550174713135
Model None Epoch 3 Batch 27: Loss 1.491416573524475
Model None Epoch 3 Batch 28: Loss 1.4609050750732422
Model None Epoch 3 Batch 29: Loss 1.5849660634994507
Model None Epoch 3 Batch 30: Loss 1.5031447410583496
Model None Epoch 3 Batch 31: Loss 1.5981018543243408
Model None Epoch 3 Batch 32: Loss 1.495750904083252
Model None Epoch 3 Batch 33: Loss 1.5377373695373535
Model None Epoch 3 Batch 34: Loss 1.518200397491455
Model None Epoch 3 Batch 35: Loss 1.5642611980438232
Model None Epoch 3 Batch 36: Loss 1.5334988832473755
Model None Epoch 3 Batch 37: Loss 1.57419753074646
Model None Epoch 3 Batch 38: Loss 1.5468581914901733
Model None Epoch 3 Batch 39: Loss 1.4774147272109985
Model None Epoch 3 Batch 40: Loss 1.5200637578964233
Model None Epoch 3 Batch 41: Loss 1.5398249626159668
Model None Epoch 3 Batch 42: Loss 1.5344387292861938
Model None Epoch 3 Batch 43: Loss 1.4891853332519531
Model None Epoch 3 Batch 44: Loss 1.4928683042526245
Model None Epoch 3 Batch 45: Loss 1.605395793914795
Model None Epoch 3 Batch 46: Loss 1.4434908628463745
Model None Epoch 3 Batch 47: Loss 1.5425350666046143
Model None Epoch 3 Batch 48: Loss 1.4640874862670898
Model None Epoch 3 Batch 49: Loss 1.5035769939422607
Downstream Train Epoch: 3 [12800/50000 (26%)]	Loss: 1.473519
Model None Epoch 3 Batch 50: Loss 1.4735193252563477
Model None Epoch 3 Batch 51: Loss 1.570794939994812
Model None Epoch 3 Batch 52: Loss 1.5284194946289062
Model None Epoch 3 Batch 53: Loss 1.5437777042388916
Model None Epoch 3 Batch 54: Loss 1.4511196613311768
Model None Epoch 3 Batch 55: Loss 1.5183717012405396
Model None Epoch 3 Batch 56: Loss 1.5837318897247314
Model None Epoch 3 Batch 57: Loss 1.5103296041488647
Model None Epoch 3 Batch 58: Loss 1.551458716392517
Model None Epoch 3 Batch 59: Loss 1.4543390274047852
Model None Epoch 3 Batch 60: Loss 1.3914979696273804
Model None Epoch 3 Batch 61: Loss 1.5269156694412231
Model None Epoch 3 Batch 62: Loss 1.4761204719543457
Model None Epoch 3 Batch 63: Loss 1.5087711811065674
Model None Epoch 3 Batch 64: Loss 1.4360864162445068
Model None Epoch 3 Batch 65: Loss 1.5052655935287476
Model None Epoch 3 Batch 66: Loss 1.5702221393585205
Model None Epoch 3 Batch 67: Loss 1.4746012687683105
Model None Epoch 3 Batch 68: Loss 1.5524115562438965
Model None Epoch 3 Batch 69: Loss 1.3956587314605713
Model None Epoch 3 Batch 70: Loss 1.5105721950531006
Model None Epoch 3 Batch 71: Loss 1.6584153175354004
Model None Epoch 3 Batch 72: Loss 1.44182288646698
Model None Epoch 3 Batch 73: Loss 1.4367873668670654
Model None Epoch 3 Batch 74: Loss 1.58816397190094
Model None Epoch 3 Batch 75: Loss 1.5297881364822388
Model None Epoch 3 Batch 76: Loss 1.5231934785842896
Model None Epoch 3 Batch 77: Loss 1.434030532836914
Model None Epoch 3 Batch 78: Loss 1.431822419166565
Model None Epoch 3 Batch 79: Loss 1.453402042388916
Model None Epoch 3 Batch 80: Loss 1.501824140548706
Model None Epoch 3 Batch 81: Loss 1.4538836479187012
Model None Epoch 3 Batch 82: Loss 1.5119960308074951
Model None Epoch 3 Batch 83: Loss 1.622694492340088
Model None Epoch 3 Batch 84: Loss 1.4990731477737427
Model None Epoch 3 Batch 85: Loss 1.4899860620498657
Model None Epoch 3 Batch 86: Loss 1.5123504400253296
Model None Epoch 3 Batch 87: Loss 1.5146903991699219
Model None Epoch 3 Batch 88: Loss 1.485852599143982
Model None Epoch 3 Batch 89: Loss 1.4927712678909302
Model None Epoch 3 Batch 90: Loss 1.4613534212112427
Model None Epoch 3 Batch 91: Loss 1.4686189889907837
Model None Epoch 3 Batch 92: Loss 1.4818329811096191
Model None Epoch 3 Batch 93: Loss 1.5548548698425293
Model None Epoch 3 Batch 94: Loss 1.5152628421783447
Model None Epoch 3 Batch 95: Loss 1.455434799194336
Model None Epoch 3 Batch 96: Loss 1.4159626960754395
Model None Epoch 3 Batch 97: Loss 1.4897197484970093
Model None Epoch 3 Batch 98: Loss 1.5587438344955444
Model None Epoch 3 Batch 99: Loss 1.5280932188034058
Downstream Train Epoch: 3 [25600/50000 (51%)]	Loss: 1.531768
Model None Epoch 3 Batch 100: Loss 1.5317676067352295
Model None Epoch 3 Batch 101: Loss 1.5282906293869019
Model None Epoch 3 Batch 102: Loss 1.5248788595199585
Model None Epoch 3 Batch 103: Loss 1.3876707553863525
Model None Epoch 3 Batch 104: Loss 1.4461313486099243
Model None Epoch 3 Batch 105: Loss 1.4749646186828613
Model None Epoch 3 Batch 106: Loss 1.5134419202804565
Model None Epoch 3 Batch 107: Loss 1.5544475317001343
Model None Epoch 3 Batch 108: Loss 1.5617543458938599
Model None Epoch 3 Batch 109: Loss 1.5051387548446655
Model None Epoch 3 Batch 110: Loss 1.477347493171692
Model None Epoch 3 Batch 111: Loss 1.464698314666748
Model None Epoch 3 Batch 112: Loss 1.5388656854629517
Model None Epoch 3 Batch 113: Loss 1.5836949348449707
Model None Epoch 3 Batch 114: Loss 1.4047737121582031
Model None Epoch 3 Batch 115: Loss 1.3756903409957886
Model None Epoch 3 Batch 116: Loss 1.4964760541915894
Model None Epoch 3 Batch 117: Loss 1.4777987003326416
Model None Epoch 3 Batch 118: Loss 1.508535385131836
Model None Epoch 3 Batch 119: Loss 1.463341474533081
Model None Epoch 3 Batch 120: Loss 1.567542314529419
Model None Epoch 3 Batch 121: Loss 1.498630404472351
Model None Epoch 3 Batch 122: Loss 1.6472305059432983
Model None Epoch 3 Batch 123: Loss 1.5286345481872559
Model None Epoch 3 Batch 124: Loss 1.4488950967788696
Model None Epoch 3 Batch 125: Loss 1.511020541191101
Model None Epoch 3 Batch 126: Loss 1.5035881996154785
Model None Epoch 3 Batch 127: Loss 1.5204004049301147
Model None Epoch 3 Batch 128: Loss 1.4983160495758057
Model None Epoch 3 Batch 129: Loss 1.5311388969421387
Model None Epoch 3 Batch 130: Loss 1.5458139181137085
Model None Epoch 3 Batch 131: Loss 1.5879169702529907
Model None Epoch 3 Batch 132: Loss 1.4736047983169556
Model None Epoch 3 Batch 133: Loss 1.5301131010055542
Model None Epoch 3 Batch 134: Loss 1.5313174724578857
Model None Epoch 3 Batch 135: Loss 1.4800876379013062
Model None Epoch 3 Batch 136: Loss 1.507203459739685
Model None Epoch 3 Batch 137: Loss 1.47343111038208
Model None Epoch 3 Batch 138: Loss 1.4302951097488403
Model None Epoch 3 Batch 139: Loss 1.545058012008667
Model None Epoch 3 Batch 140: Loss 1.5011998414993286
Model None Epoch 3 Batch 141: Loss 1.4500435590744019
Model None Epoch 3 Batch 142: Loss 1.5227127075195312
Model None Epoch 3 Batch 143: Loss 1.531697154045105
Model None Epoch 3 Batch 144: Loss 1.5104758739471436
Model None Epoch 3 Batch 145: Loss 1.480445384979248
Model None Epoch 3 Batch 146: Loss 1.5107135772705078
Model None Epoch 3 Batch 147: Loss 1.5267623662948608
Model None Epoch 3 Batch 148: Loss 1.4583302736282349
Model None Epoch 3 Batch 149: Loss 1.4742485284805298
Downstream Train Epoch: 3 [38400/50000 (77%)]	Loss: 1.484509
Model None Epoch 3 Batch 150: Loss 1.4845094680786133
Model None Epoch 3 Batch 151: Loss 1.551138997077942
Model None Epoch 3 Batch 152: Loss 1.4905592203140259
Model None Epoch 3 Batch 153: Loss 1.4268864393234253
Model None Epoch 3 Batch 154: Loss 1.5307282209396362
Model None Epoch 3 Batch 155: Loss 1.5662630796432495
Model None Epoch 3 Batch 156: Loss 1.4744296073913574
Model None Epoch 3 Batch 157: Loss 1.593256950378418
Model None Epoch 3 Batch 158: Loss 1.5179775953292847
Model None Epoch 3 Batch 159: Loss 1.4714394807815552
Model None Epoch 3 Batch 160: Loss 1.358931303024292
Model None Epoch 3 Batch 161: Loss 1.379116415977478
Model None Epoch 3 Batch 162: Loss 1.4730802774429321
Model None Epoch 3 Batch 163: Loss 1.4327117204666138
Model None Epoch 3 Batch 164: Loss 1.4269335269927979
Model None Epoch 3 Batch 165: Loss 1.556168794631958
Model None Epoch 3 Batch 166: Loss 1.5298629999160767
Model None Epoch 3 Batch 167: Loss 1.4759422540664673
Model None Epoch 3 Batch 168: Loss 1.4739009141921997
Model None Epoch 3 Batch 169: Loss 1.5045661926269531
Model None Epoch 3 Batch 170: Loss 1.5077409744262695
Model None Epoch 3 Batch 171: Loss 1.4637004137039185
Model None Epoch 3 Batch 172: Loss 1.4339481592178345
Model None Epoch 3 Batch 173: Loss 1.531783103942871
Model None Epoch 3 Batch 174: Loss 1.5080212354660034
Model None Epoch 3 Batch 175: Loss 1.5276365280151367
Model None Epoch 3 Batch 176: Loss 1.515833854675293
Model None Epoch 3 Batch 177: Loss 1.5714436769485474
Model None Epoch 3 Batch 178: Loss 1.4635602235794067
Model None Epoch 3 Batch 179: Loss 1.3773245811462402
Model None Epoch 3 Batch 180: Loss 1.4378730058670044
Model None Epoch 3 Batch 181: Loss 1.496249794960022
Model None Epoch 3 Batch 182: Loss 1.5590872764587402
Model None Epoch 3 Batch 183: Loss 1.5132158994674683
Model None Epoch 3 Batch 184: Loss 1.5470243692398071
Model None Epoch 3 Batch 185: Loss 1.5074859857559204
Model None Epoch 3 Batch 186: Loss 1.4653517007827759
Model None Epoch 3 Batch 187: Loss 1.4481585025787354
Model None Epoch 3 Batch 188: Loss 1.4784027338027954
Model None Epoch 3 Batch 189: Loss 1.4363510608673096
Model None Epoch 3 Batch 190: Loss 1.4819313287734985
Model None Epoch 3 Batch 191: Loss 1.5488803386688232
Model None Epoch 3 Batch 192: Loss 1.5160435438156128
Model None Epoch 3 Batch 193: Loss 1.5766295194625854
Model None Epoch 3 Batch 194: Loss 1.6096322536468506
Model None Epoch 3 Batch 195: Loss 1.6134341955184937

 Downstream Train loss: 1.5051061991526156 Acc: 0.5144
Downstream Train Epoch: 4 [0/50000 (0%)]	Loss: 1.454437
Model None Epoch 4 Batch 0: Loss 1.4544365406036377
Model None Epoch 4 Batch 1: Loss 1.5355007648468018
Model None Epoch 4 Batch 2: Loss 1.4633609056472778
Model None Epoch 4 Batch 3: Loss 1.4894522428512573
Model None Epoch 4 Batch 4: Loss 1.4493787288665771
Model None Epoch 4 Batch 5: Loss 1.6490693092346191
Model None Epoch 4 Batch 6: Loss 1.4391902685165405
Model None Epoch 4 Batch 7: Loss 1.5169585943222046
Model None Epoch 4 Batch 8: Loss 1.4883896112442017
Model None Epoch 4 Batch 9: Loss 1.5325872898101807
Model None Epoch 4 Batch 10: Loss 1.4452499151229858
Model None Epoch 4 Batch 11: Loss 1.502549171447754
Model None Epoch 4 Batch 12: Loss 1.5299967527389526
Model None Epoch 4 Batch 13: Loss 1.512224793434143
Model None Epoch 4 Batch 14: Loss 1.5931371450424194
Model None Epoch 4 Batch 15: Loss 1.5231159925460815
Model None Epoch 4 Batch 16: Loss 1.5676463842391968
Model None Epoch 4 Batch 17: Loss 1.5351665019989014
Model None Epoch 4 Batch 18: Loss 1.390136480331421
Model None Epoch 4 Batch 19: Loss 1.3740242719650269
Model None Epoch 4 Batch 20: Loss 1.5651048421859741
Model None Epoch 4 Batch 21: Loss 1.5194611549377441
Model None Epoch 4 Batch 22: Loss 1.485002040863037
Model None Epoch 4 Batch 23: Loss 1.5022891759872437
Model None Epoch 4 Batch 24: Loss 1.6161425113677979
Model None Epoch 4 Batch 25: Loss 1.6247689723968506
Model None Epoch 4 Batch 26: Loss 1.5918883085250854
Model None Epoch 4 Batch 27: Loss 1.4496549367904663
Model None Epoch 4 Batch 28: Loss 1.526366949081421
Model None Epoch 4 Batch 29: Loss 1.3943121433258057
Model None Epoch 4 Batch 30: Loss 1.5038670301437378
Model None Epoch 4 Batch 31: Loss 1.4467734098434448
Model None Epoch 4 Batch 32: Loss 1.4536651372909546
Model None Epoch 4 Batch 33: Loss 1.4231135845184326
Model None Epoch 4 Batch 34: Loss 1.4854165315628052
Model None Epoch 4 Batch 35: Loss 1.4490673542022705
Model None Epoch 4 Batch 36: Loss 1.3862483501434326
Model None Epoch 4 Batch 37: Loss 1.6429953575134277
Model None Epoch 4 Batch 38: Loss 1.4294980764389038
Model None Epoch 4 Batch 39: Loss 1.5974756479263306
Model None Epoch 4 Batch 40: Loss 1.4421714544296265
Model None Epoch 4 Batch 41: Loss 1.5586656332015991
Model None Epoch 4 Batch 42: Loss 1.394371509552002
Model None Epoch 4 Batch 43: Loss 1.615151047706604
Model None Epoch 4 Batch 44: Loss 1.44290030002594
Model None Epoch 4 Batch 45: Loss 1.4804452657699585
Model None Epoch 4 Batch 46: Loss 1.6204558610916138
Model None Epoch 4 Batch 47: Loss 1.5875000953674316
Model None Epoch 4 Batch 48: Loss 1.5200706720352173
Model None Epoch 4 Batch 49: Loss 1.5385233163833618
Downstream Train Epoch: 4 [12800/50000 (26%)]	Loss: 1.406186
Model None Epoch 4 Batch 50: Loss 1.4061856269836426
Model None Epoch 4 Batch 51: Loss 1.4898812770843506
Model None Epoch 4 Batch 52: Loss 1.419923186302185
Model None Epoch 4 Batch 53: Loss 1.5745069980621338
Model None Epoch 4 Batch 54: Loss 1.4359776973724365
Model None Epoch 4 Batch 55: Loss 1.4336451292037964
Model None Epoch 4 Batch 56: Loss 1.514734148979187
Model None Epoch 4 Batch 57: Loss 1.4874378442764282
Model None Epoch 4 Batch 58: Loss 1.5509153604507446
Model None Epoch 4 Batch 59: Loss 1.5093979835510254
Model None Epoch 4 Batch 60: Loss 1.376121997833252
Model None Epoch 4 Batch 61: Loss 1.4885493516921997
Model None Epoch 4 Batch 62: Loss 1.4295471906661987
Model None Epoch 4 Batch 63: Loss 1.4963972568511963
Model None Epoch 4 Batch 64: Loss 1.4510658979415894
Model None Epoch 4 Batch 65: Loss 1.5287262201309204
Model None Epoch 4 Batch 66: Loss 1.5075550079345703
Model None Epoch 4 Batch 67: Loss 1.3856927156448364
Model None Epoch 4 Batch 68: Loss 1.4555031061172485
Model None Epoch 4 Batch 69: Loss 1.4296112060546875
Model None Epoch 4 Batch 70: Loss 1.5985519886016846
Model None Epoch 4 Batch 71: Loss 1.4369624853134155
Model None Epoch 4 Batch 72: Loss 1.3845161199569702
Model None Epoch 4 Batch 73: Loss 1.5448124408721924
Model None Epoch 4 Batch 74: Loss 1.5238003730773926
Model None Epoch 4 Batch 75: Loss 1.536862850189209
Model None Epoch 4 Batch 76: Loss 1.4116359949111938
Model None Epoch 4 Batch 77: Loss 1.5330009460449219
Model None Epoch 4 Batch 78: Loss 1.5063658952713013
Model None Epoch 4 Batch 79: Loss 1.5411901473999023
Model None Epoch 4 Batch 80: Loss 1.4001593589782715
Model None Epoch 4 Batch 81: Loss 1.5009883642196655
Model None Epoch 4 Batch 82: Loss 1.5248849391937256
Model None Epoch 4 Batch 83: Loss 1.421064019203186
Model None Epoch 4 Batch 84: Loss 1.4849177598953247
Model None Epoch 4 Batch 85: Loss 1.4866935014724731
Model None Epoch 4 Batch 86: Loss 1.4658727645874023
Model None Epoch 4 Batch 87: Loss 1.492464542388916
Model None Epoch 4 Batch 88: Loss 1.5456974506378174
Model None Epoch 4 Batch 89: Loss 1.424938440322876
Model None Epoch 4 Batch 90: Loss 1.4586966037750244
Model None Epoch 4 Batch 91: Loss 1.4847031831741333
Model None Epoch 4 Batch 92: Loss 1.5387375354766846
Model None Epoch 4 Batch 93: Loss 1.4486440420150757
Model None Epoch 4 Batch 94: Loss 1.5802791118621826
Model None Epoch 4 Batch 95: Loss 1.5696783065795898
Model None Epoch 4 Batch 96: Loss 1.4868313074111938
Model None Epoch 4 Batch 97: Loss 1.590860366821289
Model None Epoch 4 Batch 98: Loss 1.512393593788147
Model None Epoch 4 Batch 99: Loss 1.570603370666504
Downstream Train Epoch: 4 [25600/50000 (51%)]	Loss: 1.430833
Model None Epoch 4 Batch 100: Loss 1.4308329820632935
Model None Epoch 4 Batch 101: Loss 1.4590802192687988
Model None Epoch 4 Batch 102: Loss 1.3905466794967651
Model None Epoch 4 Batch 103: Loss 1.5162822008132935
Model None Epoch 4 Batch 104: Loss 1.4604706764221191
Model None Epoch 4 Batch 105: Loss 1.3831982612609863
Model None Epoch 4 Batch 106: Loss 1.3085395097732544
Model None Epoch 4 Batch 107: Loss 1.5069078207015991
Model None Epoch 4 Batch 108: Loss 1.4668214321136475
Model None Epoch 4 Batch 109: Loss 1.5496463775634766
Model None Epoch 4 Batch 110: Loss 1.4990257024765015
Model None Epoch 4 Batch 111: Loss 1.5362931489944458
Model None Epoch 4 Batch 112: Loss 1.551192045211792
Model None Epoch 4 Batch 113: Loss 1.50498366355896
Model None Epoch 4 Batch 114: Loss 1.5360218286514282
Model None Epoch 4 Batch 115: Loss 1.5082018375396729
Model None Epoch 4 Batch 116: Loss 1.3564906120300293
Model None Epoch 4 Batch 117: Loss 1.4231301546096802
Model None Epoch 4 Batch 118: Loss 1.5104268789291382
Model None Epoch 4 Batch 119: Loss 1.4863901138305664
Model None Epoch 4 Batch 120: Loss 1.473151445388794
Model None Epoch 4 Batch 121: Loss 1.6436293125152588
Model None Epoch 4 Batch 122: Loss 1.4220006465911865
Model None Epoch 4 Batch 123: Loss 1.419892430305481
Model None Epoch 4 Batch 124: Loss 1.5503172874450684
Model None Epoch 4 Batch 125: Loss 1.447912335395813
Model None Epoch 4 Batch 126: Loss 1.4772812128067017
Model None Epoch 4 Batch 127: Loss 1.4842782020568848
Model None Epoch 4 Batch 128: Loss 1.5191268920898438
Model None Epoch 4 Batch 129: Loss 1.4356274604797363
Model None Epoch 4 Batch 130: Loss 1.3790701627731323
Model None Epoch 4 Batch 131: Loss 1.504970908164978
Model None Epoch 4 Batch 132: Loss 1.485338568687439
Model None Epoch 4 Batch 133: Loss 1.6629916429519653
Model None Epoch 4 Batch 134: Loss 1.3993010520935059
Model None Epoch 4 Batch 135: Loss 1.4949876070022583
Model None Epoch 4 Batch 136: Loss 1.5008587837219238
Model None Epoch 4 Batch 137: Loss 1.5266164541244507
Model None Epoch 4 Batch 138: Loss 1.5678008794784546
Model None Epoch 4 Batch 139: Loss 1.4462851285934448
Model None Epoch 4 Batch 140: Loss 1.5484373569488525
Model None Epoch 4 Batch 141: Loss 1.4521058797836304
Model None Epoch 4 Batch 142: Loss 1.4513201713562012
Model None Epoch 4 Batch 143: Loss 1.5127160549163818
Model None Epoch 4 Batch 144: Loss 1.4404244422912598
Model None Epoch 4 Batch 145: Loss 1.5357766151428223
Model None Epoch 4 Batch 146: Loss 1.580988883972168
Model None Epoch 4 Batch 147: Loss 1.4075913429260254
Model None Epoch 4 Batch 148: Loss 1.5616812705993652
Model None Epoch 4 Batch 149: Loss 1.473922848701477
Downstream Train Epoch: 4 [38400/50000 (77%)]	Loss: 1.447307
Model None Epoch 4 Batch 150: Loss 1.447306752204895
Model None Epoch 4 Batch 151: Loss 1.490543007850647
Model None Epoch 4 Batch 152: Loss 1.461838960647583
Model None Epoch 4 Batch 153: Loss 1.3925365209579468
Model None Epoch 4 Batch 154: Loss 1.4685335159301758
Model None Epoch 4 Batch 155: Loss 1.5227735042572021
Model None Epoch 4 Batch 156: Loss 1.5068461894989014
Model None Epoch 4 Batch 157: Loss 1.5718894004821777
Model None Epoch 4 Batch 158: Loss 1.4495526552200317
Model None Epoch 4 Batch 159: Loss 1.506859302520752
Model None Epoch 4 Batch 160: Loss 1.4426259994506836
Model None Epoch 4 Batch 161: Loss 1.475058674812317
Model None Epoch 4 Batch 162: Loss 1.586159348487854
Model None Epoch 4 Batch 163: Loss 1.617411494255066
Model None Epoch 4 Batch 164: Loss 1.6106140613555908
Model None Epoch 4 Batch 165: Loss 1.479132056236267
Model None Epoch 4 Batch 166: Loss 1.492655873298645
Model None Epoch 4 Batch 167: Loss 1.5169376134872437
Model None Epoch 4 Batch 168: Loss 1.473429560661316
Model None Epoch 4 Batch 169: Loss 1.5229277610778809
Model None Epoch 4 Batch 170: Loss 1.4029549360275269
Model None Epoch 4 Batch 171: Loss 1.5151888132095337
Model None Epoch 4 Batch 172: Loss 1.5231074094772339
Model None Epoch 4 Batch 173: Loss 1.4687330722808838
Model None Epoch 4 Batch 174: Loss 1.4829297065734863
Model None Epoch 4 Batch 175: Loss 1.506826639175415
Model None Epoch 4 Batch 176: Loss 1.4172438383102417
Model None Epoch 4 Batch 177: Loss 1.4525794982910156
Model None Epoch 4 Batch 178: Loss 1.6927217245101929
Model None Epoch 4 Batch 179: Loss 1.5220773220062256
Model None Epoch 4 Batch 180: Loss 1.5867702960968018
Model None Epoch 4 Batch 181: Loss 1.5602385997772217
Model None Epoch 4 Batch 182: Loss 1.420788049697876
Model None Epoch 4 Batch 183: Loss 1.4616461992263794
Model None Epoch 4 Batch 184: Loss 1.4219943284988403
Model None Epoch 4 Batch 185: Loss 1.4678984880447388
Model None Epoch 4 Batch 186: Loss 1.413510799407959
Model None Epoch 4 Batch 187: Loss 1.4866927862167358
Model None Epoch 4 Batch 188: Loss 1.5473469495773315
Model None Epoch 4 Batch 189: Loss 1.4988900423049927
Model None Epoch 4 Batch 190: Loss 1.4676361083984375
Model None Epoch 4 Batch 191: Loss 1.4404921531677246
Model None Epoch 4 Batch 192: Loss 1.4132812023162842
Model None Epoch 4 Batch 193: Loss 1.3971558809280396
Model None Epoch 4 Batch 194: Loss 1.4139292240142822
Model None Epoch 4 Batch 195: Loss 1.5056565999984741

 Downstream Train loss: 1.4917241310586735 Acc: 0.5233
Downstream Train Epoch: 5 [0/50000 (0%)]	Loss: 1.476713
Model None Epoch 5 Batch 0: Loss 1.4767132997512817
Model None Epoch 5 Batch 1: Loss 1.4540494680404663
Model None Epoch 5 Batch 2: Loss 1.5449835062026978
Model None Epoch 5 Batch 3: Loss 1.5494087934494019
Model None Epoch 5 Batch 4: Loss 1.5668483972549438
Model None Epoch 5 Batch 5: Loss 1.4992350339889526
Model None Epoch 5 Batch 6: Loss 1.4043152332305908
Model None Epoch 5 Batch 7: Loss 1.5115278959274292
Model None Epoch 5 Batch 8: Loss 1.3859878778457642
Model None Epoch 5 Batch 9: Loss 1.4530251026153564
Model None Epoch 5 Batch 10: Loss 1.5029878616333008
Model None Epoch 5 Batch 11: Loss 1.4725341796875
Model None Epoch 5 Batch 12: Loss 1.5573853254318237
Model None Epoch 5 Batch 13: Loss 1.4450840950012207
Model None Epoch 5 Batch 14: Loss 1.524746298789978
Model None Epoch 5 Batch 15: Loss 1.3869178295135498
Model None Epoch 5 Batch 16: Loss 1.5266749858856201
Model None Epoch 5 Batch 17: Loss 1.5318214893341064
Model None Epoch 5 Batch 18: Loss 1.426322102546692
Model None Epoch 5 Batch 19: Loss 1.4185885190963745
Model None Epoch 5 Batch 20: Loss 1.5351707935333252
Model None Epoch 5 Batch 21: Loss 1.4750523567199707
Model None Epoch 5 Batch 22: Loss 1.579575538635254
Model None Epoch 5 Batch 23: Loss 1.403088092803955
Model None Epoch 5 Batch 24: Loss 1.4912155866622925
Model None Epoch 5 Batch 25: Loss 1.5780034065246582
Model None Epoch 5 Batch 26: Loss 1.539718747138977
Model None Epoch 5 Batch 27: Loss 1.5305144786834717
Model None Epoch 5 Batch 28: Loss 1.41742742061615
Model None Epoch 5 Batch 29: Loss 1.5571093559265137
Model None Epoch 5 Batch 30: Loss 1.4988658428192139
Model None Epoch 5 Batch 31: Loss 1.405788779258728
Model None Epoch 5 Batch 32: Loss 1.4175755977630615
Model None Epoch 5 Batch 33: Loss 1.4240639209747314
Model None Epoch 5 Batch 34: Loss 1.413621187210083
Model None Epoch 5 Batch 35: Loss 1.4105489253997803
Model None Epoch 5 Batch 36: Loss 1.4681768417358398
Model None Epoch 5 Batch 37: Loss 1.3334681987762451
Model None Epoch 5 Batch 38: Loss 1.4140585660934448
Model None Epoch 5 Batch 39: Loss 1.4839199781417847
Model None Epoch 5 Batch 40: Loss 1.3982480764389038
Model None Epoch 5 Batch 41: Loss 1.536611557006836
Model None Epoch 5 Batch 42: Loss 1.5509403944015503
Model None Epoch 5 Batch 43: Loss 1.422041893005371
Model None Epoch 5 Batch 44: Loss 1.3758976459503174
Model None Epoch 5 Batch 45: Loss 1.4920434951782227
Model None Epoch 5 Batch 46: Loss 1.5178548097610474
Model None Epoch 5 Batch 47: Loss 1.3326653242111206
Model None Epoch 5 Batch 48: Loss 1.4564803838729858
Model None Epoch 5 Batch 49: Loss 1.511443018913269
Downstream Train Epoch: 5 [12800/50000 (26%)]	Loss: 1.547176
Model None Epoch 5 Batch 50: Loss 1.547175645828247
Model None Epoch 5 Batch 51: Loss 1.5080631971359253
Model None Epoch 5 Batch 52: Loss 1.4104121923446655
Model None Epoch 5 Batch 53: Loss 1.3982585668563843
Model None Epoch 5 Batch 54: Loss 1.446823239326477
Model None Epoch 5 Batch 55: Loss 1.5466371774673462
Model None Epoch 5 Batch 56: Loss 1.490372657775879
Model None Epoch 5 Batch 57: Loss 1.516041874885559
Model None Epoch 5 Batch 58: Loss 1.3991316556930542
Model None Epoch 5 Batch 59: Loss 1.6343138217926025
Model None Epoch 5 Batch 60: Loss 1.490662932395935
Model None Epoch 5 Batch 61: Loss 1.3750897645950317
Model None Epoch 5 Batch 62: Loss 1.464148998260498
Model None Epoch 5 Batch 63: Loss 1.4364265203475952
Model None Epoch 5 Batch 64: Loss 1.580889105796814
Model None Epoch 5 Batch 65: Loss 1.563120722770691
Model None Epoch 5 Batch 66: Loss 1.5615079402923584
Model None Epoch 5 Batch 67: Loss 1.4508309364318848
Model None Epoch 5 Batch 68: Loss 1.486189365386963
Model None Epoch 5 Batch 69: Loss 1.478041172027588
Model None Epoch 5 Batch 70: Loss 1.460381031036377
Model None Epoch 5 Batch 71: Loss 1.4921038150787354
Model None Epoch 5 Batch 72: Loss 1.395158290863037
Model None Epoch 5 Batch 73: Loss 1.493249773979187
Model None Epoch 5 Batch 74: Loss 1.4560918807983398
Model None Epoch 5 Batch 75: Loss 1.4042145013809204
Model None Epoch 5 Batch 76: Loss 1.5300612449645996
Model None Epoch 5 Batch 77: Loss 1.4962620735168457
Model None Epoch 5 Batch 78: Loss 1.508095383644104
Model None Epoch 5 Batch 79: Loss 1.2919716835021973
Model None Epoch 5 Batch 80: Loss 1.4289082288742065
Model None Epoch 5 Batch 81: Loss 1.5575939416885376
Model None Epoch 5 Batch 82: Loss 1.4745073318481445
Model None Epoch 5 Batch 83: Loss 1.5020805597305298
Model None Epoch 5 Batch 84: Loss 1.4675740003585815
Model None Epoch 5 Batch 85: Loss 1.5289300680160522
Model None Epoch 5 Batch 86: Loss 1.509461760520935
Model None Epoch 5 Batch 87: Loss 1.504813313484192
Model None Epoch 5 Batch 88: Loss 1.6093480587005615
Model None Epoch 5 Batch 89: Loss 1.4198768138885498
Model None Epoch 5 Batch 90: Loss 1.3813598155975342
Model None Epoch 5 Batch 91: Loss 1.3780196905136108
Model None Epoch 5 Batch 92: Loss 1.4981578588485718
Model None Epoch 5 Batch 93: Loss 1.5556750297546387
Model None Epoch 5 Batch 94: Loss 1.4090782403945923
Model None Epoch 5 Batch 95: Loss 1.4869905710220337
Model None Epoch 5 Batch 96: Loss 1.5293749570846558
Model None Epoch 5 Batch 97: Loss 1.5142908096313477
Model None Epoch 5 Batch 98: Loss 1.493905782699585
Model None Epoch 5 Batch 99: Loss 1.5071074962615967
Downstream Train Epoch: 5 [25600/50000 (51%)]	Loss: 1.371801
Model None Epoch 5 Batch 100: Loss 1.3718006610870361
Model None Epoch 5 Batch 101: Loss 1.360994577407837
Model None Epoch 5 Batch 102: Loss 1.5196552276611328
Model None Epoch 5 Batch 103: Loss 1.4466129541397095
Model None Epoch 5 Batch 104: Loss 1.5355345010757446
Model None Epoch 5 Batch 105: Loss 1.5437815189361572
Model None Epoch 5 Batch 106: Loss 1.5215158462524414
Model None Epoch 5 Batch 107: Loss 1.4967005252838135
Model None Epoch 5 Batch 108: Loss 1.463340163230896
Model None Epoch 5 Batch 109: Loss 1.5432603359222412
Model None Epoch 5 Batch 110: Loss 1.5009740591049194
Model None Epoch 5 Batch 111: Loss 1.393717646598816
Model None Epoch 5 Batch 112: Loss 1.4854538440704346
Model None Epoch 5 Batch 113: Loss 1.410346508026123
Model None Epoch 5 Batch 114: Loss 1.5439754724502563
Model None Epoch 5 Batch 115: Loss 1.6796455383300781
Model None Epoch 5 Batch 116: Loss 1.632785677909851
Model None Epoch 5 Batch 117: Loss 1.5225714445114136
Model None Epoch 5 Batch 118: Loss 1.4680925607681274
Model None Epoch 5 Batch 119: Loss 1.605570673942566
Model None Epoch 5 Batch 120: Loss 1.4880306720733643
Model None Epoch 5 Batch 121: Loss 1.4980934858322144
Model None Epoch 5 Batch 122: Loss 1.468489170074463
Model None Epoch 5 Batch 123: Loss 1.471924901008606
Model None Epoch 5 Batch 124: Loss 1.511919379234314
Model None Epoch 5 Batch 125: Loss 1.3942341804504395
Model None Epoch 5 Batch 126: Loss 1.4394850730895996
Model None Epoch 5 Batch 127: Loss 1.5607244968414307
Model None Epoch 5 Batch 128: Loss 1.400760531425476
Model None Epoch 5 Batch 129: Loss 1.4048771858215332
Model None Epoch 5 Batch 130: Loss 1.4459116458892822
Model None Epoch 5 Batch 131: Loss 1.4461147785186768
Model None Epoch 5 Batch 132: Loss 1.3996278047561646
Model None Epoch 5 Batch 133: Loss 1.4246268272399902
Model None Epoch 5 Batch 134: Loss 1.5223970413208008
Model None Epoch 5 Batch 135: Loss 1.5339405536651611
Model None Epoch 5 Batch 136: Loss 1.4420346021652222
Model None Epoch 5 Batch 137: Loss 1.4578582048416138
Model None Epoch 5 Batch 138: Loss 1.4252846240997314
Model None Epoch 5 Batch 139: Loss 1.499908685684204
Model None Epoch 5 Batch 140: Loss 1.5870397090911865
Model None Epoch 5 Batch 141: Loss 1.4243654012680054
Model None Epoch 5 Batch 142: Loss 1.415483832359314
Model None Epoch 5 Batch 143: Loss 1.6412568092346191
Model None Epoch 5 Batch 144: Loss 1.4196408987045288
Model None Epoch 5 Batch 145: Loss 1.5407679080963135
Model None Epoch 5 Batch 146: Loss 1.3967342376708984
Model None Epoch 5 Batch 147: Loss 1.4505802392959595
Model None Epoch 5 Batch 148: Loss 1.4242572784423828
Model None Epoch 5 Batch 149: Loss 1.5003970861434937
Downstream Train Epoch: 5 [38400/50000 (77%)]	Loss: 1.362974
Model None Epoch 5 Batch 150: Loss 1.362973690032959
Model None Epoch 5 Batch 151: Loss 1.5542694330215454
Model None Epoch 5 Batch 152: Loss 1.5003231763839722
Model None Epoch 5 Batch 153: Loss 1.4645254611968994
Model None Epoch 5 Batch 154: Loss 1.510008692741394
Model None Epoch 5 Batch 155: Loss 1.3850945234298706
Model None Epoch 5 Batch 156: Loss 1.474554419517517
Model None Epoch 5 Batch 157: Loss 1.4989756345748901
Model None Epoch 5 Batch 158: Loss 1.4226469993591309
Model None Epoch 5 Batch 159: Loss 1.5249029397964478
Model None Epoch 5 Batch 160: Loss 1.5645931959152222
Model None Epoch 5 Batch 161: Loss 1.4307340383529663
Model None Epoch 5 Batch 162: Loss 1.5160820484161377
Model None Epoch 5 Batch 163: Loss 1.5188288688659668
Model None Epoch 5 Batch 164: Loss 1.5013433694839478
Model None Epoch 5 Batch 165: Loss 1.3037763833999634
Model None Epoch 5 Batch 166: Loss 1.5707021951675415
Model None Epoch 5 Batch 167: Loss 1.5268340110778809
Model None Epoch 5 Batch 168: Loss 1.4757773876190186
Model None Epoch 5 Batch 169: Loss 1.488606572151184
Model None Epoch 5 Batch 170: Loss 1.4972972869873047
Model None Epoch 5 Batch 171: Loss 1.463268756866455
Model None Epoch 5 Batch 172: Loss 1.4749943017959595
Model None Epoch 5 Batch 173: Loss 1.4935020208358765
Model None Epoch 5 Batch 174: Loss 1.5077714920043945
Model None Epoch 5 Batch 175: Loss 1.4509423971176147
Model None Epoch 5 Batch 176: Loss 1.561549186706543
Model None Epoch 5 Batch 177: Loss 1.5127006769180298
Model None Epoch 5 Batch 178: Loss 1.3128384351730347
Model None Epoch 5 Batch 179: Loss 1.4831229448318481
Model None Epoch 5 Batch 180: Loss 1.4193038940429688
Model None Epoch 5 Batch 181: Loss 1.3439276218414307
Model None Epoch 5 Batch 182: Loss 1.4055010080337524
Model None Epoch 5 Batch 183: Loss 1.4259753227233887
Model None Epoch 5 Batch 184: Loss 1.5748538970947266
Model None Epoch 5 Batch 185: Loss 1.5696656703948975
Model None Epoch 5 Batch 186: Loss 1.5102851390838623
Model None Epoch 5 Batch 187: Loss 1.4298440217971802
Model None Epoch 5 Batch 188: Loss 1.4650694131851196
Model None Epoch 5 Batch 189: Loss 1.4336073398590088
Model None Epoch 5 Batch 190: Loss 1.4016342163085938
Model None Epoch 5 Batch 191: Loss 1.4191033840179443
Model None Epoch 5 Batch 192: Loss 1.436288595199585
Model None Epoch 5 Batch 193: Loss 1.451045274734497
Model None Epoch 5 Batch 194: Loss 1.5209664106369019
Model None Epoch 5 Batch 195: Loss 1.7355310916900635

 Downstream Train loss: 1.4778488205403697 Acc: 0.5306
Downstream Train Epoch: 6 [0/50000 (0%)]	Loss: 1.462613
Model None Epoch 6 Batch 0: Loss 1.4626128673553467
Model None Epoch 6 Batch 1: Loss 1.4124839305877686
Model None Epoch 6 Batch 2: Loss 1.4715536832809448
Model None Epoch 6 Batch 3: Loss 1.5820376873016357
Model None Epoch 6 Batch 4: Loss 1.5058846473693848
Model None Epoch 6 Batch 5: Loss 1.3756353855133057
Model None Epoch 6 Batch 6: Loss 1.4949215650558472
Model None Epoch 6 Batch 7: Loss 1.4873961210250854
Model None Epoch 6 Batch 8: Loss 1.5450563430786133
Model None Epoch 6 Batch 9: Loss 1.5625550746917725
Model None Epoch 6 Batch 10: Loss 1.4249155521392822
Model None Epoch 6 Batch 11: Loss 1.4362242221832275
Model None Epoch 6 Batch 12: Loss 1.4770972728729248
Model None Epoch 6 Batch 13: Loss 1.4216883182525635
Model None Epoch 6 Batch 14: Loss 1.5463719367980957
Model None Epoch 6 Batch 15: Loss 1.4696649312973022
Model None Epoch 6 Batch 16: Loss 1.49248468875885
Model None Epoch 6 Batch 17: Loss 1.4004456996917725
Model None Epoch 6 Batch 18: Loss 1.3860684633255005
Model None Epoch 6 Batch 19: Loss 1.5493850708007812
Model None Epoch 6 Batch 20: Loss 1.3268502950668335
Model None Epoch 6 Batch 21: Loss 1.4797428846359253
Model None Epoch 6 Batch 22: Loss 1.4278489351272583
Model None Epoch 6 Batch 23: Loss 1.4876043796539307
Model None Epoch 6 Batch 24: Loss 1.4180469512939453
Model None Epoch 6 Batch 25: Loss 1.3902629613876343
Model None Epoch 6 Batch 26: Loss 1.4569525718688965
Model None Epoch 6 Batch 27: Loss 1.5196130275726318
Model None Epoch 6 Batch 28: Loss 1.4712066650390625
Model None Epoch 6 Batch 29: Loss 1.4938342571258545
Model None Epoch 6 Batch 30: Loss 1.4224767684936523
Model None Epoch 6 Batch 31: Loss 1.3524901866912842
Model None Epoch 6 Batch 32: Loss 1.4755315780639648
Model None Epoch 6 Batch 33: Loss 1.480773687362671
Model None Epoch 6 Batch 34: Loss 1.5466727018356323
Model None Epoch 6 Batch 35: Loss 1.371848702430725
Model None Epoch 6 Batch 36: Loss 1.4674040079116821
Model None Epoch 6 Batch 37: Loss 1.513745903968811
Model None Epoch 6 Batch 38: Loss 1.5246163606643677
Model None Epoch 6 Batch 39: Loss 1.5658576488494873
Model None Epoch 6 Batch 40: Loss 1.3614776134490967
Model None Epoch 6 Batch 41: Loss 1.5973745584487915
Model None Epoch 6 Batch 42: Loss 1.5901274681091309
Model None Epoch 6 Batch 43: Loss 1.4171457290649414
Model None Epoch 6 Batch 44: Loss 1.43678879737854
Model None Epoch 6 Batch 45: Loss 1.4391247034072876
Model None Epoch 6 Batch 46: Loss 1.4039967060089111
Model None Epoch 6 Batch 47: Loss 1.4774081707000732
Model None Epoch 6 Batch 48: Loss 1.4335016012191772
Model None Epoch 6 Batch 49: Loss 1.4753005504608154
Downstream Train Epoch: 6 [12800/50000 (26%)]	Loss: 1.480525
Model None Epoch 6 Batch 50: Loss 1.4805254936218262
Model None Epoch 6 Batch 51: Loss 1.4774525165557861
Model None Epoch 6 Batch 52: Loss 1.3272180557250977
Model None Epoch 6 Batch 53: Loss 1.3933906555175781
Model None Epoch 6 Batch 54: Loss 1.3976548910140991
Model None Epoch 6 Batch 55: Loss 1.5580989122390747
Model None Epoch 6 Batch 56: Loss 1.5561866760253906
Model None Epoch 6 Batch 57: Loss 1.563952922821045
Model None Epoch 6 Batch 58: Loss 1.4973663091659546
Model None Epoch 6 Batch 59: Loss 1.484137773513794
Model None Epoch 6 Batch 60: Loss 1.4328402280807495
Model None Epoch 6 Batch 61: Loss 1.512759804725647
Model None Epoch 6 Batch 62: Loss 1.4498363733291626
Model None Epoch 6 Batch 63: Loss 1.5015912055969238
Model None Epoch 6 Batch 64: Loss 1.364999532699585
Model None Epoch 6 Batch 65: Loss 1.4942677021026611
Model None Epoch 6 Batch 66: Loss 1.4693351984024048
Model None Epoch 6 Batch 67: Loss 1.6474257707595825
Model None Epoch 6 Batch 68: Loss 1.355905294418335
Model None Epoch 6 Batch 69: Loss 1.4885388612747192
Model None Epoch 6 Batch 70: Loss 1.3939448595046997
Model None Epoch 6 Batch 71: Loss 1.4628050327301025
Model None Epoch 6 Batch 72: Loss 1.4958704710006714
Model None Epoch 6 Batch 73: Loss 1.545103907585144
Model None Epoch 6 Batch 74: Loss 1.5158624649047852
Model None Epoch 6 Batch 75: Loss 1.403067708015442
Model None Epoch 6 Batch 76: Loss 1.512939214706421
Model None Epoch 6 Batch 77: Loss 1.4917956590652466
Model None Epoch 6 Batch 78: Loss 1.4834647178649902
Model None Epoch 6 Batch 79: Loss 1.430003046989441
Model None Epoch 6 Batch 80: Loss 1.4784644842147827
Model None Epoch 6 Batch 81: Loss 1.4703326225280762
Model None Epoch 6 Batch 82: Loss 1.56281316280365
Model None Epoch 6 Batch 83: Loss 1.3565804958343506
Model None Epoch 6 Batch 84: Loss 1.5327314138412476
Model None Epoch 6 Batch 85: Loss 1.373871088027954
Model None Epoch 6 Batch 86: Loss 1.4951585531234741
Model None Epoch 6 Batch 87: Loss 1.4455634355545044
Model None Epoch 6 Batch 88: Loss 1.4066705703735352
Model None Epoch 6 Batch 89: Loss 1.3836544752120972
Model None Epoch 6 Batch 90: Loss 1.454163908958435
Model None Epoch 6 Batch 91: Loss 1.4704103469848633
Model None Epoch 6 Batch 92: Loss 1.4909837245941162
Model None Epoch 6 Batch 93: Loss 1.4369856119155884
Model None Epoch 6 Batch 94: Loss 1.415497899055481
Model None Epoch 6 Batch 95: Loss 1.4892388582229614
Model None Epoch 6 Batch 96: Loss 1.4957720041275024
Model None Epoch 6 Batch 97: Loss 1.370349645614624
Model None Epoch 6 Batch 98: Loss 1.5572813749313354
Model None Epoch 6 Batch 99: Loss 1.5380557775497437
Downstream Train Epoch: 6 [25600/50000 (51%)]	Loss: 1.526247
Model None Epoch 6 Batch 100: Loss 1.5262465476989746
Model None Epoch 6 Batch 101: Loss 1.5305415391921997
Model None Epoch 6 Batch 102: Loss 1.6073850393295288
Model None Epoch 6 Batch 103: Loss 1.4781852960586548
Model None Epoch 6 Batch 104: Loss 1.49591064453125
Model None Epoch 6 Batch 105: Loss 1.5324996709823608
Model None Epoch 6 Batch 106: Loss 1.437296986579895
Model None Epoch 6 Batch 107: Loss 1.432579755783081
Model None Epoch 6 Batch 108: Loss 1.4708307981491089
Model None Epoch 6 Batch 109: Loss 1.5891302824020386
Model None Epoch 6 Batch 110: Loss 1.4769688844680786
Model None Epoch 6 Batch 111: Loss 1.3657442331314087
Model None Epoch 6 Batch 112: Loss 1.3988356590270996
Model None Epoch 6 Batch 113: Loss 1.361635446548462
Model None Epoch 6 Batch 114: Loss 1.606449007987976
Model None Epoch 6 Batch 115: Loss 1.4747464656829834
Model None Epoch 6 Batch 116: Loss 1.4241466522216797
Model None Epoch 6 Batch 117: Loss 1.5557501316070557
Model None Epoch 6 Batch 118: Loss 1.3813432455062866
Model None Epoch 6 Batch 119: Loss 1.4396377801895142
Model None Epoch 6 Batch 120: Loss 1.6686303615570068
Model None Epoch 6 Batch 121: Loss 1.5007327795028687
Model None Epoch 6 Batch 122: Loss 1.4634459018707275
Model None Epoch 6 Batch 123: Loss 1.4334595203399658
Model None Epoch 6 Batch 124: Loss 1.4864399433135986
Model None Epoch 6 Batch 125: Loss 1.4050201177597046
Model None Epoch 6 Batch 126: Loss 1.4283164739608765
Model None Epoch 6 Batch 127: Loss 1.5048459768295288
Model None Epoch 6 Batch 128: Loss 1.552697777748108
Model None Epoch 6 Batch 129: Loss 1.5057955980300903
Model None Epoch 6 Batch 130: Loss 1.4915417432785034
Model None Epoch 6 Batch 131: Loss 1.498050332069397
Model None Epoch 6 Batch 132: Loss 1.4691917896270752
Model None Epoch 6 Batch 133: Loss 1.3940658569335938
Model None Epoch 6 Batch 134: Loss 1.5613948106765747
Model None Epoch 6 Batch 135: Loss 1.4623531103134155
Model None Epoch 6 Batch 136: Loss 1.346325397491455
Model None Epoch 6 Batch 137: Loss 1.4492782354354858
Model None Epoch 6 Batch 138: Loss 1.4726295471191406
Model None Epoch 6 Batch 139: Loss 1.436992883682251
Model None Epoch 6 Batch 140: Loss 1.3189525604248047
Model None Epoch 6 Batch 141: Loss 1.4990870952606201
Model None Epoch 6 Batch 142: Loss 1.4994078874588013
Model None Epoch 6 Batch 143: Loss 1.4608838558197021
Model None Epoch 6 Batch 144: Loss 1.4196635484695435
Model None Epoch 6 Batch 145: Loss 1.5002264976501465
Model None Epoch 6 Batch 146: Loss 1.4736883640289307
Model None Epoch 6 Batch 147: Loss 1.504839301109314
Model None Epoch 6 Batch 148: Loss 1.3217636346817017
Model None Epoch 6 Batch 149: Loss 1.354178786277771
Downstream Train Epoch: 6 [38400/50000 (77%)]	Loss: 1.493802
Model None Epoch 6 Batch 150: Loss 1.4938019514083862
Model None Epoch 6 Batch 151: Loss 1.6032367944717407
Model None Epoch 6 Batch 152: Loss 1.4999525547027588
Model None Epoch 6 Batch 153: Loss 1.3460463285446167
Model None Epoch 6 Batch 154: Loss 1.458290934562683
Model None Epoch 6 Batch 155: Loss 1.5065251588821411
Model None Epoch 6 Batch 156: Loss 1.4384905099868774
Model None Epoch 6 Batch 157: Loss 1.4931436777114868
Model None Epoch 6 Batch 158: Loss 1.5341863632202148
Model None Epoch 6 Batch 159: Loss 1.5943663120269775
Model None Epoch 6 Batch 160: Loss 1.5059711933135986
Model None Epoch 6 Batch 161: Loss 1.455263614654541
Model None Epoch 6 Batch 162: Loss 1.4468960762023926
Model None Epoch 6 Batch 163: Loss 1.4250805377960205
Model None Epoch 6 Batch 164: Loss 1.5052815675735474
Model None Epoch 6 Batch 165: Loss 1.3899574279785156
Model None Epoch 6 Batch 166: Loss 1.477402687072754
Model None Epoch 6 Batch 167: Loss 1.525291085243225
Model None Epoch 6 Batch 168: Loss 1.4954158067703247
Model None Epoch 6 Batch 169: Loss 1.3800088167190552
Model None Epoch 6 Batch 170: Loss 1.4976222515106201
Model None Epoch 6 Batch 171: Loss 1.3012384176254272
Model None Epoch 6 Batch 172: Loss 1.533255696296692
Model None Epoch 6 Batch 173: Loss 1.46548593044281
Model None Epoch 6 Batch 174: Loss 1.540263056755066
Model None Epoch 6 Batch 175: Loss 1.432548999786377
Model None Epoch 6 Batch 176: Loss 1.580714225769043
Model None Epoch 6 Batch 177: Loss 1.407257080078125
Model None Epoch 6 Batch 178: Loss 1.424051284790039
Model None Epoch 6 Batch 179: Loss 1.5559881925582886
Model None Epoch 6 Batch 180: Loss 1.5164728164672852
Model None Epoch 6 Batch 181: Loss 1.4420207738876343
Model None Epoch 6 Batch 182: Loss 1.5049552917480469
Model None Epoch 6 Batch 183: Loss 1.491567611694336
Model None Epoch 6 Batch 184: Loss 1.5238494873046875
Model None Epoch 6 Batch 185: Loss 1.513946533203125
Model None Epoch 6 Batch 186: Loss 1.3952244520187378
Model None Epoch 6 Batch 187: Loss 1.5724056959152222
Model None Epoch 6 Batch 188: Loss 1.4383659362792969
Model None Epoch 6 Batch 189: Loss 1.5121514797210693
Model None Epoch 6 Batch 190: Loss 1.5045597553253174
Model None Epoch 6 Batch 191: Loss 1.365592360496521
Model None Epoch 6 Batch 192: Loss 1.483872890472412
Model None Epoch 6 Batch 193: Loss 1.3879647254943848
Model None Epoch 6 Batch 194: Loss 1.5005649328231812
Model None Epoch 6 Batch 195: Loss 1.2355879545211792

 Downstream Train loss: 1.4689537323251063 Acc: 0.5306
Downstream Train Epoch: 7 [0/50000 (0%)]	Loss: 1.526715
Model None Epoch 7 Batch 0: Loss 1.52671480178833
Model None Epoch 7 Batch 1: Loss 1.4995574951171875
Model None Epoch 7 Batch 2: Loss 1.5412852764129639
Model None Epoch 7 Batch 3: Loss 1.4038296937942505
Model None Epoch 7 Batch 4: Loss 1.4397259950637817
Model None Epoch 7 Batch 5: Loss 1.4477437734603882
Model None Epoch 7 Batch 6: Loss 1.5420604944229126
Model None Epoch 7 Batch 7: Loss 1.5376404523849487
Model None Epoch 7 Batch 8: Loss 1.3092634677886963
Model None Epoch 7 Batch 9: Loss 1.405271291732788
Model None Epoch 7 Batch 10: Loss 1.4641979932785034
Model None Epoch 7 Batch 11: Loss 1.6279478073120117
Model None Epoch 7 Batch 12: Loss 1.4705724716186523
Model None Epoch 7 Batch 13: Loss 1.4355915784835815
Model None Epoch 7 Batch 14: Loss 1.450460433959961
Model None Epoch 7 Batch 15: Loss 1.4770756959915161
Model None Epoch 7 Batch 16: Loss 1.465094804763794
Model None Epoch 7 Batch 17: Loss 1.4233819246292114
Model None Epoch 7 Batch 18: Loss 1.4983142614364624
Model None Epoch 7 Batch 19: Loss 1.4440906047821045
Model None Epoch 7 Batch 20: Loss 1.5004234313964844
Model None Epoch 7 Batch 21: Loss 1.3841031789779663
Model None Epoch 7 Batch 22: Loss 1.5414389371871948
Model None Epoch 7 Batch 23: Loss 1.344335675239563
Model None Epoch 7 Batch 24: Loss 1.458657145500183
Model None Epoch 7 Batch 25: Loss 1.4729623794555664
Model None Epoch 7 Batch 26: Loss 1.4121620655059814
Model None Epoch 7 Batch 27: Loss 1.4817286729812622
Model None Epoch 7 Batch 28: Loss 1.3756340742111206
Model None Epoch 7 Batch 29: Loss 1.489310383796692
Model None Epoch 7 Batch 30: Loss 1.5962027311325073
Model None Epoch 7 Batch 31: Loss 1.5043222904205322
Model None Epoch 7 Batch 32: Loss 1.5240570306777954
Model None Epoch 7 Batch 33: Loss 1.4915863275527954
Model None Epoch 7 Batch 34: Loss 1.4994527101516724
Model None Epoch 7 Batch 35: Loss 1.435782551765442
Model None Epoch 7 Batch 36: Loss 1.3875463008880615
Model None Epoch 7 Batch 37: Loss 1.5299293994903564
Model None Epoch 7 Batch 38: Loss 1.4025713205337524
Model None Epoch 7 Batch 39: Loss 1.449612021446228
Model None Epoch 7 Batch 40: Loss 1.478264331817627
Model None Epoch 7 Batch 41: Loss 1.4311683177947998
Model None Epoch 7 Batch 42: Loss 1.5630766153335571
Model None Epoch 7 Batch 43: Loss 1.5574147701263428
Model None Epoch 7 Batch 44: Loss 1.4579917192459106
Model None Epoch 7 Batch 45: Loss 1.4858527183532715
Model None Epoch 7 Batch 46: Loss 1.381892204284668
Model None Epoch 7 Batch 47: Loss 1.5651568174362183
Model None Epoch 7 Batch 48: Loss 1.4959006309509277
Model None Epoch 7 Batch 49: Loss 1.3940253257751465
Downstream Train Epoch: 7 [12800/50000 (26%)]	Loss: 1.429316
Model None Epoch 7 Batch 50: Loss 1.4293156862258911
Model None Epoch 7 Batch 51: Loss 1.5901668071746826
Model None Epoch 7 Batch 52: Loss 1.5319256782531738
Model None Epoch 7 Batch 53: Loss 1.4293853044509888
Model None Epoch 7 Batch 54: Loss 1.3870164155960083
Model None Epoch 7 Batch 55: Loss 1.354009985923767
Model None Epoch 7 Batch 56: Loss 1.5537129640579224
Model None Epoch 7 Batch 57: Loss 1.4459831714630127
Model None Epoch 7 Batch 58: Loss 1.4964872598648071
Model None Epoch 7 Batch 59: Loss 1.4882826805114746
Model None Epoch 7 Batch 60: Loss 1.4556931257247925
Model None Epoch 7 Batch 61: Loss 1.5278899669647217
Model None Epoch 7 Batch 62: Loss 1.377608299255371
Model None Epoch 7 Batch 63: Loss 1.519656777381897
Model None Epoch 7 Batch 64: Loss 1.493362545967102
Model None Epoch 7 Batch 65: Loss 1.4176777601242065
Model None Epoch 7 Batch 66: Loss 1.3862619400024414
Model None Epoch 7 Batch 67: Loss 1.5914950370788574
Model None Epoch 7 Batch 68: Loss 1.5063602924346924
Model None Epoch 7 Batch 69: Loss 1.369785189628601
Model None Epoch 7 Batch 70: Loss 1.581506609916687
Model None Epoch 7 Batch 71: Loss 1.418247103691101
Model None Epoch 7 Batch 72: Loss 1.3844828605651855
Model None Epoch 7 Batch 73: Loss 1.531635046005249
Model None Epoch 7 Batch 74: Loss 1.4391692876815796
Model None Epoch 7 Batch 75: Loss 1.4534673690795898
Model None Epoch 7 Batch 76: Loss 1.4700956344604492
Model None Epoch 7 Batch 77: Loss 1.4796411991119385
Model None Epoch 7 Batch 78: Loss 1.4436453580856323
Model None Epoch 7 Batch 79: Loss 1.5507041215896606
Model None Epoch 7 Batch 80: Loss 1.5531030893325806
Model None Epoch 7 Batch 81: Loss 1.5040218830108643
Model None Epoch 7 Batch 82: Loss 1.5697433948516846
Model None Epoch 7 Batch 83: Loss 1.412528395652771
Model None Epoch 7 Batch 84: Loss 1.4438737630844116
Model None Epoch 7 Batch 85: Loss 1.466444492340088
Model None Epoch 7 Batch 86: Loss 1.4272369146347046
Model None Epoch 7 Batch 87: Loss 1.530212163925171
Model None Epoch 7 Batch 88: Loss 1.4642422199249268
Model None Epoch 7 Batch 89: Loss 1.4767637252807617
Model None Epoch 7 Batch 90: Loss 1.4299695491790771
Model None Epoch 7 Batch 91: Loss 1.45241379737854
Model None Epoch 7 Batch 92: Loss 1.427842378616333
Model None Epoch 7 Batch 93: Loss 1.484081745147705
Model None Epoch 7 Batch 94: Loss 1.3358570337295532
Model None Epoch 7 Batch 95: Loss 1.6129976511001587
Model None Epoch 7 Batch 96: Loss 1.4834760427474976
Model None Epoch 7 Batch 97: Loss 1.5201464891433716
Model None Epoch 7 Batch 98: Loss 1.4352375268936157
Model None Epoch 7 Batch 99: Loss 1.438024878501892
Downstream Train Epoch: 7 [25600/50000 (51%)]	Loss: 1.425505
Model None Epoch 7 Batch 100: Loss 1.4255048036575317
Model None Epoch 7 Batch 101: Loss 1.38776695728302
Model None Epoch 7 Batch 102: Loss 1.5302518606185913
Model None Epoch 7 Batch 103: Loss 1.4712769985198975
Model None Epoch 7 Batch 104: Loss 1.4156280755996704
Model None Epoch 7 Batch 105: Loss 1.4874969720840454
Model None Epoch 7 Batch 106: Loss 1.3965426683425903
Model None Epoch 7 Batch 107: Loss 1.4503885507583618
Model None Epoch 7 Batch 108: Loss 1.3585765361785889
Model None Epoch 7 Batch 109: Loss 1.4715194702148438
Model None Epoch 7 Batch 110: Loss 1.3772097826004028
Model None Epoch 7 Batch 111: Loss 1.4856113195419312
Model None Epoch 7 Batch 112: Loss 1.4438068866729736
Model None Epoch 7 Batch 113: Loss 1.5034196376800537
Model None Epoch 7 Batch 114: Loss 1.561020016670227
Model None Epoch 7 Batch 115: Loss 1.454086184501648
Model None Epoch 7 Batch 116: Loss 1.3478556871414185
Model None Epoch 7 Batch 117: Loss 1.6062088012695312
Model None Epoch 7 Batch 118: Loss 1.3827372789382935
Model None Epoch 7 Batch 119: Loss 1.361690878868103
Model None Epoch 7 Batch 120: Loss 1.4310396909713745
Model None Epoch 7 Batch 121: Loss 1.5393368005752563
Model None Epoch 7 Batch 122: Loss 1.5463050603866577
Model None Epoch 7 Batch 123: Loss 1.5513354539871216
Model None Epoch 7 Batch 124: Loss 1.5470099449157715
Model None Epoch 7 Batch 125: Loss 1.50080406665802
Model None Epoch 7 Batch 126: Loss 1.454507827758789
Model None Epoch 7 Batch 127: Loss 1.5285125970840454
Model None Epoch 7 Batch 128: Loss 1.4199100732803345
Model None Epoch 7 Batch 129: Loss 1.5843931436538696
Model None Epoch 7 Batch 130: Loss 1.5717599391937256
Model None Epoch 7 Batch 131: Loss 1.606233835220337
Model None Epoch 7 Batch 132: Loss 1.546542763710022
Model None Epoch 7 Batch 133: Loss 1.4820497035980225
Model None Epoch 7 Batch 134: Loss 1.602712869644165
Model None Epoch 7 Batch 135: Loss 1.4973092079162598
Model None Epoch 7 Batch 136: Loss 1.4507813453674316
Model None Epoch 7 Batch 137: Loss 1.344497799873352
Model None Epoch 7 Batch 138: Loss 1.5843032598495483
Model None Epoch 7 Batch 139: Loss 1.493830680847168
Model None Epoch 7 Batch 140: Loss 1.3887677192687988
Model None Epoch 7 Batch 141: Loss 1.4655026197433472
Model None Epoch 7 Batch 142: Loss 1.4743176698684692
Model None Epoch 7 Batch 143: Loss 1.4604588747024536
Model None Epoch 7 Batch 144: Loss 1.4772869348526
Model None Epoch 7 Batch 145: Loss 1.430519461631775
Model None Epoch 7 Batch 146: Loss 1.5635558366775513
Model None Epoch 7 Batch 147: Loss 1.4938652515411377
Model None Epoch 7 Batch 148: Loss 1.5711325407028198
Model None Epoch 7 Batch 149: Loss 1.5006803274154663
Downstream Train Epoch: 7 [38400/50000 (77%)]	Loss: 1.494984
Model None Epoch 7 Batch 150: Loss 1.4949842691421509
Model None Epoch 7 Batch 151: Loss 1.4430872201919556
Model None Epoch 7 Batch 152: Loss 1.4133199453353882
Model None Epoch 7 Batch 153: Loss 1.4419630765914917
Model None Epoch 7 Batch 154: Loss 1.4542548656463623
Model None Epoch 7 Batch 155: Loss 1.5115681886672974
Model None Epoch 7 Batch 156: Loss 1.581212043762207
Model None Epoch 7 Batch 157: Loss 1.4280941486358643
Model None Epoch 7 Batch 158: Loss 1.461615800857544
Model None Epoch 7 Batch 159: Loss 1.4758590459823608
Model None Epoch 7 Batch 160: Loss 1.540643334388733
Model None Epoch 7 Batch 161: Loss 1.4406840801239014
Model None Epoch 7 Batch 162: Loss 1.428633451461792
Model None Epoch 7 Batch 163: Loss 1.4877519607543945
Model None Epoch 7 Batch 164: Loss 1.4935805797576904
Model None Epoch 7 Batch 165: Loss 1.5243301391601562
Model None Epoch 7 Batch 166: Loss 1.4801238775253296
Model None Epoch 7 Batch 167: Loss 1.4204291105270386
Model None Epoch 7 Batch 168: Loss 1.5247738361358643
Model None Epoch 7 Batch 169: Loss 1.5513381958007812
Model None Epoch 7 Batch 170: Loss 1.4665690660476685
Model None Epoch 7 Batch 171: Loss 1.5516520738601685
Model None Epoch 7 Batch 172: Loss 1.598433017730713
Model None Epoch 7 Batch 173: Loss 1.3720364570617676
Model None Epoch 7 Batch 174: Loss 1.5389446020126343
Model None Epoch 7 Batch 175: Loss 1.4054347276687622
Model None Epoch 7 Batch 176: Loss 1.43320631980896
Model None Epoch 7 Batch 177: Loss 1.4231457710266113
Model None Epoch 7 Batch 178: Loss 1.434509038925171
Model None Epoch 7 Batch 179: Loss 1.4176641702651978
Model None Epoch 7 Batch 180: Loss 1.5297791957855225
Model None Epoch 7 Batch 181: Loss 1.4540455341339111
Model None Epoch 7 Batch 182: Loss 1.443233847618103
Model None Epoch 7 Batch 183: Loss 1.5247433185577393
Model None Epoch 7 Batch 184: Loss 1.5195767879486084
Model None Epoch 7 Batch 185: Loss 1.4951595067977905
Model None Epoch 7 Batch 186: Loss 1.441664457321167
Model None Epoch 7 Batch 187: Loss 1.439363956451416
Model None Epoch 7 Batch 188: Loss 1.4182958602905273
Model None Epoch 7 Batch 189: Loss 1.4574395418167114
Model None Epoch 7 Batch 190: Loss 1.4606729745864868
Model None Epoch 7 Batch 191: Loss 1.4806811809539795
Model None Epoch 7 Batch 192: Loss 1.4813517332077026
Model None Epoch 7 Batch 193: Loss 1.4344614744186401
Model None Epoch 7 Batch 194: Loss 1.3384584188461304
Model None Epoch 7 Batch 195: Loss 1.4666422605514526

 Downstream Train loss: 1.4731048272580516 Acc: 0.5328
Downstream Train Epoch: 8 [0/50000 (0%)]	Loss: 1.447958
Model None Epoch 8 Batch 0: Loss 1.4479575157165527
Model None Epoch 8 Batch 1: Loss 1.4139933586120605
Model None Epoch 8 Batch 2: Loss 1.460412859916687
Model None Epoch 8 Batch 3: Loss 1.5316979885101318
Model None Epoch 8 Batch 4: Loss 1.46183180809021
Model None Epoch 8 Batch 5: Loss 1.4143832921981812
Model None Epoch 8 Batch 6: Loss 1.3900054693222046
Model None Epoch 8 Batch 7: Loss 1.539118766784668
Model None Epoch 8 Batch 8: Loss 1.4335252046585083
Model None Epoch 8 Batch 9: Loss 1.472365140914917
Model None Epoch 8 Batch 10: Loss 1.520308017730713
Model None Epoch 8 Batch 11: Loss 1.4595931768417358
Model None Epoch 8 Batch 12: Loss 1.3158957958221436
Model None Epoch 8 Batch 13: Loss 1.4520686864852905
Model None Epoch 8 Batch 14: Loss 1.5078928470611572
Model None Epoch 8 Batch 15: Loss 1.5452109575271606
Model None Epoch 8 Batch 16: Loss 1.4959444999694824
Model None Epoch 8 Batch 17: Loss 1.4056057929992676
Model None Epoch 8 Batch 18: Loss 1.4028828144073486
Model None Epoch 8 Batch 19: Loss 1.441728115081787
Model None Epoch 8 Batch 20: Loss 1.3506309986114502
Model None Epoch 8 Batch 21: Loss 1.4907974004745483
Model None Epoch 8 Batch 22: Loss 1.56057608127594
Model None Epoch 8 Batch 23: Loss 1.5613298416137695
Model None Epoch 8 Batch 24: Loss 1.5287268161773682
Model None Epoch 8 Batch 25: Loss 1.5490745306015015
Model None Epoch 8 Batch 26: Loss 1.4648979902267456
Model None Epoch 8 Batch 27: Loss 1.3666670322418213
Model None Epoch 8 Batch 28: Loss 1.5487717390060425
Model None Epoch 8 Batch 29: Loss 1.5239334106445312
Model None Epoch 8 Batch 30: Loss 1.4068914651870728
Model None Epoch 8 Batch 31: Loss 1.3658835887908936
Model None Epoch 8 Batch 32: Loss 1.446171760559082
Model None Epoch 8 Batch 33: Loss 1.5826549530029297
Model None Epoch 8 Batch 34: Loss 1.5092929601669312
Model None Epoch 8 Batch 35: Loss 1.3444139957427979
Model None Epoch 8 Batch 36: Loss 1.528891682624817
Model None Epoch 8 Batch 37: Loss 1.4568883180618286
Model None Epoch 8 Batch 38: Loss 1.3967039585113525
Model None Epoch 8 Batch 39: Loss 1.3835238218307495
Model None Epoch 8 Batch 40: Loss 1.4307836294174194
Model None Epoch 8 Batch 41: Loss 1.414332389831543
Model None Epoch 8 Batch 42: Loss 1.2776098251342773
Model None Epoch 8 Batch 43: Loss 1.3681761026382446
Model None Epoch 8 Batch 44: Loss 1.5197514295578003
Model None Epoch 8 Batch 45: Loss 1.525273323059082
Model None Epoch 8 Batch 46: Loss 1.459185242652893
Model None Epoch 8 Batch 47: Loss 1.568376898765564
Model None Epoch 8 Batch 48: Loss 1.4713294506072998
Model None Epoch 8 Batch 49: Loss 1.520625352859497
Downstream Train Epoch: 8 [12800/50000 (26%)]	Loss: 1.466331
Model None Epoch 8 Batch 50: Loss 1.466330885887146
Model None Epoch 8 Batch 51: Loss 1.5012521743774414
Model None Epoch 8 Batch 52: Loss 1.3423552513122559
Model None Epoch 8 Batch 53: Loss 1.5628111362457275
Model None Epoch 8 Batch 54: Loss 1.490883469581604
Model None Epoch 8 Batch 55: Loss 1.465804934501648
Model None Epoch 8 Batch 56: Loss 1.3931665420532227
Model None Epoch 8 Batch 57: Loss 1.4517217874526978
Model None Epoch 8 Batch 58: Loss 1.3776403665542603
Model None Epoch 8 Batch 59: Loss 1.391099214553833
Model None Epoch 8 Batch 60: Loss 1.599012851715088
Model None Epoch 8 Batch 61: Loss 1.4516090154647827
Model None Epoch 8 Batch 62: Loss 1.4514864683151245
Model None Epoch 8 Batch 63: Loss 1.3904900550842285
Model None Epoch 8 Batch 64: Loss 1.5080925226211548
Model None Epoch 8 Batch 65: Loss 1.4036329984664917
Model None Epoch 8 Batch 66: Loss 1.507344126701355
Model None Epoch 8 Batch 67: Loss 1.4268542528152466
Model None Epoch 8 Batch 68: Loss 1.4583864212036133
Model None Epoch 8 Batch 69: Loss 1.4339711666107178
Model None Epoch 8 Batch 70: Loss 1.5584965944290161
Model None Epoch 8 Batch 71: Loss 1.5344592332839966
Model None Epoch 8 Batch 72: Loss 1.382730484008789
Model None Epoch 8 Batch 73: Loss 1.4718937873840332
Model None Epoch 8 Batch 74: Loss 1.3980848789215088
Model None Epoch 8 Batch 75: Loss 1.3709197044372559
Model None Epoch 8 Batch 76: Loss 1.4354584217071533
Model None Epoch 8 Batch 77: Loss 1.5194461345672607
Model None Epoch 8 Batch 78: Loss 1.333898663520813
Model None Epoch 8 Batch 79: Loss 1.4126360416412354
Model None Epoch 8 Batch 80: Loss 1.429087519645691
Model None Epoch 8 Batch 81: Loss 1.5848610401153564
Model None Epoch 8 Batch 82: Loss 1.4646621942520142
Model None Epoch 8 Batch 83: Loss 1.3352991342544556
Model None Epoch 8 Batch 84: Loss 1.4081369638442993
Model None Epoch 8 Batch 85: Loss 1.339162826538086
Model None Epoch 8 Batch 86: Loss 1.508738398551941
Model None Epoch 8 Batch 87: Loss 1.4500489234924316
Model None Epoch 8 Batch 88: Loss 1.38812255859375
Model None Epoch 8 Batch 89: Loss 1.527642011642456
Model None Epoch 8 Batch 90: Loss 1.4759724140167236
Model None Epoch 8 Batch 91: Loss 1.4691435098648071
Model None Epoch 8 Batch 92: Loss 1.391741394996643
Model None Epoch 8 Batch 93: Loss 1.489090919494629
Model None Epoch 8 Batch 94: Loss 1.6028732061386108
Model None Epoch 8 Batch 95: Loss 1.3137118816375732
Model None Epoch 8 Batch 96: Loss 1.5570807456970215
Model None Epoch 8 Batch 97: Loss 1.4449344873428345
Model None Epoch 8 Batch 98: Loss 1.4397833347320557
Model None Epoch 8 Batch 99: Loss 1.3871541023254395
Downstream Train Epoch: 8 [25600/50000 (51%)]	Loss: 1.547567
Model None Epoch 8 Batch 100: Loss 1.5475671291351318
Model None Epoch 8 Batch 101: Loss 1.459998369216919
Model None Epoch 8 Batch 102: Loss 1.3488893508911133
Model None Epoch 8 Batch 103: Loss 1.4154318571090698
Model None Epoch 8 Batch 104: Loss 1.434504508972168
Model None Epoch 8 Batch 105: Loss 1.4148571491241455
Model None Epoch 8 Batch 106: Loss 1.4741137027740479
Model None Epoch 8 Batch 107: Loss 1.5266640186309814
Model None Epoch 8 Batch 108: Loss 1.4792486429214478
Model None Epoch 8 Batch 109: Loss 1.3549582958221436
Model None Epoch 8 Batch 110: Loss 1.4812506437301636
Model None Epoch 8 Batch 111: Loss 1.4976165294647217
Model None Epoch 8 Batch 112: Loss 1.5196173191070557
Model None Epoch 8 Batch 113: Loss 1.4636958837509155
Model None Epoch 8 Batch 114: Loss 1.5138219594955444
Model None Epoch 8 Batch 115: Loss 1.476657509803772
Model None Epoch 8 Batch 116: Loss 1.5692967176437378
Model None Epoch 8 Batch 117: Loss 1.3356118202209473
Model None Epoch 8 Batch 118: Loss 1.479419231414795
Model None Epoch 8 Batch 119: Loss 1.4419701099395752
Model None Epoch 8 Batch 120: Loss 1.4522372484207153
Model None Epoch 8 Batch 121: Loss 1.4553282260894775
Model None Epoch 8 Batch 122: Loss 1.51225745677948
Model None Epoch 8 Batch 123: Loss 1.4503681659698486
Model None Epoch 8 Batch 124: Loss 1.5530359745025635
Model None Epoch 8 Batch 125: Loss 1.3952316045761108
Model None Epoch 8 Batch 126: Loss 1.5528621673583984
Model None Epoch 8 Batch 127: Loss 1.3995739221572876
Model None Epoch 8 Batch 128: Loss 1.460113763809204
Model None Epoch 8 Batch 129: Loss 1.5038976669311523
Model None Epoch 8 Batch 130: Loss 1.424423098564148
Model None Epoch 8 Batch 131: Loss 1.5314010381698608
Model None Epoch 8 Batch 132: Loss 1.5362249612808228
Model None Epoch 8 Batch 133: Loss 1.4458998441696167
Model None Epoch 8 Batch 134: Loss 1.5433764457702637
Model None Epoch 8 Batch 135: Loss 1.374395728111267
Model None Epoch 8 Batch 136: Loss 1.4317338466644287
Model None Epoch 8 Batch 137: Loss 1.429242730140686
Model None Epoch 8 Batch 138: Loss 1.3923084735870361
Model None Epoch 8 Batch 139: Loss 1.495588779449463
Model None Epoch 8 Batch 140: Loss 1.4568005800247192
Model None Epoch 8 Batch 141: Loss 1.5463839769363403
Model None Epoch 8 Batch 142: Loss 1.481266736984253
Model None Epoch 8 Batch 143: Loss 1.431874394416809
Model None Epoch 8 Batch 144: Loss 1.5780471563339233
Model None Epoch 8 Batch 145: Loss 1.396628499031067
Model None Epoch 8 Batch 146: Loss 1.5906400680541992
Model None Epoch 8 Batch 147: Loss 1.5347980260849
Model None Epoch 8 Batch 148: Loss 1.4168004989624023
Model None Epoch 8 Batch 149: Loss 1.487026572227478
Downstream Train Epoch: 8 [38400/50000 (77%)]	Loss: 1.345770
Model None Epoch 8 Batch 150: Loss 1.3457698822021484
Model None Epoch 8 Batch 151: Loss 1.5617748498916626
Model None Epoch 8 Batch 152: Loss 1.4479860067367554
Model None Epoch 8 Batch 153: Loss 1.5392298698425293
Model None Epoch 8 Batch 154: Loss 1.3899235725402832
Model None Epoch 8 Batch 155: Loss 1.4759422540664673
Model None Epoch 8 Batch 156: Loss 1.3728108406066895
Model None Epoch 8 Batch 157: Loss 1.3974828720092773
Model None Epoch 8 Batch 158: Loss 1.4067914485931396
Model None Epoch 8 Batch 159: Loss 1.4896576404571533
Model None Epoch 8 Batch 160: Loss 1.4713109731674194
Model None Epoch 8 Batch 161: Loss 1.5271317958831787
Model None Epoch 8 Batch 162: Loss 1.4048922061920166
Model None Epoch 8 Batch 163: Loss 1.3733110427856445
Model None Epoch 8 Batch 164: Loss 1.5169211626052856
Model None Epoch 8 Batch 165: Loss 1.4349792003631592
Model None Epoch 8 Batch 166: Loss 1.5453726053237915
Model None Epoch 8 Batch 167: Loss 1.485059142112732
Model None Epoch 8 Batch 168: Loss 1.4236441850662231
Model None Epoch 8 Batch 169: Loss 1.3661466836929321
Model None Epoch 8 Batch 170: Loss 1.45867919921875
Model None Epoch 8 Batch 171: Loss 1.4421062469482422
Model None Epoch 8 Batch 172: Loss 1.4963747262954712
Model None Epoch 8 Batch 173: Loss 1.400226354598999
Model None Epoch 8 Batch 174: Loss 1.5488834381103516
Model None Epoch 8 Batch 175: Loss 1.4108389616012573
Model None Epoch 8 Batch 176: Loss 1.4476410150527954
Model None Epoch 8 Batch 177: Loss 1.3914060592651367
Model None Epoch 8 Batch 178: Loss 1.4085043668746948
Model None Epoch 8 Batch 179: Loss 1.4714797735214233
Model None Epoch 8 Batch 180: Loss 1.399196743965149
Model None Epoch 8 Batch 181: Loss 1.4256082773208618
Model None Epoch 8 Batch 182: Loss 1.4772210121154785
Model None Epoch 8 Batch 183: Loss 1.4706023931503296
Model None Epoch 8 Batch 184: Loss 1.3894492387771606
Model None Epoch 8 Batch 185: Loss 1.5182493925094604
Model None Epoch 8 Batch 186: Loss 1.3610690832138062
Model None Epoch 8 Batch 187: Loss 1.3947577476501465
Model None Epoch 8 Batch 188: Loss 1.5430707931518555
Model None Epoch 8 Batch 189: Loss 1.5150524377822876
Model None Epoch 8 Batch 190: Loss 1.4849162101745605
Model None Epoch 8 Batch 191: Loss 1.3779581785202026
Model None Epoch 8 Batch 192: Loss 1.5257465839385986
Model None Epoch 8 Batch 193: Loss 1.416694164276123
Model None Epoch 8 Batch 194: Loss 1.323938250541687
Model None Epoch 8 Batch 195: Loss 1.5072587728500366

 Downstream Train loss: 1.4572032209561796 Acc: 0.5328
Downstream Train Epoch: 9 [0/50000 (0%)]	Loss: 1.441882
Model None Epoch 9 Batch 0: Loss 1.4418816566467285
Model None Epoch 9 Batch 1: Loss 1.5138812065124512
Model None Epoch 9 Batch 2: Loss 1.5172438621520996
Model None Epoch 9 Batch 3: Loss 1.357414960861206
Model None Epoch 9 Batch 4: Loss 1.4636640548706055
Model None Epoch 9 Batch 5: Loss 1.3819384574890137
Model None Epoch 9 Batch 6: Loss 1.4877427816390991
Model None Epoch 9 Batch 7: Loss 1.4487838745117188
Model None Epoch 9 Batch 8: Loss 1.4118040800094604
Model None Epoch 9 Batch 9: Loss 1.5578218698501587
Model None Epoch 9 Batch 10: Loss 1.5015158653259277
Model None Epoch 9 Batch 11: Loss 1.4729807376861572
Model None Epoch 9 Batch 12: Loss 1.3420320749282837
Model None Epoch 9 Batch 13: Loss 1.5515269041061401
Model None Epoch 9 Batch 14: Loss 1.4042611122131348
Model None Epoch 9 Batch 15: Loss 1.5462661981582642
Model None Epoch 9 Batch 16: Loss 1.4230221509933472
Model None Epoch 9 Batch 17: Loss 1.437665581703186
Model None Epoch 9 Batch 18: Loss 1.53379487991333
Model None Epoch 9 Batch 19: Loss 1.5274498462677002
Model None Epoch 9 Batch 20: Loss 1.4303662776947021
Model None Epoch 9 Batch 21: Loss 1.4320780038833618
Model None Epoch 9 Batch 22: Loss 1.4796628952026367
Model None Epoch 9 Batch 23: Loss 1.5267659425735474
Model None Epoch 9 Batch 24: Loss 1.4347120523452759
Model None Epoch 9 Batch 25: Loss 1.4350003004074097
Model None Epoch 9 Batch 26: Loss 1.5252918004989624
Model None Epoch 9 Batch 27: Loss 1.4393703937530518
Model None Epoch 9 Batch 28: Loss 1.4765092134475708
Model None Epoch 9 Batch 29: Loss 1.5519362688064575
Model None Epoch 9 Batch 30: Loss 1.4177569150924683
Model None Epoch 9 Batch 31: Loss 1.4455220699310303
Model None Epoch 9 Batch 32: Loss 1.3998438119888306
Model None Epoch 9 Batch 33: Loss 1.5194813013076782
Model None Epoch 9 Batch 34: Loss 1.3342012166976929
Model None Epoch 9 Batch 35: Loss 1.394178032875061
Model None Epoch 9 Batch 36: Loss 1.5589662790298462
Model None Epoch 9 Batch 37: Loss 1.4121514558792114
Model None Epoch 9 Batch 38: Loss 1.4717862606048584
Model None Epoch 9 Batch 39: Loss 1.4464656114578247
Model None Epoch 9 Batch 40: Loss 1.4344210624694824
Model None Epoch 9 Batch 41: Loss 1.365598440170288
Model None Epoch 9 Batch 42: Loss 1.4703606367111206
Model None Epoch 9 Batch 43: Loss 1.4378066062927246
Model None Epoch 9 Batch 44: Loss 1.4796993732452393
Model None Epoch 9 Batch 45: Loss 1.51569664478302
Model None Epoch 9 Batch 46: Loss 1.348158597946167
Model None Epoch 9 Batch 47: Loss 1.3517364263534546
Model None Epoch 9 Batch 48: Loss 1.5355312824249268
Model None Epoch 9 Batch 49: Loss 1.5476866960525513
Downstream Train Epoch: 9 [12800/50000 (26%)]	Loss: 1.565892
Model None Epoch 9 Batch 50: Loss 1.5658916234970093
Model None Epoch 9 Batch 51: Loss 1.49689519405365
Model None Epoch 9 Batch 52: Loss 1.433955430984497
Model None Epoch 9 Batch 53: Loss 1.501381278038025
Model None Epoch 9 Batch 54: Loss 1.4010628461837769
Model None Epoch 9 Batch 55: Loss 1.292521357536316
Model None Epoch 9 Batch 56: Loss 1.458878993988037
Model None Epoch 9 Batch 57: Loss 1.4822416305541992
Model None Epoch 9 Batch 58: Loss 1.4829790592193604
Model None Epoch 9 Batch 59: Loss 1.466402530670166
Model None Epoch 9 Batch 60: Loss 1.3679178953170776
Model None Epoch 9 Batch 61: Loss 1.4917937517166138
Model None Epoch 9 Batch 62: Loss 1.428226351737976
Model None Epoch 9 Batch 63: Loss 1.4716122150421143
Model None Epoch 9 Batch 64: Loss 1.5514285564422607
Model None Epoch 9 Batch 65: Loss 1.369030475616455
Model None Epoch 9 Batch 66: Loss 1.4541679620742798
Model None Epoch 9 Batch 67: Loss 1.3449844121932983
Model None Epoch 9 Batch 68: Loss 1.506074070930481
Model None Epoch 9 Batch 69: Loss 1.4956014156341553
Model None Epoch 9 Batch 70: Loss 1.49088716506958
Model None Epoch 9 Batch 71: Loss 1.516909122467041
Model None Epoch 9 Batch 72: Loss 1.5129622220993042
Model None Epoch 9 Batch 73: Loss 1.514162302017212
Model None Epoch 9 Batch 74: Loss 1.433354377746582
Model None Epoch 9 Batch 75: Loss 1.4219502210617065
Model None Epoch 9 Batch 76: Loss 1.5307284593582153
Model None Epoch 9 Batch 77: Loss 1.35470449924469
Model None Epoch 9 Batch 78: Loss 1.4756299257278442
Model None Epoch 9 Batch 79: Loss 1.4092739820480347
Model None Epoch 9 Batch 80: Loss 1.4059722423553467
Model None Epoch 9 Batch 81: Loss 1.4656774997711182
Model None Epoch 9 Batch 82: Loss 1.4065598249435425
Model None Epoch 9 Batch 83: Loss 1.5205153226852417
Model None Epoch 9 Batch 84: Loss 1.414630651473999
Model None Epoch 9 Batch 85: Loss 1.4819302558898926
Model None Epoch 9 Batch 86: Loss 1.3369052410125732
Model None Epoch 9 Batch 87: Loss 1.5171451568603516
Model None Epoch 9 Batch 88: Loss 1.4917548894882202
Model None Epoch 9 Batch 89: Loss 1.5103387832641602
Model None Epoch 9 Batch 90: Loss 1.4952306747436523
Model None Epoch 9 Batch 91: Loss 1.4476035833358765
Model None Epoch 9 Batch 92: Loss 1.354243278503418
Model None Epoch 9 Batch 93: Loss 1.5068333148956299
Model None Epoch 9 Batch 94: Loss 1.465828776359558
Model None Epoch 9 Batch 95: Loss 1.5584725141525269
Model None Epoch 9 Batch 96: Loss 1.481810450553894
Model None Epoch 9 Batch 97: Loss 1.4416284561157227
Model None Epoch 9 Batch 98: Loss 1.5011414289474487
Model None Epoch 9 Batch 99: Loss 1.4482964277267456
Downstream Train Epoch: 9 [25600/50000 (51%)]	Loss: 1.475605
Model None Epoch 9 Batch 100: Loss 1.475604772567749
Model None Epoch 9 Batch 101: Loss 1.4928176403045654
Model None Epoch 9 Batch 102: Loss 1.473196268081665
Model None Epoch 9 Batch 103: Loss 1.4719918966293335
Model None Epoch 9 Batch 104: Loss 1.5379704236984253
Model None Epoch 9 Batch 105: Loss 1.4746932983398438
Model None Epoch 9 Batch 106: Loss 1.4586341381072998
Model None Epoch 9 Batch 107: Loss 1.4511969089508057
Model None Epoch 9 Batch 108: Loss 1.5133016109466553
Model None Epoch 9 Batch 109: Loss 1.3314359188079834
Model None Epoch 9 Batch 110: Loss 1.5373040437698364
Model None Epoch 9 Batch 111: Loss 1.3500161170959473
Model None Epoch 9 Batch 112: Loss 1.37441086769104
Model None Epoch 9 Batch 113: Loss 1.557841181755066
Model None Epoch 9 Batch 114: Loss 1.4754106998443604
Model None Epoch 9 Batch 115: Loss 1.4770299196243286
Model None Epoch 9 Batch 116: Loss 1.3315927982330322
Model None Epoch 9 Batch 117: Loss 1.469358205795288
Model None Epoch 9 Batch 118: Loss 1.451196551322937
Model None Epoch 9 Batch 119: Loss 1.519274353981018
Model None Epoch 9 Batch 120: Loss 1.4402905702590942
Model None Epoch 9 Batch 121: Loss 1.5364149808883667
Model None Epoch 9 Batch 122: Loss 1.4607936143875122
Model None Epoch 9 Batch 123: Loss 1.5645782947540283
Model None Epoch 9 Batch 124: Loss 1.4860379695892334
Model None Epoch 9 Batch 125: Loss 1.429315209388733
Model None Epoch 9 Batch 126: Loss 1.3767248392105103
Model None Epoch 9 Batch 127: Loss 1.390260934829712
Model None Epoch 9 Batch 128: Loss 1.5545709133148193
Model None Epoch 9 Batch 129: Loss 1.4512797594070435
Model None Epoch 9 Batch 130: Loss 1.4469640254974365
Model None Epoch 9 Batch 131: Loss 1.4083995819091797
Model None Epoch 9 Batch 132: Loss 1.470428466796875
Model None Epoch 9 Batch 133: Loss 1.3733136653900146
Model None Epoch 9 Batch 134: Loss 1.553215742111206
Model None Epoch 9 Batch 135: Loss 1.4586790800094604
Model None Epoch 9 Batch 136: Loss 1.3733164072036743
Model None Epoch 9 Batch 137: Loss 1.5402473211288452
Model None Epoch 9 Batch 138: Loss 1.5067216157913208
Model None Epoch 9 Batch 139: Loss 1.4764829874038696
Model None Epoch 9 Batch 140: Loss 1.4392690658569336
Model None Epoch 9 Batch 141: Loss 1.500454068183899
Model None Epoch 9 Batch 142: Loss 1.4887120723724365
Model None Epoch 9 Batch 143: Loss 1.431492805480957
Model None Epoch 9 Batch 144: Loss 1.4556236267089844
Model None Epoch 9 Batch 145: Loss 1.4959951639175415
Model None Epoch 9 Batch 146: Loss 1.530623197555542
Model None Epoch 9 Batch 147: Loss 1.4298210144042969
Model None Epoch 9 Batch 148: Loss 1.5764960050582886
Model None Epoch 9 Batch 149: Loss 1.3775655031204224
Downstream Train Epoch: 9 [38400/50000 (77%)]	Loss: 1.393916
Model None Epoch 9 Batch 150: Loss 1.3939155340194702
Model None Epoch 9 Batch 151: Loss 1.4634919166564941
Model None Epoch 9 Batch 152: Loss 1.4467352628707886
Model None Epoch 9 Batch 153: Loss 1.3981090784072876
Model None Epoch 9 Batch 154: Loss 1.469390630722046
Model None Epoch 9 Batch 155: Loss 1.3938452005386353
Model None Epoch 9 Batch 156: Loss 1.436008334159851
Model None Epoch 9 Batch 157: Loss 1.4858912229537964
Model None Epoch 9 Batch 158: Loss 1.4993205070495605
Model None Epoch 9 Batch 159: Loss 1.4457788467407227
Model None Epoch 9 Batch 160: Loss 1.5605710744857788
Model None Epoch 9 Batch 161: Loss 1.4117404222488403
Model None Epoch 9 Batch 162: Loss 1.3760566711425781
Model None Epoch 9 Batch 163: Loss 1.4211645126342773
Model None Epoch 9 Batch 164: Loss 1.4917839765548706
Model None Epoch 9 Batch 165: Loss 1.4253959655761719
Model None Epoch 9 Batch 166: Loss 1.6506869792938232
Model None Epoch 9 Batch 167: Loss 1.5069310665130615
Model None Epoch 9 Batch 168: Loss 1.4710896015167236
Model None Epoch 9 Batch 169: Loss 1.5016511678695679
Model None Epoch 9 Batch 170: Loss 1.362334132194519
Model None Epoch 9 Batch 171: Loss 1.5421220064163208
Model None Epoch 9 Batch 172: Loss 1.415815830230713
Model None Epoch 9 Batch 173: Loss 1.4552760124206543
Model None Epoch 9 Batch 174: Loss 1.3808417320251465
Model None Epoch 9 Batch 175: Loss 1.4733879566192627
Model None Epoch 9 Batch 176: Loss 1.5792818069458008
Model None Epoch 9 Batch 177: Loss 1.6066904067993164
Model None Epoch 9 Batch 178: Loss 1.4099305868148804
Model None Epoch 9 Batch 179: Loss 1.4006421566009521
Model None Epoch 9 Batch 180: Loss 1.3303396701812744
Model None Epoch 9 Batch 181: Loss 1.4551317691802979
Model None Epoch 9 Batch 182: Loss 1.4889001846313477
Model None Epoch 9 Batch 183: Loss 1.5269743204116821
Model None Epoch 9 Batch 184: Loss 1.4637153148651123
Model None Epoch 9 Batch 185: Loss 1.4302769899368286
Model None Epoch 9 Batch 186: Loss 1.4903210401535034
Model None Epoch 9 Batch 187: Loss 1.4407285451889038
Model None Epoch 9 Batch 188: Loss 1.421103596687317
Model None Epoch 9 Batch 189: Loss 1.380773663520813
Model None Epoch 9 Batch 190: Loss 1.5109490156173706
Model None Epoch 9 Batch 191: Loss 1.3966169357299805
Model None Epoch 9 Batch 192: Loss 1.4959062337875366
Model None Epoch 9 Batch 193: Loss 1.4477800130844116
Model None Epoch 9 Batch 194: Loss 1.4722352027893066
Model None Epoch 9 Batch 195: Loss 1.4849249124526978

 Downstream Train loss: 1.4606045216930157 Acc: 0.5328
Downstream Train Epoch: 10 [0/50000 (0%)]	Loss: 1.499678
Model None Epoch 10 Batch 0: Loss 1.4996784925460815
Model None Epoch 10 Batch 1: Loss 1.3891633749008179
Model None Epoch 10 Batch 2: Loss 1.4636329412460327
Model None Epoch 10 Batch 3: Loss 1.5018292665481567
Model None Epoch 10 Batch 4: Loss 1.3886677026748657
Model None Epoch 10 Batch 5: Loss 1.4340825080871582
Model None Epoch 10 Batch 6: Loss 1.4254393577575684
Model None Epoch 10 Batch 7: Loss 1.530577301979065
Model None Epoch 10 Batch 8: Loss 1.3762387037277222
Model None Epoch 10 Batch 9: Loss 1.2919576168060303
Model None Epoch 10 Batch 10: Loss 1.4603018760681152
Model None Epoch 10 Batch 11: Loss 1.419229507446289
Model None Epoch 10 Batch 12: Loss 1.532057285308838
Model None Epoch 10 Batch 13: Loss 1.438643455505371
Model None Epoch 10 Batch 14: Loss 1.4192057847976685
Model None Epoch 10 Batch 15: Loss 1.5291517972946167
Model None Epoch 10 Batch 16: Loss 1.4028511047363281
Model None Epoch 10 Batch 17: Loss 1.553216814994812
Model None Epoch 10 Batch 18: Loss 1.5318050384521484
Model None Epoch 10 Batch 19: Loss 1.4186012744903564
Model None Epoch 10 Batch 20: Loss 1.426737904548645
Model None Epoch 10 Batch 21: Loss 1.5031903982162476
Model None Epoch 10 Batch 22: Loss 1.4410176277160645
Model None Epoch 10 Batch 23: Loss 1.4578076601028442
Model None Epoch 10 Batch 24: Loss 1.5459403991699219
Model None Epoch 10 Batch 25: Loss 1.4300345182418823
Model None Epoch 10 Batch 26: Loss 1.4664230346679688
Model None Epoch 10 Batch 27: Loss 1.3577194213867188
Model None Epoch 10 Batch 28: Loss 1.4296082258224487
Model None Epoch 10 Batch 29: Loss 1.4730552434921265
Model None Epoch 10 Batch 30: Loss 1.3710265159606934
Model None Epoch 10 Batch 31: Loss 1.3446696996688843
Model None Epoch 10 Batch 32: Loss 1.4558379650115967
Model None Epoch 10 Batch 33: Loss 1.5641162395477295
Model None Epoch 10 Batch 34: Loss 1.456700325012207
Model None Epoch 10 Batch 35: Loss 1.4233866930007935
Model None Epoch 10 Batch 36: Loss 1.4234031438827515
Model None Epoch 10 Batch 37: Loss 1.4045583009719849
Model None Epoch 10 Batch 38: Loss 1.5076804161071777
Model None Epoch 10 Batch 39: Loss 1.53523588180542
Model None Epoch 10 Batch 40: Loss 1.6584923267364502
Model None Epoch 10 Batch 41: Loss 1.4362274408340454
Model None Epoch 10 Batch 42: Loss 1.429660439491272
Model None Epoch 10 Batch 43: Loss 1.4859730005264282
Model None Epoch 10 Batch 44: Loss 1.4850339889526367
Model None Epoch 10 Batch 45: Loss 1.601609706878662
Model None Epoch 10 Batch 46: Loss 1.3154579401016235
Model None Epoch 10 Batch 47: Loss 1.4927372932434082
Model None Epoch 10 Batch 48: Loss 1.5048221349716187
Model None Epoch 10 Batch 49: Loss 1.528942346572876
Downstream Train Epoch: 10 [12800/50000 (26%)]	Loss: 1.481117
Model None Epoch 10 Batch 50: Loss 1.4811174869537354
Model None Epoch 10 Batch 51: Loss 1.576103925704956
Model None Epoch 10 Batch 52: Loss 1.4759106636047363
Model None Epoch 10 Batch 53: Loss 1.338545799255371
Model None Epoch 10 Batch 54: Loss 1.4729323387145996
Model None Epoch 10 Batch 55: Loss 1.4949759244918823
Model None Epoch 10 Batch 56: Loss 1.492319941520691
Model None Epoch 10 Batch 57: Loss 1.5328706502914429
Model None Epoch 10 Batch 58: Loss 1.5028412342071533
Model None Epoch 10 Batch 59: Loss 1.4825103282928467
Model None Epoch 10 Batch 60: Loss 1.3737021684646606
Model None Epoch 10 Batch 61: Loss 1.5313166379928589
Model None Epoch 10 Batch 62: Loss 1.5595488548278809
Model None Epoch 10 Batch 63: Loss 1.3661246299743652
Model None Epoch 10 Batch 64: Loss 1.4534000158309937
Model None Epoch 10 Batch 65: Loss 1.5074938535690308
Model None Epoch 10 Batch 66: Loss 1.4222908020019531
Model None Epoch 10 Batch 67: Loss 1.425757884979248
Model None Epoch 10 Batch 68: Loss 1.377501130104065
Model None Epoch 10 Batch 69: Loss 1.4112330675125122
Model None Epoch 10 Batch 70: Loss 1.484594702720642
Model None Epoch 10 Batch 71: Loss 1.3817695379257202
Model None Epoch 10 Batch 72: Loss 1.4249963760375977
Model None Epoch 10 Batch 73: Loss 1.5765913724899292
Model None Epoch 10 Batch 74: Loss 1.5269042253494263
Model None Epoch 10 Batch 75: Loss 1.4819908142089844
Model None Epoch 10 Batch 76: Loss 1.4399503469467163
Model None Epoch 10 Batch 77: Loss 1.491769552230835
Model None Epoch 10 Batch 78: Loss 1.3955999612808228
Model None Epoch 10 Batch 79: Loss 1.5177112817764282
Model None Epoch 10 Batch 80: Loss 1.4079617261886597
Model None Epoch 10 Batch 81: Loss 1.4878009557724
Model None Epoch 10 Batch 82: Loss 1.432241439819336
Model None Epoch 10 Batch 83: Loss 1.3305816650390625
Model None Epoch 10 Batch 84: Loss 1.4140180349349976
Model None Epoch 10 Batch 85: Loss 1.4593313932418823
Model None Epoch 10 Batch 86: Loss 1.4634692668914795
Model None Epoch 10 Batch 87: Loss 1.433435082435608
Model None Epoch 10 Batch 88: Loss 1.3936611413955688
Model None Epoch 10 Batch 89: Loss 1.398992657661438
Model None Epoch 10 Batch 90: Loss 1.4948327541351318
Model None Epoch 10 Batch 91: Loss 1.428030252456665
Model None Epoch 10 Batch 92: Loss 1.4184181690216064
Model None Epoch 10 Batch 93: Loss 1.5166829824447632
Model None Epoch 10 Batch 94: Loss 1.4613170623779297
Model None Epoch 10 Batch 95: Loss 1.5121464729309082
Model None Epoch 10 Batch 96: Loss 1.4499558210372925
Model None Epoch 10 Batch 97: Loss 1.4075205326080322
Model None Epoch 10 Batch 98: Loss 1.370680332183838
Model None Epoch 10 Batch 99: Loss 1.4477648735046387
Downstream Train Epoch: 10 [25600/50000 (51%)]	Loss: 1.350983
Model None Epoch 10 Batch 100: Loss 1.3509831428527832
Model None Epoch 10 Batch 101: Loss 1.5421899557113647
Model None Epoch 10 Batch 102: Loss 1.3998624086380005
Model None Epoch 10 Batch 103: Loss 1.5298633575439453
Model None Epoch 10 Batch 104: Loss 1.5841948986053467
Model None Epoch 10 Batch 105: Loss 1.4191522598266602
Model None Epoch 10 Batch 106: Loss 1.4871351718902588
Model None Epoch 10 Batch 107: Loss 1.538922905921936
Model None Epoch 10 Batch 108: Loss 1.4300371408462524
Model None Epoch 10 Batch 109: Loss 1.3872826099395752
Model None Epoch 10 Batch 110: Loss 1.4367860555648804
Model None Epoch 10 Batch 111: Loss 1.4697599411010742
Model None Epoch 10 Batch 112: Loss 1.4486384391784668
Model None Epoch 10 Batch 113: Loss 1.4663119316101074
Model None Epoch 10 Batch 114: Loss 1.4578282833099365
Model None Epoch 10 Batch 115: Loss 1.4538660049438477
Model None Epoch 10 Batch 116: Loss 1.3477529287338257
Model None Epoch 10 Batch 117: Loss 1.474392056465149
Model None Epoch 10 Batch 118: Loss 1.440419316291809
Model None Epoch 10 Batch 119: Loss 1.3540681600570679
Model None Epoch 10 Batch 120: Loss 1.5553988218307495
Model None Epoch 10 Batch 121: Loss 1.6143287420272827
Model None Epoch 10 Batch 122: Loss 1.481865644454956
Model None Epoch 10 Batch 123: Loss 1.3245564699172974
Model None Epoch 10 Batch 124: Loss 1.462127923965454
Model None Epoch 10 Batch 125: Loss 1.574239730834961
Model None Epoch 10 Batch 126: Loss 1.4039007425308228
Model None Epoch 10 Batch 127: Loss 1.5537550449371338
Model None Epoch 10 Batch 128: Loss 1.3769506216049194
Model None Epoch 10 Batch 129: Loss 1.3870937824249268
Model None Epoch 10 Batch 130: Loss 1.4859983921051025
Model None Epoch 10 Batch 131: Loss 1.4708306789398193
Model None Epoch 10 Batch 132: Loss 1.5382221937179565
Model None Epoch 10 Batch 133: Loss 1.4751185178756714
Model None Epoch 10 Batch 134: Loss 1.3569269180297852
Model None Epoch 10 Batch 135: Loss 1.3459668159484863
Model None Epoch 10 Batch 136: Loss 1.53936767578125
Model None Epoch 10 Batch 137: Loss 1.4836113452911377
Model None Epoch 10 Batch 138: Loss 1.3908754587173462
Model None Epoch 10 Batch 139: Loss 1.3584824800491333
Model None Epoch 10 Batch 140: Loss 1.369661569595337
Model None Epoch 10 Batch 141: Loss 1.543524146080017
Model None Epoch 10 Batch 142: Loss 1.522768497467041
Model None Epoch 10 Batch 143: Loss 1.3572715520858765
Model None Epoch 10 Batch 144: Loss 1.412797451019287
Model None Epoch 10 Batch 145: Loss 1.2794219255447388
Model None Epoch 10 Batch 146: Loss 1.4497085809707642
Model None Epoch 10 Batch 147: Loss 1.4236226081848145
Model None Epoch 10 Batch 148: Loss 1.5175628662109375
Model None Epoch 10 Batch 149: Loss 1.3887035846710205
Downstream Train Epoch: 10 [38400/50000 (77%)]	Loss: 1.372611
Model None Epoch 10 Batch 150: Loss 1.3726106882095337
Model None Epoch 10 Batch 151: Loss 1.4347630739212036
Model None Epoch 10 Batch 152: Loss 1.5052590370178223
Model None Epoch 10 Batch 153: Loss 1.5290757417678833
Model None Epoch 10 Batch 154: Loss 1.46903395652771
Model None Epoch 10 Batch 155: Loss 1.4596232175827026
Model None Epoch 10 Batch 156: Loss 1.4076392650604248
Model None Epoch 10 Batch 157: Loss 1.5407415628433228
Model None Epoch 10 Batch 158: Loss 1.583702564239502
Model None Epoch 10 Batch 159: Loss 1.477213978767395
Model None Epoch 10 Batch 160: Loss 1.4663082361221313
Model None Epoch 10 Batch 161: Loss 1.5035203695297241
Model None Epoch 10 Batch 162: Loss 1.4242521524429321
Model None Epoch 10 Batch 163: Loss 1.4684357643127441
Model None Epoch 10 Batch 164: Loss 1.400126576423645
Model None Epoch 10 Batch 165: Loss 1.3832833766937256
Model None Epoch 10 Batch 166: Loss 1.472455382347107
Model None Epoch 10 Batch 167: Loss 1.552858829498291
Model None Epoch 10 Batch 168: Loss 1.4976792335510254
Model None Epoch 10 Batch 169: Loss 1.4837404489517212
Model None Epoch 10 Batch 170: Loss 1.477445125579834
Model None Epoch 10 Batch 171: Loss 1.4933350086212158
Model None Epoch 10 Batch 172: Loss 1.4723039865493774
Model None Epoch 10 Batch 173: Loss 1.4991317987442017
Model None Epoch 10 Batch 174: Loss 1.5834561586380005
Model None Epoch 10 Batch 175: Loss 1.4190562963485718
Model None Epoch 10 Batch 176: Loss 1.4161806106567383
Model None Epoch 10 Batch 177: Loss 1.4201899766921997
Model None Epoch 10 Batch 178: Loss 1.3894774913787842
Model None Epoch 10 Batch 179: Loss 1.566702961921692
Model None Epoch 10 Batch 180: Loss 1.4719277620315552
Model None Epoch 10 Batch 181: Loss 1.4801993370056152
Model None Epoch 10 Batch 182: Loss 1.3784327507019043
Model None Epoch 10 Batch 183: Loss 1.3602696657180786
Model None Epoch 10 Batch 184: Loss 1.5405257940292358
Model None Epoch 10 Batch 185: Loss 1.4093267917633057
Model None Epoch 10 Batch 186: Loss 1.4274219274520874
Model None Epoch 10 Batch 187: Loss 1.5362379550933838
Model None Epoch 10 Batch 188: Loss 1.4458760023117065
Model None Epoch 10 Batch 189: Loss 1.4058436155319214
Model None Epoch 10 Batch 190: Loss 1.4940754175186157
Model None Epoch 10 Batch 191: Loss 1.4760220050811768
Model None Epoch 10 Batch 192: Loss 1.6079553365707397
Model None Epoch 10 Batch 193: Loss 1.3274312019348145
Model None Epoch 10 Batch 194: Loss 1.415806531906128
Model None Epoch 10 Batch 195: Loss 1.4971482753753662

 Downstream Train loss: 1.457147278347794 Acc: 0.5328
Downstream Train Epoch: 11 [0/50000 (0%)]	Loss: 1.455223
Model None Epoch 11 Batch 0: Loss 1.4552229642868042
Model None Epoch 11 Batch 1: Loss 1.5464116334915161
Model None Epoch 11 Batch 2: Loss 1.4893553256988525
Model None Epoch 11 Batch 3: Loss 1.4204418659210205
Model None Epoch 11 Batch 4: Loss 1.3743736743927002
Model None Epoch 11 Batch 5: Loss 1.4338363409042358
Model None Epoch 11 Batch 6: Loss 1.6116241216659546
Model None Epoch 11 Batch 7: Loss 1.452418327331543
Model None Epoch 11 Batch 8: Loss 1.4751111268997192
Model None Epoch 11 Batch 9: Loss 1.5291006565093994
Model None Epoch 11 Batch 10: Loss 1.3930832147598267
Model None Epoch 11 Batch 11: Loss 1.4295135736465454
Model None Epoch 11 Batch 12: Loss 1.4673504829406738
Model None Epoch 11 Batch 13: Loss 1.3462131023406982
Model None Epoch 11 Batch 14: Loss 1.479014277458191
Model None Epoch 11 Batch 15: Loss 1.4968268871307373
Model None Epoch 11 Batch 16: Loss 1.4670628309249878
Model None Epoch 11 Batch 17: Loss 1.4675636291503906
Model None Epoch 11 Batch 18: Loss 1.5417534112930298
Model None Epoch 11 Batch 19: Loss 1.531592607498169
Model None Epoch 11 Batch 20: Loss 1.4072743654251099
Model None Epoch 11 Batch 21: Loss 1.3512955904006958
Model None Epoch 11 Batch 22: Loss 1.3585693836212158
Model None Epoch 11 Batch 23: Loss 1.527359127998352
Model None Epoch 11 Batch 24: Loss 1.455686092376709
Model None Epoch 11 Batch 25: Loss 1.3559761047363281
Model None Epoch 11 Batch 26: Loss 1.403161883354187
Model None Epoch 11 Batch 27: Loss 1.4075003862380981
Model None Epoch 11 Batch 28: Loss 1.4868640899658203
Model None Epoch 11 Batch 29: Loss 1.4555391073226929
Model None Epoch 11 Batch 30: Loss 1.2406586408615112
Model None Epoch 11 Batch 31: Loss 1.3772192001342773
Model None Epoch 11 Batch 32: Loss 1.3838304281234741
Model None Epoch 11 Batch 33: Loss 1.4726253747940063
Model None Epoch 11 Batch 34: Loss 1.3921475410461426
Model None Epoch 11 Batch 35: Loss 1.3328567743301392
Model None Epoch 11 Batch 36: Loss 1.4276442527770996
Model None Epoch 11 Batch 37: Loss 1.4366885423660278
Model None Epoch 11 Batch 38: Loss 1.501896858215332
Model None Epoch 11 Batch 39: Loss 1.4490776062011719
Model None Epoch 11 Batch 40: Loss 1.4839388132095337
Model None Epoch 11 Batch 41: Loss 1.5616021156311035
Model None Epoch 11 Batch 42: Loss 1.47659432888031
Model None Epoch 11 Batch 43: Loss 1.3611605167388916
Model None Epoch 11 Batch 44: Loss 1.4445538520812988
Model None Epoch 11 Batch 45: Loss 1.4329193830490112
Model None Epoch 11 Batch 46: Loss 1.439711570739746
Model None Epoch 11 Batch 47: Loss 1.553401231765747
Model None Epoch 11 Batch 48: Loss 1.4477819204330444
Model None Epoch 11 Batch 49: Loss 1.4745277166366577
Downstream Train Epoch: 11 [12800/50000 (26%)]	Loss: 1.515114
Model None Epoch 11 Batch 50: Loss 1.515114188194275
Model None Epoch 11 Batch 51: Loss 1.5008718967437744
Model None Epoch 11 Batch 52: Loss 1.360702633857727
Model None Epoch 11 Batch 53: Loss 1.4811298847198486
Model None Epoch 11 Batch 54: Loss 1.469265103340149
Model None Epoch 11 Batch 55: Loss 1.4965659379959106
Model None Epoch 11 Batch 56: Loss 1.5192782878875732
Model None Epoch 11 Batch 57: Loss 1.4223976135253906
Model None Epoch 11 Batch 58: Loss 1.4488368034362793
Model None Epoch 11 Batch 59: Loss 1.5209505558013916
Model None Epoch 11 Batch 60: Loss 1.6037745475769043
Model None Epoch 11 Batch 61: Loss 1.5402899980545044
Model None Epoch 11 Batch 62: Loss 1.2758827209472656
Model None Epoch 11 Batch 63: Loss 1.5556920766830444
Model None Epoch 11 Batch 64: Loss 1.4099879264831543
Model None Epoch 11 Batch 65: Loss 1.4917643070220947
Model None Epoch 11 Batch 66: Loss 1.4746301174163818
Model None Epoch 11 Batch 67: Loss 1.5474083423614502
Model None Epoch 11 Batch 68: Loss 1.426437258720398
Model None Epoch 11 Batch 69: Loss 1.4200663566589355
Model None Epoch 11 Batch 70: Loss 1.4073188304901123
Model None Epoch 11 Batch 71: Loss 1.5603866577148438
Model None Epoch 11 Batch 72: Loss 1.4986605644226074
Model None Epoch 11 Batch 73: Loss 1.544797420501709
Model None Epoch 11 Batch 74: Loss 1.424277901649475
Model None Epoch 11 Batch 75: Loss 1.4375054836273193
Model None Epoch 11 Batch 76: Loss 1.4374018907546997
Model None Epoch 11 Batch 77: Loss 1.4257330894470215
Model None Epoch 11 Batch 78: Loss 1.3954548835754395
Model None Epoch 11 Batch 79: Loss 1.4352500438690186
Model None Epoch 11 Batch 80: Loss 1.43389093875885
Model None Epoch 11 Batch 81: Loss 1.4827792644500732
Model None Epoch 11 Batch 82: Loss 1.4261187314987183
Model None Epoch 11 Batch 83: Loss 1.4175466299057007
Model None Epoch 11 Batch 84: Loss 1.3346818685531616
Model None Epoch 11 Batch 85: Loss 1.5682224035263062
Model None Epoch 11 Batch 86: Loss 1.4391238689422607
Model None Epoch 11 Batch 87: Loss 1.5811586380004883
Model None Epoch 11 Batch 88: Loss 1.4303960800170898
Model None Epoch 11 Batch 89: Loss 1.475611925125122
Model None Epoch 11 Batch 90: Loss 1.4169838428497314
Model None Epoch 11 Batch 91: Loss 1.4287586212158203
Model None Epoch 11 Batch 92: Loss 1.5390045642852783
Model None Epoch 11 Batch 93: Loss 1.4055366516113281
Model None Epoch 11 Batch 94: Loss 1.4278087615966797
Model None Epoch 11 Batch 95: Loss 1.495017409324646
Model None Epoch 11 Batch 96: Loss 1.470258116722107
Model None Epoch 11 Batch 97: Loss 1.3658082485198975
Model None Epoch 11 Batch 98: Loss 1.3325215578079224
Model None Epoch 11 Batch 99: Loss 1.584457278251648
Downstream Train Epoch: 11 [25600/50000 (51%)]	Loss: 1.412082
Model None Epoch 11 Batch 100: Loss 1.4120815992355347
Model None Epoch 11 Batch 101: Loss 1.3881505727767944
Model None Epoch 11 Batch 102: Loss 1.5859462022781372
Model None Epoch 11 Batch 103: Loss 1.5037275552749634
Model None Epoch 11 Batch 104: Loss 1.5213232040405273
Model None Epoch 11 Batch 105: Loss 1.467089295387268
Model None Epoch 11 Batch 106: Loss 1.423537254333496
Model None Epoch 11 Batch 107: Loss 1.452568769454956
Model None Epoch 11 Batch 108: Loss 1.4556623697280884
Model None Epoch 11 Batch 109: Loss 1.3205838203430176
Model None Epoch 11 Batch 110: Loss 1.3594342470169067
Model None Epoch 11 Batch 111: Loss 1.5214264392852783
Model None Epoch 11 Batch 112: Loss 1.4027879238128662
Model None Epoch 11 Batch 113: Loss 1.3852683305740356
Model None Epoch 11 Batch 114: Loss 1.3721071481704712
Model None Epoch 11 Batch 115: Loss 1.5099282264709473
Model None Epoch 11 Batch 116: Loss 1.4636791944503784
Model None Epoch 11 Batch 117: Loss 1.4098412990570068
Model None Epoch 11 Batch 118: Loss 1.4854607582092285
Model None Epoch 11 Batch 119: Loss 1.5428776741027832
Model None Epoch 11 Batch 120: Loss 1.507765769958496
Model None Epoch 11 Batch 121: Loss 1.460855484008789
Model None Epoch 11 Batch 122: Loss 1.5418452024459839
Model None Epoch 11 Batch 123: Loss 1.449302077293396
Model None Epoch 11 Batch 124: Loss 1.3930119276046753
Model None Epoch 11 Batch 125: Loss 1.6114497184753418
Model None Epoch 11 Batch 126: Loss 1.4266390800476074
Model None Epoch 11 Batch 127: Loss 1.585188627243042
Model None Epoch 11 Batch 128: Loss 1.3794788122177124
Model None Epoch 11 Batch 129: Loss 1.408894658088684
Model None Epoch 11 Batch 130: Loss 1.5025595426559448
Model None Epoch 11 Batch 131: Loss 1.4712656736373901
Model None Epoch 11 Batch 132: Loss 1.4724785089492798
Model None Epoch 11 Batch 133: Loss 1.4026730060577393
Model None Epoch 11 Batch 134: Loss 1.3518197536468506
Model None Epoch 11 Batch 135: Loss 1.4998294115066528
Model None Epoch 11 Batch 136: Loss 1.4197450876235962
Model None Epoch 11 Batch 137: Loss 1.4854066371917725
Model None Epoch 11 Batch 138: Loss 1.4376945495605469
Model None Epoch 11 Batch 139: Loss 1.500260829925537
Model None Epoch 11 Batch 140: Loss 1.4986156225204468
Model None Epoch 11 Batch 141: Loss 1.3930708169937134
Model None Epoch 11 Batch 142: Loss 1.4703682661056519
Model None Epoch 11 Batch 143: Loss 1.506011724472046
Model None Epoch 11 Batch 144: Loss 1.6007046699523926
Model None Epoch 11 Batch 145: Loss 1.36434805393219
Model None Epoch 11 Batch 146: Loss 1.4810484647750854
Model None Epoch 11 Batch 147: Loss 1.453141212463379
Model None Epoch 11 Batch 148: Loss 1.5312917232513428
Model None Epoch 11 Batch 149: Loss 1.3549989461898804
Downstream Train Epoch: 11 [38400/50000 (77%)]	Loss: 1.453140
Model None Epoch 11 Batch 150: Loss 1.4531396627426147
Model None Epoch 11 Batch 151: Loss 1.4759801626205444
Model None Epoch 11 Batch 152: Loss 1.462906837463379
Model None Epoch 11 Batch 153: Loss 1.470056414604187
Model None Epoch 11 Batch 154: Loss 1.5533063411712646
Model None Epoch 11 Batch 155: Loss 1.4469233751296997
Model None Epoch 11 Batch 156: Loss 1.3682119846343994
Model None Epoch 11 Batch 157: Loss 1.4195959568023682
Model None Epoch 11 Batch 158: Loss 1.2661731243133545
Model None Epoch 11 Batch 159: Loss 1.3786139488220215
Model None Epoch 11 Batch 160: Loss 1.4967384338378906
Model None Epoch 11 Batch 161: Loss 1.4573066234588623
Model None Epoch 11 Batch 162: Loss 1.4714535474777222
Model None Epoch 11 Batch 163: Loss 1.4194705486297607
Model None Epoch 11 Batch 164: Loss 1.4168444871902466
Model None Epoch 11 Batch 165: Loss 1.4296364784240723
Model None Epoch 11 Batch 166: Loss 1.5712000131607056
Model None Epoch 11 Batch 167: Loss 1.4390203952789307
Model None Epoch 11 Batch 168: Loss 1.3523991107940674
Model None Epoch 11 Batch 169: Loss 1.4412524700164795
Model None Epoch 11 Batch 170: Loss 1.5577168464660645
Model None Epoch 11 Batch 171: Loss 1.472188949584961
Model None Epoch 11 Batch 172: Loss 1.4515310525894165
Model None Epoch 11 Batch 173: Loss 1.5602259635925293
Model None Epoch 11 Batch 174: Loss 1.4542382955551147
Model None Epoch 11 Batch 175: Loss 1.5420445203781128
Model None Epoch 11 Batch 176: Loss 1.4469990730285645
Model None Epoch 11 Batch 177: Loss 1.611377477645874
Model None Epoch 11 Batch 178: Loss 1.4662970304489136
Model None Epoch 11 Batch 179: Loss 1.4515676498413086
Model None Epoch 11 Batch 180: Loss 1.4684181213378906
Model None Epoch 11 Batch 181: Loss 1.4376397132873535
Model None Epoch 11 Batch 182: Loss 1.5853288173675537
Model None Epoch 11 Batch 183: Loss 1.5244163274765015
Model None Epoch 11 Batch 184: Loss 1.4285845756530762
Model None Epoch 11 Batch 185: Loss 1.4311842918395996
Model None Epoch 11 Batch 186: Loss 1.395296573638916
Model None Epoch 11 Batch 187: Loss 1.4537752866744995
Model None Epoch 11 Batch 188: Loss 1.4011900424957275
Model None Epoch 11 Batch 189: Loss 1.4682608842849731
Model None Epoch 11 Batch 190: Loss 1.3369648456573486
Model None Epoch 11 Batch 191: Loss 1.4313652515411377
Model None Epoch 11 Batch 192: Loss 1.5210007429122925
Model None Epoch 11 Batch 193: Loss 1.4725966453552246
Model None Epoch 11 Batch 194: Loss 1.4156033992767334
Model None Epoch 11 Batch 195: Loss 1.4185551404953003

 Downstream Train loss: 1.4558739528364064 Acc: 0.5328
Downstream Train Epoch: 12 [0/50000 (0%)]	Loss: 1.368717
Model None Epoch 12 Batch 0: Loss 1.3687167167663574
Model None Epoch 12 Batch 1: Loss 1.4297963380813599
Model None Epoch 12 Batch 2: Loss 1.3372571468353271
Model None Epoch 12 Batch 3: Loss 1.5036606788635254
Model None Epoch 12 Batch 4: Loss 1.5413917303085327
Model None Epoch 12 Batch 5: Loss 1.4912514686584473
Model None Epoch 12 Batch 6: Loss 1.3887182474136353
Model None Epoch 12 Batch 7: Loss 1.542833924293518
Model None Epoch 12 Batch 8: Loss 1.6808245182037354
Model None Epoch 12 Batch 9: Loss 1.4700028896331787
Model None Epoch 12 Batch 10: Loss 1.5162694454193115
Model None Epoch 12 Batch 11: Loss 1.5895172357559204
Model None Epoch 12 Batch 12: Loss 1.5170559883117676
Model None Epoch 12 Batch 13: Loss 1.5160887241363525
Model None Epoch 12 Batch 14: Loss 1.379632592201233
Model None Epoch 12 Batch 15: Loss 1.462915062904358
Model None Epoch 12 Batch 16: Loss 1.5451467037200928
Model None Epoch 12 Batch 17: Loss 1.5599249601364136
Model None Epoch 12 Batch 18: Loss 1.4715033769607544
Model None Epoch 12 Batch 19: Loss 1.5053751468658447
Model None Epoch 12 Batch 20: Loss 1.4267903566360474
Model None Epoch 12 Batch 21: Loss 1.4593678712844849
Model None Epoch 12 Batch 22: Loss 1.4297184944152832
Model None Epoch 12 Batch 23: Loss 1.5172216892242432
Model None Epoch 12 Batch 24: Loss 1.448833703994751
Model None Epoch 12 Batch 25: Loss 1.5998740196228027
Model None Epoch 12 Batch 26: Loss 1.551742672920227
Model None Epoch 12 Batch 27: Loss 1.4922696352005005
Model None Epoch 12 Batch 28: Loss 1.4664424657821655
Model None Epoch 12 Batch 29: Loss 1.3988192081451416
Model None Epoch 12 Batch 30: Loss 1.4819164276123047
Model None Epoch 12 Batch 31: Loss 1.3882495164871216
Model None Epoch 12 Batch 32: Loss 1.4508826732635498
Model None Epoch 12 Batch 33: Loss 1.2518752813339233
Model None Epoch 12 Batch 34: Loss 1.4178704023361206
Model None Epoch 12 Batch 35: Loss 1.3678487539291382
Model None Epoch 12 Batch 36: Loss 1.4575387239456177
Model None Epoch 12 Batch 37: Loss 1.4496349096298218
Model None Epoch 12 Batch 38: Loss 1.48440420627594
Model None Epoch 12 Batch 39: Loss 1.3974823951721191
Model None Epoch 12 Batch 40: Loss 1.369463324546814
Model None Epoch 12 Batch 41: Loss 1.4105085134506226
Model None Epoch 12 Batch 42: Loss 1.4326465129852295
Model None Epoch 12 Batch 43: Loss 1.5595847368240356
Model None Epoch 12 Batch 44: Loss 1.3981668949127197
Model None Epoch 12 Batch 45: Loss 1.4868435859680176
Model None Epoch 12 Batch 46: Loss 1.4788705110549927
Model None Epoch 12 Batch 47: Loss 1.3711662292480469
Model None Epoch 12 Batch 48: Loss 1.5012422800064087
Model None Epoch 12 Batch 49: Loss 1.4468400478363037
Downstream Train Epoch: 12 [12800/50000 (26%)]	Loss: 1.504030
Model None Epoch 12 Batch 50: Loss 1.504029631614685
Model None Epoch 12 Batch 51: Loss 1.419363260269165
Model None Epoch 12 Batch 52: Loss 1.384596824645996
Model None Epoch 12 Batch 53: Loss 1.4266343116760254
Model None Epoch 12 Batch 54: Loss 1.551210880279541
Model None Epoch 12 Batch 55: Loss 1.4124716520309448
Model None Epoch 12 Batch 56: Loss 1.2930285930633545
Model None Epoch 12 Batch 57: Loss 1.5073081254959106
Model None Epoch 12 Batch 58: Loss 1.44825279712677
Model None Epoch 12 Batch 59: Loss 1.4487807750701904
Model None Epoch 12 Batch 60: Loss 1.4384469985961914
Model None Epoch 12 Batch 61: Loss 1.4134879112243652
Model None Epoch 12 Batch 62: Loss 1.4331982135772705
Model None Epoch 12 Batch 63: Loss 1.4321010112762451
Model None Epoch 12 Batch 64: Loss 1.5202564001083374
Model None Epoch 12 Batch 65: Loss 1.4614418745040894
Model None Epoch 12 Batch 66: Loss 1.4107757806777954
Model None Epoch 12 Batch 67: Loss 1.492335557937622
Model None Epoch 12 Batch 68: Loss 1.4280214309692383
Model None Epoch 12 Batch 69: Loss 1.5654258728027344
Model None Epoch 12 Batch 70: Loss 1.56500244140625
Model None Epoch 12 Batch 71: Loss 1.3214151859283447
Model None Epoch 12 Batch 72: Loss 1.4950138330459595
Model None Epoch 12 Batch 73: Loss 1.4064288139343262
Model None Epoch 12 Batch 74: Loss 1.41907799243927
Model None Epoch 12 Batch 75: Loss 1.4271992444992065
Model None Epoch 12 Batch 76: Loss 1.4432764053344727
Model None Epoch 12 Batch 77: Loss 1.405985713005066
Model None Epoch 12 Batch 78: Loss 1.5194591283798218
Model None Epoch 12 Batch 79: Loss 1.3091216087341309
Model None Epoch 12 Batch 80: Loss 1.4888933897018433
Model None Epoch 12 Batch 81: Loss 1.3802448511123657
Model None Epoch 12 Batch 82: Loss 1.4854234457015991
Model None Epoch 12 Batch 83: Loss 1.4749771356582642
Model None Epoch 12 Batch 84: Loss 1.4103912115097046
Model None Epoch 12 Batch 85: Loss 1.4572497606277466
Model None Epoch 12 Batch 86: Loss 1.373849868774414
Model None Epoch 12 Batch 87: Loss 1.4127092361450195
Model None Epoch 12 Batch 88: Loss 1.4524974822998047
Model None Epoch 12 Batch 89: Loss 1.4715536832809448
Model None Epoch 12 Batch 90: Loss 1.3439823389053345
Model None Epoch 12 Batch 91: Loss 1.4361376762390137
Model None Epoch 12 Batch 92: Loss 1.4341613054275513
Model None Epoch 12 Batch 93: Loss 1.4020966291427612
Model None Epoch 12 Batch 94: Loss 1.5369853973388672
Model None Epoch 12 Batch 95: Loss 1.4258464574813843
Model None Epoch 12 Batch 96: Loss 1.472711443901062
Model None Epoch 12 Batch 97: Loss 1.3959777355194092
Model None Epoch 12 Batch 98: Loss 1.427444577217102
Model None Epoch 12 Batch 99: Loss 1.540065050125122
Downstream Train Epoch: 12 [25600/50000 (51%)]	Loss: 1.375874
Model None Epoch 12 Batch 100: Loss 1.3758739233016968
Model None Epoch 12 Batch 101: Loss 1.5539755821228027
Model None Epoch 12 Batch 102: Loss 1.4557777643203735
Model None Epoch 12 Batch 103: Loss 1.314703106880188
Model None Epoch 12 Batch 104: Loss 1.2171159982681274
Model None Epoch 12 Batch 105: Loss 1.4783200025558472
Model None Epoch 12 Batch 106: Loss 1.523274302482605
Model None Epoch 12 Batch 107: Loss 1.4333148002624512
Model None Epoch 12 Batch 108: Loss 1.4478533267974854
Model None Epoch 12 Batch 109: Loss 1.3436440229415894
Model None Epoch 12 Batch 110: Loss 1.4241677522659302
Model None Epoch 12 Batch 111: Loss 1.4027600288391113
Model None Epoch 12 Batch 112: Loss 1.4773439168930054
Model None Epoch 12 Batch 113: Loss 1.5668872594833374
Model None Epoch 12 Batch 114: Loss 1.5412485599517822
Model None Epoch 12 Batch 115: Loss 1.4105689525604248
Model None Epoch 12 Batch 116: Loss 1.3668819665908813
Model None Epoch 12 Batch 117: Loss 1.505597710609436
Model None Epoch 12 Batch 118: Loss 1.407500982284546
Model None Epoch 12 Batch 119: Loss 1.434252381324768
Model None Epoch 12 Batch 120: Loss 1.36073899269104
Model None Epoch 12 Batch 121: Loss 1.4681129455566406
Model None Epoch 12 Batch 122: Loss 1.5650969743728638
Model None Epoch 12 Batch 123: Loss 1.5355430841445923
Model None Epoch 12 Batch 124: Loss 1.4534679651260376
Model None Epoch 12 Batch 125: Loss 1.4877328872680664
Model None Epoch 12 Batch 126: Loss 1.4482684135437012
Model None Epoch 12 Batch 127: Loss 1.4933205842971802
Model None Epoch 12 Batch 128: Loss 1.4918389320373535
Model None Epoch 12 Batch 129: Loss 1.5362266302108765
Model None Epoch 12 Batch 130: Loss 1.4351879358291626
Model None Epoch 12 Batch 131: Loss 1.5113697052001953
Model None Epoch 12 Batch 132: Loss 1.5515543222427368
Model None Epoch 12 Batch 133: Loss 1.419848918914795
Model None Epoch 12 Batch 134: Loss 1.5566564798355103
Model None Epoch 12 Batch 135: Loss 1.495600938796997
Model None Epoch 12 Batch 136: Loss 1.52059805393219
Model None Epoch 12 Batch 137: Loss 1.5357571840286255
Model None Epoch 12 Batch 138: Loss 1.4911127090454102
Model None Epoch 12 Batch 139: Loss 1.4843089580535889
Model None Epoch 12 Batch 140: Loss 1.3764792680740356
Model None Epoch 12 Batch 141: Loss 1.3642911911010742
Model None Epoch 12 Batch 142: Loss 1.5981165170669556
Model None Epoch 12 Batch 143: Loss 1.3796443939208984
Model None Epoch 12 Batch 144: Loss 1.502055048942566
Model None Epoch 12 Batch 145: Loss 1.477210521697998
Model None Epoch 12 Batch 146: Loss 1.408966302871704
Model None Epoch 12 Batch 147: Loss 1.387419581413269
Model None Epoch 12 Batch 148: Loss 1.4042991399765015
Model None Epoch 12 Batch 149: Loss 1.3857600688934326
Downstream Train Epoch: 12 [38400/50000 (77%)]	Loss: 1.468406
Model None Epoch 12 Batch 150: Loss 1.468405842781067
Model None Epoch 12 Batch 151: Loss 1.4136238098144531
Model None Epoch 12 Batch 152: Loss 1.3962050676345825
Model None Epoch 12 Batch 153: Loss 1.4693660736083984
Model None Epoch 12 Batch 154: Loss 1.45335853099823
Model None Epoch 12 Batch 155: Loss 1.4956297874450684
Model None Epoch 12 Batch 156: Loss 1.5269829034805298
Model None Epoch 12 Batch 157: Loss 1.5383226871490479
Model None Epoch 12 Batch 158: Loss 1.4216489791870117
Model None Epoch 12 Batch 159: Loss 1.4616668224334717
Model None Epoch 12 Batch 160: Loss 1.4296261072158813
Model None Epoch 12 Batch 161: Loss 1.4061850309371948
Model None Epoch 12 Batch 162: Loss 1.4791858196258545
Model None Epoch 12 Batch 163: Loss 1.405055046081543
Model None Epoch 12 Batch 164: Loss 1.5008083581924438
Model None Epoch 12 Batch 165: Loss 1.4350471496582031
Model None Epoch 12 Batch 166: Loss 1.3825372457504272
Model None Epoch 12 Batch 167: Loss 1.4113932847976685
Model None Epoch 12 Batch 168: Loss 1.5006496906280518
Model None Epoch 12 Batch 169: Loss 1.4518779516220093
Model None Epoch 12 Batch 170: Loss 1.580445647239685
Model None Epoch 12 Batch 171: Loss 1.405208945274353
Model None Epoch 12 Batch 172: Loss 1.524987816810608
Model None Epoch 12 Batch 173: Loss 1.4041407108306885
Model None Epoch 12 Batch 174: Loss 1.41615891456604
Model None Epoch 12 Batch 175: Loss 1.4283300638198853
Model None Epoch 12 Batch 176: Loss 1.4046080112457275
Model None Epoch 12 Batch 177: Loss 1.4351956844329834
Model None Epoch 12 Batch 178: Loss 1.5112318992614746
Model None Epoch 12 Batch 179: Loss 1.451545238494873
Model None Epoch 12 Batch 180: Loss 1.3010720014572144
Model None Epoch 12 Batch 181: Loss 1.4684500694274902
Model None Epoch 12 Batch 182: Loss 1.470139503479004
Model None Epoch 12 Batch 183: Loss 1.373776912689209
Model None Epoch 12 Batch 184: Loss 1.479378342628479
Model None Epoch 12 Batch 185: Loss 1.5045278072357178
Model None Epoch 12 Batch 186: Loss 1.512978434562683
Model None Epoch 12 Batch 187: Loss 1.545868158340454
Model None Epoch 12 Batch 188: Loss 1.3890289068222046
Model None Epoch 12 Batch 189: Loss 1.505561113357544
Model None Epoch 12 Batch 190: Loss 1.5085067749023438
Model None Epoch 12 Batch 191: Loss 1.4457398653030396
Model None Epoch 12 Batch 192: Loss 1.430677056312561
Model None Epoch 12 Batch 193: Loss 1.4493136405944824
Model None Epoch 12 Batch 194: Loss 1.5710041522979736
Model None Epoch 12 Batch 195: Loss 1.3102779388427734

 Downstream Train loss: 1.4541924626243359 Acc: 0.5328
Downstream Train Epoch: 13 [0/50000 (0%)]	Loss: 1.332222
Model None Epoch 13 Batch 0: Loss 1.332221508026123
Model None Epoch 13 Batch 1: Loss 1.407366394996643
Model None Epoch 13 Batch 2: Loss 1.4525052309036255
Model None Epoch 13 Batch 3: Loss 1.3477224111557007
Model None Epoch 13 Batch 4: Loss 1.5207277536392212
Model None Epoch 13 Batch 5: Loss 1.349273920059204
Model None Epoch 13 Batch 6: Loss 1.4345755577087402
Model None Epoch 13 Batch 7: Loss 1.4134613275527954
Model None Epoch 13 Batch 8: Loss 1.4154857397079468
Model None Epoch 13 Batch 9: Loss 1.4896315336227417
Model None Epoch 13 Batch 10: Loss 1.5243799686431885
Model None Epoch 13 Batch 11: Loss 1.5365716218948364
Model None Epoch 13 Batch 12: Loss 1.3923866748809814
Model None Epoch 13 Batch 13: Loss 1.4193187952041626
Model None Epoch 13 Batch 14: Loss 1.4988315105438232
Model None Epoch 13 Batch 15: Loss 1.56661856174469
Model None Epoch 13 Batch 16: Loss 1.3720704317092896
Model None Epoch 13 Batch 17: Loss 1.5273573398590088
Model None Epoch 13 Batch 18: Loss 1.5603224039077759
Model None Epoch 13 Batch 19: Loss 1.5111290216445923
Model None Epoch 13 Batch 20: Loss 1.498780608177185
Model None Epoch 13 Batch 21: Loss 1.399638056755066
Model None Epoch 13 Batch 22: Loss 1.4717676639556885
Model None Epoch 13 Batch 23: Loss 1.4755717515945435
Model None Epoch 13 Batch 24: Loss 1.4158800840377808
Model None Epoch 13 Batch 25: Loss 1.4148558378219604
Model None Epoch 13 Batch 26: Loss 1.5284161567687988
Model None Epoch 13 Batch 27: Loss 1.4110157489776611
Model None Epoch 13 Batch 28: Loss 1.4912523031234741
Model None Epoch 13 Batch 29: Loss 1.475917100906372
Model None Epoch 13 Batch 30: Loss 1.4736098051071167
Model None Epoch 13 Batch 31: Loss 1.5353713035583496
Model None Epoch 13 Batch 32: Loss 1.44584321975708
Model None Epoch 13 Batch 33: Loss 1.4514600038528442
Model None Epoch 13 Batch 34: Loss 1.3289059400558472
Model None Epoch 13 Batch 35: Loss 1.3625292778015137
Model None Epoch 13 Batch 36: Loss 1.4557018280029297
Model None Epoch 13 Batch 37: Loss 1.5620157718658447
Model None Epoch 13 Batch 38: Loss 1.4953337907791138
Model None Epoch 13 Batch 39: Loss 1.4224790334701538
Model None Epoch 13 Batch 40: Loss 1.4852399826049805
Model None Epoch 13 Batch 41: Loss 1.5285472869873047
Model None Epoch 13 Batch 42: Loss 1.4842212200164795
Model None Epoch 13 Batch 43: Loss 1.4723364114761353
Model None Epoch 13 Batch 44: Loss 1.384015679359436
Model None Epoch 13 Batch 45: Loss 1.400141716003418
Model None Epoch 13 Batch 46: Loss 1.3235200643539429
Model None Epoch 13 Batch 47: Loss 1.4172977209091187
Model None Epoch 13 Batch 48: Loss 1.431644082069397
Model None Epoch 13 Batch 49: Loss 1.4611467123031616
Downstream Train Epoch: 13 [12800/50000 (26%)]	Loss: 1.470434
Model None Epoch 13 Batch 50: Loss 1.4704335927963257
Model None Epoch 13 Batch 51: Loss 1.4180892705917358
Model None Epoch 13 Batch 52: Loss 1.5215325355529785
Model None Epoch 13 Batch 53: Loss 1.5246106386184692
Model None Epoch 13 Batch 54: Loss 1.538367509841919
Model None Epoch 13 Batch 55: Loss 1.4819130897521973
Model None Epoch 13 Batch 56: Loss 1.4996060132980347
Model None Epoch 13 Batch 57: Loss 1.32839834690094
Model None Epoch 13 Batch 58: Loss 1.4246771335601807
Model None Epoch 13 Batch 59: Loss 1.6779862642288208
Model None Epoch 13 Batch 60: Loss 1.41990327835083
Model None Epoch 13 Batch 61: Loss 1.3873571157455444
Model None Epoch 13 Batch 62: Loss 1.432747483253479
Model None Epoch 13 Batch 63: Loss 1.478116750717163
Model None Epoch 13 Batch 64: Loss 1.4404373168945312
Model None Epoch 13 Batch 65: Loss 1.4531463384628296
Model None Epoch 13 Batch 66: Loss 1.435085654258728
Model None Epoch 13 Batch 67: Loss 1.4470348358154297
Model None Epoch 13 Batch 68: Loss 1.4641611576080322
Model None Epoch 13 Batch 69: Loss 1.4908219575881958
Model None Epoch 13 Batch 70: Loss 1.5651332139968872
Model None Epoch 13 Batch 71: Loss 1.441862940788269
Model None Epoch 13 Batch 72: Loss 1.3645869493484497
Model None Epoch 13 Batch 73: Loss 1.4262094497680664
Model None Epoch 13 Batch 74: Loss 1.5462194681167603
Model None Epoch 13 Batch 75: Loss 1.4396185874938965
Model None Epoch 13 Batch 76: Loss 1.5021071434020996
Model None Epoch 13 Batch 77: Loss 1.5461599826812744
Model None Epoch 13 Batch 78: Loss 1.4366730451583862
Model None Epoch 13 Batch 79: Loss 1.4114112854003906
Model None Epoch 13 Batch 80: Loss 1.4549096822738647
Model None Epoch 13 Batch 81: Loss 1.4409695863723755
Model None Epoch 13 Batch 82: Loss 1.4610249996185303
Model None Epoch 13 Batch 83: Loss 1.4143757820129395
Model None Epoch 13 Batch 84: Loss 1.468705654144287
Model None Epoch 13 Batch 85: Loss 1.5406111478805542
Model None Epoch 13 Batch 86: Loss 1.445876121520996
Model None Epoch 13 Batch 87: Loss 1.4502625465393066
Model None Epoch 13 Batch 88: Loss 1.5219935178756714
Model None Epoch 13 Batch 89: Loss 1.5370805263519287
Model None Epoch 13 Batch 90: Loss 1.5104665756225586
Model None Epoch 13 Batch 91: Loss 1.387247920036316
Model None Epoch 13 Batch 92: Loss 1.3857464790344238
Model None Epoch 13 Batch 93: Loss 1.4781173467636108
Model None Epoch 13 Batch 94: Loss 1.4887738227844238
Model None Epoch 13 Batch 95: Loss 1.4156545400619507
Model None Epoch 13 Batch 96: Loss 1.3786104917526245
Model None Epoch 13 Batch 97: Loss 1.395155906677246
Model None Epoch 13 Batch 98: Loss 1.5081017017364502
Model None Epoch 13 Batch 99: Loss 1.4311341047286987
Downstream Train Epoch: 13 [25600/50000 (51%)]	Loss: 1.376220
Model None Epoch 13 Batch 100: Loss 1.3762198686599731
Model None Epoch 13 Batch 101: Loss 1.4153568744659424
Model None Epoch 13 Batch 102: Loss 1.4004828929901123
Model None Epoch 13 Batch 103: Loss 1.4773768186569214
Model None Epoch 13 Batch 104: Loss 1.408836841583252
Model None Epoch 13 Batch 105: Loss 1.4834785461425781
Model None Epoch 13 Batch 106: Loss 1.4307280778884888
Model None Epoch 13 Batch 107: Loss 1.4490675926208496
Model None Epoch 13 Batch 108: Loss 1.5314619541168213
Model None Epoch 13 Batch 109: Loss 1.539017677307129
Model None Epoch 13 Batch 110: Loss 1.3787076473236084
Model None Epoch 13 Batch 111: Loss 1.3817863464355469
Model None Epoch 13 Batch 112: Loss 1.418779969215393
Model None Epoch 13 Batch 113: Loss 1.307733178138733
Model None Epoch 13 Batch 114: Loss 1.4734013080596924
Model None Epoch 13 Batch 115: Loss 1.3893474340438843
Model None Epoch 13 Batch 116: Loss 1.4354567527770996
Model None Epoch 13 Batch 117: Loss 1.5547616481781006
Model None Epoch 13 Batch 118: Loss 1.3955954313278198
Model None Epoch 13 Batch 119: Loss 1.4230220317840576
Model None Epoch 13 Batch 120: Loss 1.4517579078674316
Model None Epoch 13 Batch 121: Loss 1.484427571296692
Model None Epoch 13 Batch 122: Loss 1.5077390670776367
Model None Epoch 13 Batch 123: Loss 1.4697390794754028
Model None Epoch 13 Batch 124: Loss 1.5391162633895874
Model None Epoch 13 Batch 125: Loss 1.4406658411026
Model None Epoch 13 Batch 126: Loss 1.5640928745269775
Model None Epoch 13 Batch 127: Loss 1.559836983680725
Model None Epoch 13 Batch 128: Loss 1.4251606464385986
Model None Epoch 13 Batch 129: Loss 1.4682461023330688
Model None Epoch 13 Batch 130: Loss 1.3444687128067017
Model None Epoch 13 Batch 131: Loss 1.4552397727966309
Model None Epoch 13 Batch 132: Loss 1.4428868293762207
Model None Epoch 13 Batch 133: Loss 1.3748551607131958
Model None Epoch 13 Batch 134: Loss 1.4611822366714478
Model None Epoch 13 Batch 135: Loss 1.2898378372192383
Model None Epoch 13 Batch 136: Loss 1.4525567293167114
Model None Epoch 13 Batch 137: Loss 1.3812154531478882
Model None Epoch 13 Batch 138: Loss 1.4413864612579346
Model None Epoch 13 Batch 139: Loss 1.3449310064315796
Model None Epoch 13 Batch 140: Loss 1.3967840671539307
Model None Epoch 13 Batch 141: Loss 1.4667667150497437
Model None Epoch 13 Batch 142: Loss 1.5357856750488281
Model None Epoch 13 Batch 143: Loss 1.3962503671646118
Model None Epoch 13 Batch 144: Loss 1.4670206308364868
Model None Epoch 13 Batch 145: Loss 1.4485100507736206
Model None Epoch 13 Batch 146: Loss 1.4454277753829956
Model None Epoch 13 Batch 147: Loss 1.4437114000320435
Model None Epoch 13 Batch 148: Loss 1.5386401414871216
Model None Epoch 13 Batch 149: Loss 1.4977765083312988
Downstream Train Epoch: 13 [38400/50000 (77%)]	Loss: 1.461235
Model None Epoch 13 Batch 150: Loss 1.46123468875885
Model None Epoch 13 Batch 151: Loss 1.4945021867752075
Model None Epoch 13 Batch 152: Loss 1.4096167087554932
Model None Epoch 13 Batch 153: Loss 1.4284253120422363
Model None Epoch 13 Batch 154: Loss 1.4562921524047852
Model None Epoch 13 Batch 155: Loss 1.4651838541030884
Model None Epoch 13 Batch 156: Loss 1.446020245552063
Model None Epoch 13 Batch 157: Loss 1.504077672958374
Model None Epoch 13 Batch 158: Loss 1.4927619695663452
Model None Epoch 13 Batch 159: Loss 1.4241867065429688
Model None Epoch 13 Batch 160: Loss 1.4432337284088135
Model None Epoch 13 Batch 161: Loss 1.249237298965454
Model None Epoch 13 Batch 162: Loss 1.3802584409713745
Model None Epoch 13 Batch 163: Loss 1.4617727994918823
Model None Epoch 13 Batch 164: Loss 1.5820086002349854
Model None Epoch 13 Batch 165: Loss 1.4579118490219116
Model None Epoch 13 Batch 166: Loss 1.4720348119735718
Model None Epoch 13 Batch 167: Loss 1.5170605182647705
Model None Epoch 13 Batch 168: Loss 1.5198514461517334
Model None Epoch 13 Batch 169: Loss 1.541906476020813
Model None Epoch 13 Batch 170: Loss 1.357559323310852
Model None Epoch 13 Batch 171: Loss 1.5305265188217163
Model None Epoch 13 Batch 172: Loss 1.5252206325531006
Model None Epoch 13 Batch 173: Loss 1.42947256565094
Model None Epoch 13 Batch 174: Loss 1.4661697149276733
Model None Epoch 13 Batch 175: Loss 1.385251760482788
Model None Epoch 13 Batch 176: Loss 1.462591528892517
Model None Epoch 13 Batch 177: Loss 1.4213000535964966
Model None Epoch 13 Batch 178: Loss 1.5135531425476074
Model None Epoch 13 Batch 179: Loss 1.3993366956710815
Model None Epoch 13 Batch 180: Loss 1.4141920804977417
Model None Epoch 13 Batch 181: Loss 1.3626996278762817
Model None Epoch 13 Batch 182: Loss 1.4680020809173584
Model None Epoch 13 Batch 183: Loss 1.428821086883545
Model None Epoch 13 Batch 184: Loss 1.492637038230896
Model None Epoch 13 Batch 185: Loss 1.3890717029571533
Model None Epoch 13 Batch 186: Loss 1.5273436307907104
Model None Epoch 13 Batch 187: Loss 1.5686711072921753
Model None Epoch 13 Batch 188: Loss 1.4810662269592285
Model None Epoch 13 Batch 189: Loss 1.344356894493103
Model None Epoch 13 Batch 190: Loss 1.45370614528656
Model None Epoch 13 Batch 191: Loss 1.4561249017715454
Model None Epoch 13 Batch 192: Loss 1.4395023584365845
Model None Epoch 13 Batch 193: Loss 1.4839472770690918
Model None Epoch 13 Batch 194: Loss 1.5623751878738403
Model None Epoch 13 Batch 195: Loss 1.2952468395233154

 Downstream Train loss: 1.452952035835811 Acc: 0.5428
Downstream Train Epoch: 14 [0/50000 (0%)]	Loss: 1.453481
Model None Epoch 14 Batch 0: Loss 1.45348060131073
Model None Epoch 14 Batch 1: Loss 1.441541314125061
Model None Epoch 14 Batch 2: Loss 1.3655636310577393
Model None Epoch 14 Batch 3: Loss 1.4583330154418945
Model None Epoch 14 Batch 4: Loss 1.469801664352417
Model None Epoch 14 Batch 5: Loss 1.5955414772033691
Model None Epoch 14 Batch 6: Loss 1.4667649269104004
Model None Epoch 14 Batch 7: Loss 1.4124419689178467
Model None Epoch 14 Batch 8: Loss 1.512619972229004
Model None Epoch 14 Batch 9: Loss 1.4002406597137451
Model None Epoch 14 Batch 10: Loss 1.3750048875808716
Model None Epoch 14 Batch 11: Loss 1.43239426612854
Model None Epoch 14 Batch 12: Loss 1.4581725597381592
Model None Epoch 14 Batch 13: Loss 1.4390424489974976
Model None Epoch 14 Batch 14: Loss 1.4379602670669556
Model None Epoch 14 Batch 15: Loss 1.3828046321868896
Model None Epoch 14 Batch 16: Loss 1.3740612268447876
Model None Epoch 14 Batch 17: Loss 1.4656522274017334
Model None Epoch 14 Batch 18: Loss 1.4170185327529907
Model None Epoch 14 Batch 19: Loss 1.4445081949234009
Model None Epoch 14 Batch 20: Loss 1.4926049709320068
Model None Epoch 14 Batch 21: Loss 1.5159627199172974
Model None Epoch 14 Batch 22: Loss 1.4060678482055664
Model None Epoch 14 Batch 23: Loss 1.4536802768707275
Model None Epoch 14 Batch 24: Loss 1.3619216680526733
Model None Epoch 14 Batch 25: Loss 1.4625645875930786
Model None Epoch 14 Batch 26: Loss 1.6145784854888916
Model None Epoch 14 Batch 27: Loss 1.3935142755508423
Model None Epoch 14 Batch 28: Loss 1.4875320196151733
Model None Epoch 14 Batch 29: Loss 1.495892882347107
Model None Epoch 14 Batch 30: Loss 1.540001392364502
Model None Epoch 14 Batch 31: Loss 1.4138423204421997
Model None Epoch 14 Batch 32: Loss 1.500036597251892
Model None Epoch 14 Batch 33: Loss 1.3582537174224854
Model None Epoch 14 Batch 34: Loss 1.4404561519622803
Model None Epoch 14 Batch 35: Loss 1.4506598711013794
Model None Epoch 14 Batch 36: Loss 1.4680463075637817
Model None Epoch 14 Batch 37: Loss 1.3639631271362305
Model None Epoch 14 Batch 38: Loss 1.4506313800811768
Model None Epoch 14 Batch 39: Loss 1.3854433298110962
Model None Epoch 14 Batch 40: Loss 1.5194569826126099
Model None Epoch 14 Batch 41: Loss 1.3639370203018188
Model None Epoch 14 Batch 42: Loss 1.518474817276001
Model None Epoch 14 Batch 43: Loss 1.441963791847229
Model None Epoch 14 Batch 44: Loss 1.4488412141799927
Model None Epoch 14 Batch 45: Loss 1.5068986415863037
Model None Epoch 14 Batch 46: Loss 1.3772343397140503
Model None Epoch 14 Batch 47: Loss 1.5205451250076294
Model None Epoch 14 Batch 48: Loss 1.4131416082382202
Model None Epoch 14 Batch 49: Loss 1.495240330696106
Downstream Train Epoch: 14 [12800/50000 (26%)]	Loss: 1.372837
Model None Epoch 14 Batch 50: Loss 1.372836709022522
Model None Epoch 14 Batch 51: Loss 1.2682448625564575
Model None Epoch 14 Batch 52: Loss 1.3355287313461304
Model None Epoch 14 Batch 53: Loss 1.54616117477417
Model None Epoch 14 Batch 54: Loss 1.423579454421997
Model None Epoch 14 Batch 55: Loss 1.3837976455688477
Model None Epoch 14 Batch 56: Loss 1.422694444656372
Model None Epoch 14 Batch 57: Loss 1.4381930828094482
Model None Epoch 14 Batch 58: Loss 1.4567816257476807
Model None Epoch 14 Batch 59: Loss 1.4221080541610718
Model None Epoch 14 Batch 60: Loss 1.4953186511993408
Model None Epoch 14 Batch 61: Loss 1.437949538230896
Model None Epoch 14 Batch 62: Loss 1.4653489589691162
Model None Epoch 14 Batch 63: Loss 1.4499906301498413
Model None Epoch 14 Batch 64: Loss 1.4130140542984009
Model None Epoch 14 Batch 65: Loss 1.5241676568984985
Model None Epoch 14 Batch 66: Loss 1.4746965169906616
Model None Epoch 14 Batch 67: Loss 1.445157766342163
Model None Epoch 14 Batch 68: Loss 1.4484390020370483
Model None Epoch 14 Batch 69: Loss 1.44412100315094
Model None Epoch 14 Batch 70: Loss 1.4365571737289429
Model None Epoch 14 Batch 71: Loss 1.4958819150924683
Model None Epoch 14 Batch 72: Loss 1.4697027206420898
Model None Epoch 14 Batch 73: Loss 1.6294329166412354
Model None Epoch 14 Batch 74: Loss 1.4289443492889404
Model None Epoch 14 Batch 75: Loss 1.3584779500961304
Model None Epoch 14 Batch 76: Loss 1.4309930801391602
Model None Epoch 14 Batch 77: Loss 1.3545889854431152
Model None Epoch 14 Batch 78: Loss 1.53327476978302
Model None Epoch 14 Batch 79: Loss 1.4670419692993164
Model None Epoch 14 Batch 80: Loss 1.4894013404846191
Model None Epoch 14 Batch 81: Loss 1.349092960357666
Model None Epoch 14 Batch 82: Loss 1.3994321823120117
Model None Epoch 14 Batch 83: Loss 1.4439336061477661
Model None Epoch 14 Batch 84: Loss 1.4778550863265991
Model None Epoch 14 Batch 85: Loss 1.4522814750671387
Model None Epoch 14 Batch 86: Loss 1.4271986484527588
Model None Epoch 14 Batch 87: Loss 1.4910801649093628
Model None Epoch 14 Batch 88: Loss 1.475972294807434
Model None Epoch 14 Batch 89: Loss 1.309151530265808
Model None Epoch 14 Batch 90: Loss 1.501131296157837
Model None Epoch 14 Batch 91: Loss 1.5384292602539062
Model None Epoch 14 Batch 92: Loss 1.5719414949417114
Model None Epoch 14 Batch 93: Loss 1.322577714920044
Model None Epoch 14 Batch 94: Loss 1.3821697235107422
Model None Epoch 14 Batch 95: Loss 1.3671811819076538
Model None Epoch 14 Batch 96: Loss 1.4740841388702393
Model None Epoch 14 Batch 97: Loss 1.364586591720581
Model None Epoch 14 Batch 98: Loss 1.4566360712051392
Model None Epoch 14 Batch 99: Loss 1.3922436237335205
Downstream Train Epoch: 14 [25600/50000 (51%)]	Loss: 1.454880
Model None Epoch 14 Batch 100: Loss 1.4548801183700562
Model None Epoch 14 Batch 101: Loss 1.4362655878067017
Model None Epoch 14 Batch 102: Loss 1.3666608333587646
Model None Epoch 14 Batch 103: Loss 1.5152486562728882
Model None Epoch 14 Batch 104: Loss 1.5039961338043213
Model None Epoch 14 Batch 105: Loss 1.4463266134262085
Model None Epoch 14 Batch 106: Loss 1.5305346250534058
Model None Epoch 14 Batch 107: Loss 1.4313708543777466
Model None Epoch 14 Batch 108: Loss 1.5425159931182861
Model None Epoch 14 Batch 109: Loss 1.4595731496810913
Model None Epoch 14 Batch 110: Loss 1.4849282503128052
Model None Epoch 14 Batch 111: Loss 1.5828005075454712
Model None Epoch 14 Batch 112: Loss 1.4362133741378784
Model None Epoch 14 Batch 113: Loss 1.4394150972366333
Model None Epoch 14 Batch 114: Loss 1.3606542348861694
Model None Epoch 14 Batch 115: Loss 1.3465875387191772
Model None Epoch 14 Batch 116: Loss 1.4353382587432861
Model None Epoch 14 Batch 117: Loss 1.495221495628357
Model None Epoch 14 Batch 118: Loss 1.5009422302246094
Model None Epoch 14 Batch 119: Loss 1.3766858577728271
Model None Epoch 14 Batch 120: Loss 1.3053596019744873
Model None Epoch 14 Batch 121: Loss 1.3944460153579712
Model None Epoch 14 Batch 122: Loss 1.4404423236846924
Model None Epoch 14 Batch 123: Loss 1.4005194902420044
Model None Epoch 14 Batch 124: Loss 1.452221393585205
Model None Epoch 14 Batch 125: Loss 1.4497771263122559
Model None Epoch 14 Batch 126: Loss 1.3984297513961792
Model None Epoch 14 Batch 127: Loss 1.43517005443573
Model None Epoch 14 Batch 128: Loss 1.3677858114242554
Model None Epoch 14 Batch 129: Loss 1.5287896394729614
Model None Epoch 14 Batch 130: Loss 1.4251338243484497
Model None Epoch 14 Batch 131: Loss 1.4629862308502197
Model None Epoch 14 Batch 132: Loss 1.5595139265060425
Model None Epoch 14 Batch 133: Loss 1.532313585281372
Model None Epoch 14 Batch 134: Loss 1.4260629415512085
Model None Epoch 14 Batch 135: Loss 1.3657968044281006
Model None Epoch 14 Batch 136: Loss 1.412994623184204
Model None Epoch 14 Batch 137: Loss 1.4971109628677368
Model None Epoch 14 Batch 138: Loss 1.5344460010528564
Model None Epoch 14 Batch 139: Loss 1.530916690826416
Model None Epoch 14 Batch 140: Loss 1.5475313663482666
Model None Epoch 14 Batch 141: Loss 1.4135178327560425
Model None Epoch 14 Batch 142: Loss 1.4911417961120605
Model None Epoch 14 Batch 143: Loss 1.403550386428833
Model None Epoch 14 Batch 144: Loss 1.4969227313995361
Model None Epoch 14 Batch 145: Loss 1.4102575778961182
Model None Epoch 14 Batch 146: Loss 1.4397977590560913
Model None Epoch 14 Batch 147: Loss 1.476287841796875
Model None Epoch 14 Batch 148: Loss 1.4579607248306274
Model None Epoch 14 Batch 149: Loss 1.3952606916427612
Downstream Train Epoch: 14 [38400/50000 (77%)]	Loss: 1.485180
Model None Epoch 14 Batch 150: Loss 1.4851802587509155
Model None Epoch 14 Batch 151: Loss 1.4397087097167969
Model None Epoch 14 Batch 152: Loss 1.495082974433899
Model None Epoch 14 Batch 153: Loss 1.493019700050354
Model None Epoch 14 Batch 154: Loss 1.5177135467529297
Model None Epoch 14 Batch 155: Loss 1.454960584640503
Model None Epoch 14 Batch 156: Loss 1.343184471130371
Model None Epoch 14 Batch 157: Loss 1.4147580862045288
Model None Epoch 14 Batch 158: Loss 1.4417002201080322
Model None Epoch 14 Batch 159: Loss 1.389801025390625
Model None Epoch 14 Batch 160: Loss 1.3743619918823242
Model None Epoch 14 Batch 161: Loss 1.5783370733261108
Model None Epoch 14 Batch 162: Loss 1.2814764976501465
Model None Epoch 14 Batch 163: Loss 1.4948500394821167
Model None Epoch 14 Batch 164: Loss 1.3682115077972412
Model None Epoch 14 Batch 165: Loss 1.4651938676834106
Model None Epoch 14 Batch 166: Loss 1.3357254266738892
Model None Epoch 14 Batch 167: Loss 1.5134010314941406
Model None Epoch 14 Batch 168: Loss 1.4066907167434692
Model None Epoch 14 Batch 169: Loss 1.362376093864441
Model None Epoch 14 Batch 170: Loss 1.2693557739257812
Model None Epoch 14 Batch 171: Loss 1.4660247564315796
Model None Epoch 14 Batch 172: Loss 1.3344285488128662
Model None Epoch 14 Batch 173: Loss 1.3099758625030518
Model None Epoch 14 Batch 174: Loss 1.484744906425476
Model None Epoch 14 Batch 175: Loss 1.4301296472549438
Model None Epoch 14 Batch 176: Loss 1.6480830907821655
Model None Epoch 14 Batch 177: Loss 1.3939540386199951
Model None Epoch 14 Batch 178: Loss 1.4282492399215698
Model None Epoch 14 Batch 179: Loss 1.372689127922058
Model None Epoch 14 Batch 180: Loss 1.4192116260528564
Model None Epoch 14 Batch 181: Loss 1.412891149520874
Model None Epoch 14 Batch 182: Loss 1.5333945751190186
Model None Epoch 14 Batch 183: Loss 1.4723581075668335
Model None Epoch 14 Batch 184: Loss 1.4598362445831299
Model None Epoch 14 Batch 185: Loss 1.5194016695022583
Model None Epoch 14 Batch 186: Loss 1.409425973892212
Model None Epoch 14 Batch 187: Loss 1.4451887607574463
Model None Epoch 14 Batch 188: Loss 1.426322340965271
Model None Epoch 14 Batch 189: Loss 1.3130003213882446
Model None Epoch 14 Batch 190: Loss 1.3946651220321655
Model None Epoch 14 Batch 191: Loss 1.3681061267852783
Model None Epoch 14 Batch 192: Loss 1.438194990158081
Model None Epoch 14 Batch 193: Loss 1.3689639568328857
Model None Epoch 14 Batch 194: Loss 1.4116804599761963
Model None Epoch 14 Batch 195: Loss 1.3817800283432007

 Downstream Train loss: 1.441786414506484 Acc: 0.5428
Downstream Train Epoch: 15 [0/50000 (0%)]	Loss: 1.399484
Model None Epoch 15 Batch 0: Loss 1.3994842767715454
Model None Epoch 15 Batch 1: Loss 1.3712615966796875
Model None Epoch 15 Batch 2: Loss 1.405580759048462
Model None Epoch 15 Batch 3: Loss 1.5227559804916382
Model None Epoch 15 Batch 4: Loss 1.4244898557662964
Model None Epoch 15 Batch 5: Loss 1.454477071762085
Model None Epoch 15 Batch 6: Loss 1.533915638923645
Model None Epoch 15 Batch 7: Loss 1.4009416103363037
Model None Epoch 15 Batch 8: Loss 1.5140610933303833
Model None Epoch 15 Batch 9: Loss 1.47002375125885
Model None Epoch 15 Batch 10: Loss 1.4831634759902954
Model None Epoch 15 Batch 11: Loss 1.4482172727584839
Model None Epoch 15 Batch 12: Loss 1.398016333580017
Model None Epoch 15 Batch 13: Loss 1.4080804586410522
Model None Epoch 15 Batch 14: Loss 1.4597890377044678
Model None Epoch 15 Batch 15: Loss 1.476163387298584
Model None Epoch 15 Batch 16: Loss 1.4520888328552246
Model None Epoch 15 Batch 17: Loss 1.4780826568603516
Model None Epoch 15 Batch 18: Loss 1.416450023651123
Model None Epoch 15 Batch 19: Loss 1.4072136878967285
Model None Epoch 15 Batch 20: Loss 1.4466766119003296
Model None Epoch 15 Batch 21: Loss 1.2811342477798462
Model None Epoch 15 Batch 22: Loss 1.5264079570770264
Model None Epoch 15 Batch 23: Loss 1.3293814659118652
Model None Epoch 15 Batch 24: Loss 1.4192776679992676
Model None Epoch 15 Batch 25: Loss 1.4158940315246582
Model None Epoch 15 Batch 26: Loss 1.446009635925293
Model None Epoch 15 Batch 27: Loss 1.5219703912734985
Model None Epoch 15 Batch 28: Loss 1.3249460458755493
Model None Epoch 15 Batch 29: Loss 1.4334886074066162
Model None Epoch 15 Batch 30: Loss 1.4927719831466675
Model None Epoch 15 Batch 31: Loss 1.325874924659729
Model None Epoch 15 Batch 32: Loss 1.3891972303390503
Model None Epoch 15 Batch 33: Loss 1.4152402877807617
Model None Epoch 15 Batch 34: Loss 1.3900076150894165
Model None Epoch 15 Batch 35: Loss 1.4021226167678833
Model None Epoch 15 Batch 36: Loss 1.474853515625
Model None Epoch 15 Batch 37: Loss 1.4915578365325928
Model None Epoch 15 Batch 38: Loss 1.4190621376037598
Model None Epoch 15 Batch 39: Loss 1.4426113367080688
Model None Epoch 15 Batch 40: Loss 1.3594472408294678
Model None Epoch 15 Batch 41: Loss 1.5325593948364258
Model None Epoch 15 Batch 42: Loss 1.6201121807098389
Model None Epoch 15 Batch 43: Loss 1.41221284866333
Model None Epoch 15 Batch 44: Loss 1.4112398624420166
Model None Epoch 15 Batch 45: Loss 1.3458492755889893
Model None Epoch 15 Batch 46: Loss 1.4367376565933228
Model None Epoch 15 Batch 47: Loss 1.5331816673278809
Model None Epoch 15 Batch 48: Loss 1.4589990377426147
Model None Epoch 15 Batch 49: Loss 1.3534551858901978
Downstream Train Epoch: 15 [12800/50000 (26%)]	Loss: 1.346115
Model None Epoch 15 Batch 50: Loss 1.3461153507232666
Model None Epoch 15 Batch 51: Loss 1.4376208782196045
Model None Epoch 15 Batch 52: Loss 1.4118454456329346
Model None Epoch 15 Batch 53: Loss 1.4445388317108154
Model None Epoch 15 Batch 54: Loss 1.4652621746063232
Model None Epoch 15 Batch 55: Loss 1.4606832265853882
Model None Epoch 15 Batch 56: Loss 1.3216038942337036
Model None Epoch 15 Batch 57: Loss 1.3888276815414429
Model None Epoch 15 Batch 58: Loss 1.4632689952850342
Model None Epoch 15 Batch 59: Loss 1.4906389713287354
Model None Epoch 15 Batch 60: Loss 1.4276808500289917
Model None Epoch 15 Batch 61: Loss 1.5617235898971558
Model None Epoch 15 Batch 62: Loss 1.4333361387252808
Model None Epoch 15 Batch 63: Loss 1.4668443202972412
Model None Epoch 15 Batch 64: Loss 1.3731859922409058
Model None Epoch 15 Batch 65: Loss 1.4553154706954956
Model None Epoch 15 Batch 66: Loss 1.3767203092575073
Model None Epoch 15 Batch 67: Loss 1.5104312896728516
Model None Epoch 15 Batch 68: Loss 1.5542391538619995
Model None Epoch 15 Batch 69: Loss 1.499569058418274
Model None Epoch 15 Batch 70: Loss 1.441037654876709
Model None Epoch 15 Batch 71: Loss 1.5121647119522095
Model None Epoch 15 Batch 72: Loss 1.5432368516921997
Model None Epoch 15 Batch 73: Loss 1.3978919982910156
Model None Epoch 15 Batch 74: Loss 1.4428222179412842
Model None Epoch 15 Batch 75: Loss 1.6952831745147705
Model None Epoch 15 Batch 76: Loss 1.3958369493484497
Model None Epoch 15 Batch 77: Loss 1.3431105613708496
Model None Epoch 15 Batch 78: Loss 1.413820505142212
Model None Epoch 15 Batch 79: Loss 1.4659734964370728
Model None Epoch 15 Batch 80: Loss 1.535630702972412
Model None Epoch 15 Batch 81: Loss 1.4687749147415161
Model None Epoch 15 Batch 82: Loss 1.377532958984375
Model None Epoch 15 Batch 83: Loss 1.3298572301864624
Model None Epoch 15 Batch 84: Loss 1.3142268657684326
Model None Epoch 15 Batch 85: Loss 1.4642189741134644
Model None Epoch 15 Batch 86: Loss 1.4728233814239502
Model None Epoch 15 Batch 87: Loss 1.4086745977401733
Model None Epoch 15 Batch 88: Loss 1.4923840761184692
Model None Epoch 15 Batch 89: Loss 1.5094704627990723
Model None Epoch 15 Batch 90: Loss 1.3770015239715576
Model None Epoch 15 Batch 91: Loss 1.5283277034759521
Model None Epoch 15 Batch 92: Loss 1.3765431642532349
Model None Epoch 15 Batch 93: Loss 1.338999629020691
Model None Epoch 15 Batch 94: Loss 1.45421302318573
Model None Epoch 15 Batch 95: Loss 1.42803156375885
Model None Epoch 15 Batch 96: Loss 1.5220731496810913
Model None Epoch 15 Batch 97: Loss 1.416507363319397
Model None Epoch 15 Batch 98: Loss 1.3538055419921875
Model None Epoch 15 Batch 99: Loss 1.3852695226669312
Downstream Train Epoch: 15 [25600/50000 (51%)]	Loss: 1.480200
Model None Epoch 15 Batch 100: Loss 1.4801996946334839
Model None Epoch 15 Batch 101: Loss 1.4678184986114502
Model None Epoch 15 Batch 102: Loss 1.4702476263046265
Model None Epoch 15 Batch 103: Loss 1.5333609580993652
Model None Epoch 15 Batch 104: Loss 1.3404505252838135
Model None Epoch 15 Batch 105: Loss 1.5532231330871582
Model None Epoch 15 Batch 106: Loss 1.325880527496338
Model None Epoch 15 Batch 107: Loss 1.5107499361038208
Model None Epoch 15 Batch 108: Loss 1.5109516382217407
Model None Epoch 15 Batch 109: Loss 1.3917698860168457
Model None Epoch 15 Batch 110: Loss 1.4549022912979126
Model None Epoch 15 Batch 111: Loss 1.4836527109146118
Model None Epoch 15 Batch 112: Loss 1.4133740663528442
Model None Epoch 15 Batch 113: Loss 1.5632038116455078
Model None Epoch 15 Batch 114: Loss 1.5388840436935425
Model None Epoch 15 Batch 115: Loss 1.6015113592147827
Model None Epoch 15 Batch 116: Loss 1.3437612056732178
Model None Epoch 15 Batch 117: Loss 1.3606938123703003
Model None Epoch 15 Batch 118: Loss 1.4561845064163208
Model None Epoch 15 Batch 119: Loss 1.5403320789337158
Model None Epoch 15 Batch 120: Loss 1.5132160186767578
Model None Epoch 15 Batch 121: Loss 1.5263651609420776
Model None Epoch 15 Batch 122: Loss 1.42924964427948
Model None Epoch 15 Batch 123: Loss 1.4466443061828613
Model None Epoch 15 Batch 124: Loss 1.4227747917175293
Model None Epoch 15 Batch 125: Loss 1.6412265300750732
Model None Epoch 15 Batch 126: Loss 1.4682079553604126
Model None Epoch 15 Batch 127: Loss 1.5206154584884644
Model None Epoch 15 Batch 128: Loss 1.3896840810775757
Model None Epoch 15 Batch 129: Loss 1.2982388734817505
Model None Epoch 15 Batch 130: Loss 1.3940837383270264
Model None Epoch 15 Batch 131: Loss 1.463232159614563
Model None Epoch 15 Batch 132: Loss 1.3941560983657837
Model None Epoch 15 Batch 133: Loss 1.418937087059021
Model None Epoch 15 Batch 134: Loss 1.440645694732666
Model None Epoch 15 Batch 135: Loss 1.3819674253463745
Model None Epoch 15 Batch 136: Loss 1.467376947402954
Model None Epoch 15 Batch 137: Loss 1.3154938220977783
Model None Epoch 15 Batch 138: Loss 1.3804666996002197
Model None Epoch 15 Batch 139: Loss 1.35136079788208
Model None Epoch 15 Batch 140: Loss 1.3787156343460083
Model None Epoch 15 Batch 141: Loss 1.4687589406967163
Model None Epoch 15 Batch 142: Loss 1.5124434232711792
Model None Epoch 15 Batch 143: Loss 1.4333178997039795
Model None Epoch 15 Batch 144: Loss 1.511054515838623
Model None Epoch 15 Batch 145: Loss 1.4612817764282227
Model None Epoch 15 Batch 146: Loss 1.4844027757644653
Model None Epoch 15 Batch 147: Loss 1.4363350868225098
Model None Epoch 15 Batch 148: Loss 1.4494258165359497
Model None Epoch 15 Batch 149: Loss 1.3463064432144165
Downstream Train Epoch: 15 [38400/50000 (77%)]	Loss: 1.562447
Model None Epoch 15 Batch 150: Loss 1.5624473094940186
Model None Epoch 15 Batch 151: Loss 1.5040661096572876
Model None Epoch 15 Batch 152: Loss 1.5012528896331787
Model None Epoch 15 Batch 153: Loss 1.3766703605651855
Model None Epoch 15 Batch 154: Loss 1.3568142652511597
Model None Epoch 15 Batch 155: Loss 1.4374675750732422
Model None Epoch 15 Batch 156: Loss 1.5238556861877441
Model None Epoch 15 Batch 157: Loss 1.4119763374328613
Model None Epoch 15 Batch 158: Loss 1.388581395149231
Model None Epoch 15 Batch 159: Loss 1.406687617301941
Model None Epoch 15 Batch 160: Loss 1.522732138633728
Model None Epoch 15 Batch 161: Loss 1.4120694398880005
Model None Epoch 15 Batch 162: Loss 1.4387226104736328
Model None Epoch 15 Batch 163: Loss 1.5046724081039429
Model None Epoch 15 Batch 164: Loss 1.3449163436889648
Model None Epoch 15 Batch 165: Loss 1.388908863067627
Model None Epoch 15 Batch 166: Loss 1.377119779586792
Model None Epoch 15 Batch 167: Loss 1.3935682773590088
Model None Epoch 15 Batch 168: Loss 1.317556619644165
Model None Epoch 15 Batch 169: Loss 1.4006156921386719
Model None Epoch 15 Batch 170: Loss 1.431635856628418
Model None Epoch 15 Batch 171: Loss 1.360090970993042
Model None Epoch 15 Batch 172: Loss 1.3377997875213623
Model None Epoch 15 Batch 173: Loss 1.4997609853744507
Model None Epoch 15 Batch 174: Loss 1.4564145803451538
Model None Epoch 15 Batch 175: Loss 1.3413842916488647
Model None Epoch 15 Batch 176: Loss 1.5759613513946533
Model None Epoch 15 Batch 177: Loss 1.4541573524475098
Model None Epoch 15 Batch 178: Loss 1.3429055213928223
Model None Epoch 15 Batch 179: Loss 1.557671308517456
Model None Epoch 15 Batch 180: Loss 1.4072108268737793
Model None Epoch 15 Batch 181: Loss 1.4535415172576904
Model None Epoch 15 Batch 182: Loss 1.539711356163025
Model None Epoch 15 Batch 183: Loss 1.4373066425323486
Model None Epoch 15 Batch 184: Loss 1.559448480606079
Model None Epoch 15 Batch 185: Loss 1.439173698425293
Model None Epoch 15 Batch 186: Loss 1.3951051235198975
Model None Epoch 15 Batch 187: Loss 1.4084810018539429
Model None Epoch 15 Batch 188: Loss 1.4195600748062134
Model None Epoch 15 Batch 189: Loss 1.388777494430542
Model None Epoch 15 Batch 190: Loss 1.4080549478530884
Model None Epoch 15 Batch 191: Loss 1.5028784275054932
Model None Epoch 15 Batch 192: Loss 1.515282154083252
Model None Epoch 15 Batch 193: Loss 1.472581148147583
Model None Epoch 15 Batch 194: Loss 1.4397584199905396
Model None Epoch 15 Batch 195: Loss 1.6082223653793335

 Downstream Train loss: 1.4422563709774796 Acc: 0.5428
Downstream Train Epoch: 16 [0/50000 (0%)]	Loss: 1.392328
Model None Epoch 16 Batch 0: Loss 1.3923280239105225
Model None Epoch 16 Batch 1: Loss 1.4184468984603882
Model None Epoch 16 Batch 2: Loss 1.4436362981796265
Model None Epoch 16 Batch 3: Loss 1.3509000539779663
Model None Epoch 16 Batch 4: Loss 1.5297608375549316
Model None Epoch 16 Batch 5: Loss 1.4889020919799805
Model None Epoch 16 Batch 6: Loss 1.484508991241455
Model None Epoch 16 Batch 7: Loss 1.4349597692489624
Model None Epoch 16 Batch 8: Loss 1.4411832094192505
Model None Epoch 16 Batch 9: Loss 1.4876078367233276
Model None Epoch 16 Batch 10: Loss 1.3353602886199951
Model None Epoch 16 Batch 11: Loss 1.3197507858276367
Model None Epoch 16 Batch 12: Loss 1.5023270845413208
Model None Epoch 16 Batch 13: Loss 1.3545724153518677
Model None Epoch 16 Batch 14: Loss 1.405497431755066
Model None Epoch 16 Batch 15: Loss 1.2841870784759521
Model None Epoch 16 Batch 16: Loss 1.4715367555618286
Model None Epoch 16 Batch 17: Loss 1.3535270690917969
Model None Epoch 16 Batch 18: Loss 1.4325824975967407
Model None Epoch 16 Batch 19: Loss 1.4786407947540283
Model None Epoch 16 Batch 20: Loss 1.4778072834014893
Model None Epoch 16 Batch 21: Loss 1.3712410926818848
Model None Epoch 16 Batch 22: Loss 1.4636682271957397
Model None Epoch 16 Batch 23: Loss 1.4289931058883667
Model None Epoch 16 Batch 24: Loss 1.4098973274230957
Model None Epoch 16 Batch 25: Loss 1.3693311214447021
Model None Epoch 16 Batch 26: Loss 1.4929310083389282
Model None Epoch 16 Batch 27: Loss 1.416643500328064
Model None Epoch 16 Batch 28: Loss 1.426709532737732
Model None Epoch 16 Batch 29: Loss 1.434242844581604
Model None Epoch 16 Batch 30: Loss 1.4328683614730835
Model None Epoch 16 Batch 31: Loss 1.4934297800064087
Model None Epoch 16 Batch 32: Loss 1.371748924255371
Model None Epoch 16 Batch 33: Loss 1.4321385622024536
Model None Epoch 16 Batch 34: Loss 1.398572564125061
Model None Epoch 16 Batch 35: Loss 1.43574059009552
Model None Epoch 16 Batch 36: Loss 1.493042230606079
Model None Epoch 16 Batch 37: Loss 1.44607675075531
Model None Epoch 16 Batch 38: Loss 1.4650975465774536
Model None Epoch 16 Batch 39: Loss 1.4613069295883179
Model None Epoch 16 Batch 40: Loss 1.4698803424835205
Model None Epoch 16 Batch 41: Loss 1.5121554136276245
Model None Epoch 16 Batch 42: Loss 1.3458678722381592
Model None Epoch 16 Batch 43: Loss 1.443235993385315
Model None Epoch 16 Batch 44: Loss 1.4363852739334106
Model None Epoch 16 Batch 45: Loss 1.4547053575515747
Model None Epoch 16 Batch 46: Loss 1.4538027048110962
Model None Epoch 16 Batch 47: Loss 1.373524785041809
Model None Epoch 16 Batch 48: Loss 1.5221598148345947
Model None Epoch 16 Batch 49: Loss 1.4831392765045166
Downstream Train Epoch: 16 [12800/50000 (26%)]	Loss: 1.350579
Model None Epoch 16 Batch 50: Loss 1.3505792617797852
Model None Epoch 16 Batch 51: Loss 1.3728796243667603
Model None Epoch 16 Batch 52: Loss 1.5458869934082031
Model None Epoch 16 Batch 53: Loss 1.4805805683135986
Model None Epoch 16 Batch 54: Loss 1.390505313873291
Model None Epoch 16 Batch 55: Loss 1.4459774494171143
Model None Epoch 16 Batch 56: Loss 1.4168883562088013
Model None Epoch 16 Batch 57: Loss 1.3916354179382324
Model None Epoch 16 Batch 58: Loss 1.4394444227218628
Model None Epoch 16 Batch 59: Loss 1.3449957370758057
Model None Epoch 16 Batch 60: Loss 1.4853641986846924
Model None Epoch 16 Batch 61: Loss 1.4116777181625366
Model None Epoch 16 Batch 62: Loss 1.3772401809692383
Model None Epoch 16 Batch 63: Loss 1.5038045644760132
Model None Epoch 16 Batch 64: Loss 1.5418965816497803
Model None Epoch 16 Batch 65: Loss 1.5095220804214478
Model None Epoch 16 Batch 66: Loss 1.416164755821228
Model None Epoch 16 Batch 67: Loss 1.523358941078186
Model None Epoch 16 Batch 68: Loss 1.4623862504959106
Model None Epoch 16 Batch 69: Loss 1.3151134252548218
Model None Epoch 16 Batch 70: Loss 1.3740208148956299
Model None Epoch 16 Batch 71: Loss 1.4388889074325562
Model None Epoch 16 Batch 72: Loss 1.4605462551116943
Model None Epoch 16 Batch 73: Loss 1.3070327043533325
Model None Epoch 16 Batch 74: Loss 1.486554503440857
Model None Epoch 16 Batch 75: Loss 1.4651652574539185
Model None Epoch 16 Batch 76: Loss 1.5144222974777222
Model None Epoch 16 Batch 77: Loss 1.5304814577102661
Model None Epoch 16 Batch 78: Loss 1.375163197517395
Model None Epoch 16 Batch 79: Loss 1.4132355451583862
Model None Epoch 16 Batch 80: Loss 1.539429783821106
Model None Epoch 16 Batch 81: Loss 1.4140007495880127
Model None Epoch 16 Batch 82: Loss 1.4001567363739014
Model None Epoch 16 Batch 83: Loss 1.4909687042236328
Model None Epoch 16 Batch 84: Loss 1.4074710607528687
Model None Epoch 16 Batch 85: Loss 1.3611137866973877
Model None Epoch 16 Batch 86: Loss 1.4226902723312378
Model None Epoch 16 Batch 87: Loss 1.5696356296539307
Model None Epoch 16 Batch 88: Loss 1.3419486284255981
Model None Epoch 16 Batch 89: Loss 1.4862104654312134
Model None Epoch 16 Batch 90: Loss 1.457006573677063
Model None Epoch 16 Batch 91: Loss 1.3822332620620728
Model None Epoch 16 Batch 92: Loss 1.402456283569336
Model None Epoch 16 Batch 93: Loss 1.400424599647522
Model None Epoch 16 Batch 94: Loss 1.406170129776001
Model None Epoch 16 Batch 95: Loss 1.4253758192062378
Model None Epoch 16 Batch 96: Loss 1.4482511281967163
Model None Epoch 16 Batch 97: Loss 1.2480171918869019
Model None Epoch 16 Batch 98: Loss 1.4158921241760254
Model None Epoch 16 Batch 99: Loss 1.4067696332931519
Downstream Train Epoch: 16 [25600/50000 (51%)]	Loss: 1.406241
Model None Epoch 16 Batch 100: Loss 1.4062411785125732
Model None Epoch 16 Batch 101: Loss 1.2646796703338623
Model None Epoch 16 Batch 102: Loss 1.532355785369873
Model None Epoch 16 Batch 103: Loss 1.3860782384872437
Model None Epoch 16 Batch 104: Loss 1.3722259998321533
Model None Epoch 16 Batch 105: Loss 1.5078611373901367
Model None Epoch 16 Batch 106: Loss 1.4655821323394775
Model None Epoch 16 Batch 107: Loss 1.419169545173645
Model None Epoch 16 Batch 108: Loss 1.4217723608016968
Model None Epoch 16 Batch 109: Loss 1.3666408061981201
Model None Epoch 16 Batch 110: Loss 1.5145889520645142
Model None Epoch 16 Batch 111: Loss 1.3677057027816772
Model None Epoch 16 Batch 112: Loss 1.413672924041748
Model None Epoch 16 Batch 113: Loss 1.5357826948165894
Model None Epoch 16 Batch 114: Loss 1.442735195159912
Model None Epoch 16 Batch 115: Loss 1.4167933464050293
Model None Epoch 16 Batch 116: Loss 1.4877128601074219
Model None Epoch 16 Batch 117: Loss 1.5954889059066772
Model None Epoch 16 Batch 118: Loss 1.3727624416351318
Model None Epoch 16 Batch 119: Loss 1.3120859861373901
Model None Epoch 16 Batch 120: Loss 1.3785169124603271
Model None Epoch 16 Batch 121: Loss 1.386789321899414
Model None Epoch 16 Batch 122: Loss 1.420162558555603
Model None Epoch 16 Batch 123: Loss 1.4770911931991577
Model None Epoch 16 Batch 124: Loss 1.4076443910598755
Model None Epoch 16 Batch 125: Loss 1.4791524410247803
Model None Epoch 16 Batch 126: Loss 1.4440923929214478
Model None Epoch 16 Batch 127: Loss 1.4016916751861572
Model None Epoch 16 Batch 128: Loss 1.347461223602295
Model None Epoch 16 Batch 129: Loss 1.44486403465271
Model None Epoch 16 Batch 130: Loss 1.502707839012146
Model None Epoch 16 Batch 131: Loss 1.4522285461425781
Model None Epoch 16 Batch 132: Loss 1.5263638496398926
Model None Epoch 16 Batch 133: Loss 1.56832754611969
Model None Epoch 16 Batch 134: Loss 1.4911730289459229
Model None Epoch 16 Batch 135: Loss 1.3518937826156616
Model None Epoch 16 Batch 136: Loss 1.4056111574172974
Model None Epoch 16 Batch 137: Loss 1.3928136825561523
Model None Epoch 16 Batch 138: Loss 1.360666275024414
Model None Epoch 16 Batch 139: Loss 1.4264500141143799
Model None Epoch 16 Batch 140: Loss 1.5039312839508057
Model None Epoch 16 Batch 141: Loss 1.320573329925537
Model None Epoch 16 Batch 142: Loss 1.5142529010772705
Model None Epoch 16 Batch 143: Loss 1.383851170539856
Model None Epoch 16 Batch 144: Loss 1.4113103151321411
Model None Epoch 16 Batch 145: Loss 1.340261697769165
Model None Epoch 16 Batch 146: Loss 1.472872018814087
Model None Epoch 16 Batch 147: Loss 1.4039602279663086
Model None Epoch 16 Batch 148: Loss 1.4578678607940674
Model None Epoch 16 Batch 149: Loss 1.4457134008407593
Downstream Train Epoch: 16 [38400/50000 (77%)]	Loss: 1.403535
Model None Epoch 16 Batch 150: Loss 1.4035354852676392
Model None Epoch 16 Batch 151: Loss 1.4204686880111694
Model None Epoch 16 Batch 152: Loss 1.3231406211853027
Model None Epoch 16 Batch 153: Loss 1.4075905084609985
Model None Epoch 16 Batch 154: Loss 1.5169466733932495
Model None Epoch 16 Batch 155: Loss 1.489436149597168
Model None Epoch 16 Batch 156: Loss 1.4132812023162842
Model None Epoch 16 Batch 157: Loss 1.4716976881027222
Model None Epoch 16 Batch 158: Loss 1.3525869846343994
Model None Epoch 16 Batch 159: Loss 1.4710047245025635
Model None Epoch 16 Batch 160: Loss 1.369852900505066
Model None Epoch 16 Batch 161: Loss 1.4741506576538086
Model None Epoch 16 Batch 162: Loss 1.4514155387878418
Model None Epoch 16 Batch 163: Loss 1.4050216674804688
Model None Epoch 16 Batch 164: Loss 1.3808419704437256
Model None Epoch 16 Batch 165: Loss 1.4846402406692505
Model None Epoch 16 Batch 166: Loss 1.479187250137329
Model None Epoch 16 Batch 167: Loss 1.5141664743423462
Model None Epoch 16 Batch 168: Loss 1.4600417613983154
Model None Epoch 16 Batch 169: Loss 1.4865773916244507
Model None Epoch 16 Batch 170: Loss 1.5101934671401978
Model None Epoch 16 Batch 171: Loss 1.458233118057251
Model None Epoch 16 Batch 172: Loss 1.371616244316101
Model None Epoch 16 Batch 173: Loss 1.407173752784729
Model None Epoch 16 Batch 174: Loss 1.3997820615768433
Model None Epoch 16 Batch 175: Loss 1.2884234189987183
Model None Epoch 16 Batch 176: Loss 1.4000065326690674
Model None Epoch 16 Batch 177: Loss 1.4497650861740112
Model None Epoch 16 Batch 178: Loss 1.4828170537948608
Model None Epoch 16 Batch 179: Loss 1.4076509475708008
Model None Epoch 16 Batch 180: Loss 1.428902268409729
Model None Epoch 16 Batch 181: Loss 1.4667965173721313
Model None Epoch 16 Batch 182: Loss 1.3727996349334717
Model None Epoch 16 Batch 183: Loss 1.4781805276870728
Model None Epoch 16 Batch 184: Loss 1.5103278160095215
Model None Epoch 16 Batch 185: Loss 1.3861076831817627
Model None Epoch 16 Batch 186: Loss 1.4715030193328857
Model None Epoch 16 Batch 187: Loss 1.4112602472305298
Model None Epoch 16 Batch 188: Loss 1.4207613468170166
Model None Epoch 16 Batch 189: Loss 1.4349621534347534
Model None Epoch 16 Batch 190: Loss 1.3511219024658203
Model None Epoch 16 Batch 191: Loss 1.4342516660690308
Model None Epoch 16 Batch 192: Loss 1.573352575302124
Model None Epoch 16 Batch 193: Loss 1.437836766242981
Model None Epoch 16 Batch 194: Loss 1.4709619283676147
Model None Epoch 16 Batch 195: Loss 1.3734792470932007

 Downstream Train loss: 1.4318381590502602 Acc: 0.5428
Downstream Train Epoch: 17 [0/50000 (0%)]	Loss: 1.454323
Model None Epoch 17 Batch 0: Loss 1.4543228149414062
Model None Epoch 17 Batch 1: Loss 1.4796578884124756
Model None Epoch 17 Batch 2: Loss 1.5028369426727295
Model None Epoch 17 Batch 3: Loss 1.4131827354431152
Model None Epoch 17 Batch 4: Loss 1.6048951148986816
Model None Epoch 17 Batch 5: Loss 1.403658151626587
Model None Epoch 17 Batch 6: Loss 1.304846167564392
Model None Epoch 17 Batch 7: Loss 1.4847244024276733
Model None Epoch 17 Batch 8: Loss 1.4576489925384521
Model None Epoch 17 Batch 9: Loss 1.3671553134918213
Model None Epoch 17 Batch 10: Loss 1.3816577196121216
Model None Epoch 17 Batch 11: Loss 1.4056376218795776
Model None Epoch 17 Batch 12: Loss 1.3841043710708618
Model None Epoch 17 Batch 13: Loss 1.4405591487884521
Model None Epoch 17 Batch 14: Loss 1.5108028650283813
Model None Epoch 17 Batch 15: Loss 1.4198042154312134
Model None Epoch 17 Batch 16: Loss 1.4050631523132324
Model None Epoch 17 Batch 17: Loss 1.417640209197998
Model None Epoch 17 Batch 18: Loss 1.4752368927001953
Model None Epoch 17 Batch 19: Loss 1.3954209089279175
Model None Epoch 17 Batch 20: Loss 1.3664427995681763
Model None Epoch 17 Batch 21: Loss 1.411428689956665
Model None Epoch 17 Batch 22: Loss 1.3947391510009766
Model None Epoch 17 Batch 23: Loss 1.5234286785125732
Model None Epoch 17 Batch 24: Loss 1.3703327178955078
Model None Epoch 17 Batch 25: Loss 1.4451851844787598
Model None Epoch 17 Batch 26: Loss 1.4345234632492065
Model None Epoch 17 Batch 27: Loss 1.4443045854568481
Model None Epoch 17 Batch 28: Loss 1.4387032985687256
Model None Epoch 17 Batch 29: Loss 1.4181824922561646
Model None Epoch 17 Batch 30: Loss 1.4962677955627441
Model None Epoch 17 Batch 31: Loss 1.4208790063858032
Model None Epoch 17 Batch 32: Loss 1.3258147239685059
Model None Epoch 17 Batch 33: Loss 1.409751057624817
Model None Epoch 17 Batch 34: Loss 1.3833197355270386
Model None Epoch 17 Batch 35: Loss 1.5006444454193115
Model None Epoch 17 Batch 36: Loss 1.4838570356369019
Model None Epoch 17 Batch 37: Loss 1.4952609539031982
Model None Epoch 17 Batch 38: Loss 1.4291397333145142
Model None Epoch 17 Batch 39: Loss 1.3601008653640747
Model None Epoch 17 Batch 40: Loss 1.3806325197219849
Model None Epoch 17 Batch 41: Loss 1.4099302291870117
Model None Epoch 17 Batch 42: Loss 1.3984274864196777
Model None Epoch 17 Batch 43: Loss 1.3586519956588745
Model None Epoch 17 Batch 44: Loss 1.4523357152938843
Model None Epoch 17 Batch 45: Loss 1.4309197664260864
Model None Epoch 17 Batch 46: Loss 1.3699734210968018
Model None Epoch 17 Batch 47: Loss 1.5488824844360352
Model None Epoch 17 Batch 48: Loss 1.4541172981262207
Model None Epoch 17 Batch 49: Loss 1.4210573434829712
Downstream Train Epoch: 17 [12800/50000 (26%)]	Loss: 1.409858
Model None Epoch 17 Batch 50: Loss 1.4098577499389648
Model None Epoch 17 Batch 51: Loss 1.4059319496154785
Model None Epoch 17 Batch 52: Loss 1.4455158710479736
Model None Epoch 17 Batch 53: Loss 1.4488192796707153
Model None Epoch 17 Batch 54: Loss 1.519493579864502
Model None Epoch 17 Batch 55: Loss 1.4917603731155396
Model None Epoch 17 Batch 56: Loss 1.4200083017349243
Model None Epoch 17 Batch 57: Loss 1.4102228879928589
Model None Epoch 17 Batch 58: Loss 1.361332893371582
Model None Epoch 17 Batch 59: Loss 1.260166049003601
Model None Epoch 17 Batch 60: Loss 1.3431721925735474
Model None Epoch 17 Batch 61: Loss 1.3713093996047974
Model None Epoch 17 Batch 62: Loss 1.446190595626831
Model None Epoch 17 Batch 63: Loss 1.5812664031982422
Model None Epoch 17 Batch 64: Loss 1.463423728942871
Model None Epoch 17 Batch 65: Loss 1.4447402954101562
Model None Epoch 17 Batch 66: Loss 1.446763038635254
Model None Epoch 17 Batch 67: Loss 1.365397572517395
Model None Epoch 17 Batch 68: Loss 1.404419183731079
Model None Epoch 17 Batch 69: Loss 1.3710561990737915
Model None Epoch 17 Batch 70: Loss 1.3680002689361572
Model None Epoch 17 Batch 71: Loss 1.5138946771621704
Model None Epoch 17 Batch 72: Loss 1.3494038581848145
Model None Epoch 17 Batch 73: Loss 1.3144125938415527
Model None Epoch 17 Batch 74: Loss 1.4073102474212646
Model None Epoch 17 Batch 75: Loss 1.56959867477417
Model None Epoch 17 Batch 76: Loss 1.47442626953125
Model None Epoch 17 Batch 77: Loss 1.4394605159759521
Model None Epoch 17 Batch 78: Loss 1.4439510107040405
Model None Epoch 17 Batch 79: Loss 1.4269479513168335
Model None Epoch 17 Batch 80: Loss 1.4684932231903076
Model None Epoch 17 Batch 81: Loss 1.586530327796936
Model None Epoch 17 Batch 82: Loss 1.4545735120773315
Model None Epoch 17 Batch 83: Loss 1.4982757568359375
Model None Epoch 17 Batch 84: Loss 1.465855360031128
Model None Epoch 17 Batch 85: Loss 1.4186818599700928
Model None Epoch 17 Batch 86: Loss 1.386549711227417
Model None Epoch 17 Batch 87: Loss 1.417360544204712
Model None Epoch 17 Batch 88: Loss 1.457026481628418
Model None Epoch 17 Batch 89: Loss 1.4521021842956543
Model None Epoch 17 Batch 90: Loss 1.4748635292053223
Model None Epoch 17 Batch 91: Loss 1.4276683330535889
Model None Epoch 17 Batch 92: Loss 1.4101886749267578
Model None Epoch 17 Batch 93: Loss 1.4972240924835205
Model None Epoch 17 Batch 94: Loss 1.5474066734313965
Model None Epoch 17 Batch 95: Loss 1.4752520322799683
Model None Epoch 17 Batch 96: Loss 1.4296823740005493
Model None Epoch 17 Batch 97: Loss 1.5008023977279663
Model None Epoch 17 Batch 98: Loss 1.4430841207504272
Model None Epoch 17 Batch 99: Loss 1.448418140411377
Downstream Train Epoch: 17 [25600/50000 (51%)]	Loss: 1.389684
Model None Epoch 17 Batch 100: Loss 1.389683723449707
Model None Epoch 17 Batch 101: Loss 1.3662034273147583
Model None Epoch 17 Batch 102: Loss 1.4991998672485352
Model None Epoch 17 Batch 103: Loss 1.4396471977233887
Model None Epoch 17 Batch 104: Loss 1.5477540493011475
Model None Epoch 17 Batch 105: Loss 1.3843202590942383
Model None Epoch 17 Batch 106: Loss 1.4171833992004395
Model None Epoch 17 Batch 107: Loss 1.4730454683303833
Model None Epoch 17 Batch 108: Loss 1.2813788652420044
Model None Epoch 17 Batch 109: Loss 1.434706211090088
Model None Epoch 17 Batch 110: Loss 1.3776366710662842
Model None Epoch 17 Batch 111: Loss 1.513149380683899
Model None Epoch 17 Batch 112: Loss 1.5066300630569458
Model None Epoch 17 Batch 113: Loss 1.4318735599517822
Model None Epoch 17 Batch 114: Loss 1.377602219581604
Model None Epoch 17 Batch 115: Loss 1.435889482498169
Model None Epoch 17 Batch 116: Loss 1.286521077156067
Model None Epoch 17 Batch 117: Loss 1.5156400203704834
Model None Epoch 17 Batch 118: Loss 1.3477996587753296
Model None Epoch 17 Batch 119: Loss 1.3933417797088623
Model None Epoch 17 Batch 120: Loss 1.4187171459197998
Model None Epoch 17 Batch 121: Loss 1.3469430208206177
Model None Epoch 17 Batch 122: Loss 1.4325493574142456
Model None Epoch 17 Batch 123: Loss 1.4284603595733643
Model None Epoch 17 Batch 124: Loss 1.5012158155441284
Model None Epoch 17 Batch 125: Loss 1.4552723169326782
Model None Epoch 17 Batch 126: Loss 1.413428783416748
Model None Epoch 17 Batch 127: Loss 1.442156434059143
Model None Epoch 17 Batch 128: Loss 1.4505144357681274
Model None Epoch 17 Batch 129: Loss 1.592122197151184
Model None Epoch 17 Batch 130: Loss 1.5472553968429565
Model None Epoch 17 Batch 131: Loss 1.3698992729187012
Model None Epoch 17 Batch 132: Loss 1.48026704788208
Model None Epoch 17 Batch 133: Loss 1.4429969787597656
Model None Epoch 17 Batch 134: Loss 1.4468402862548828
Model None Epoch 17 Batch 135: Loss 1.4801051616668701
Model None Epoch 17 Batch 136: Loss 1.354006290435791
Model None Epoch 17 Batch 137: Loss 1.4375723600387573
Model None Epoch 17 Batch 138: Loss 1.4045844078063965
Model None Epoch 17 Batch 139: Loss 1.502656102180481
Model None Epoch 17 Batch 140: Loss 1.3715510368347168
Model None Epoch 17 Batch 141: Loss 1.3712066411972046
Model None Epoch 17 Batch 142: Loss 1.4562020301818848
Model None Epoch 17 Batch 143: Loss 1.3997257947921753
Model None Epoch 17 Batch 144: Loss 1.346813440322876
Model None Epoch 17 Batch 145: Loss 1.473885178565979
Model None Epoch 17 Batch 146: Loss 1.4327583312988281
Model None Epoch 17 Batch 147: Loss 1.4556132555007935
Model None Epoch 17 Batch 148: Loss 1.4537228345870972
Model None Epoch 17 Batch 149: Loss 1.4226620197296143
Downstream Train Epoch: 17 [38400/50000 (77%)]	Loss: 1.456098
Model None Epoch 17 Batch 150: Loss 1.456097960472107
Model None Epoch 17 Batch 151: Loss 1.472153902053833
Model None Epoch 17 Batch 152: Loss 1.3764357566833496
Model None Epoch 17 Batch 153: Loss 1.4071300029754639
Model None Epoch 17 Batch 154: Loss 1.4418511390686035
Model None Epoch 17 Batch 155: Loss 1.5283339023590088
Model None Epoch 17 Batch 156: Loss 1.5293591022491455
Model None Epoch 17 Batch 157: Loss 1.4632948637008667
Model None Epoch 17 Batch 158: Loss 1.4856544733047485
Model None Epoch 17 Batch 159: Loss 1.3927768468856812
Model None Epoch 17 Batch 160: Loss 1.3892431259155273
Model None Epoch 17 Batch 161: Loss 1.4221768379211426
Model None Epoch 17 Batch 162: Loss 1.4145286083221436
Model None Epoch 17 Batch 163: Loss 1.389201283454895
Model None Epoch 17 Batch 164: Loss 1.476528286933899
Model None Epoch 17 Batch 165: Loss 1.4721039533615112
Model None Epoch 17 Batch 166: Loss 1.4090862274169922
Model None Epoch 17 Batch 167: Loss 1.4696944952011108
Model None Epoch 17 Batch 168: Loss 1.405429482460022
Model None Epoch 17 Batch 169: Loss 1.4439970254898071
Model None Epoch 17 Batch 170: Loss 1.3638334274291992
Model None Epoch 17 Batch 171: Loss 1.5320390462875366
Model None Epoch 17 Batch 172: Loss 1.431235671043396
Model None Epoch 17 Batch 173: Loss 1.4952096939086914
Model None Epoch 17 Batch 174: Loss 1.4963234663009644
Model None Epoch 17 Batch 175: Loss 1.4298720359802246
Model None Epoch 17 Batch 176: Loss 1.444999098777771
Model None Epoch 17 Batch 177: Loss 1.3296784162521362
Model None Epoch 17 Batch 178: Loss 1.3287650346755981
Model None Epoch 17 Batch 179: Loss 1.42255699634552
Model None Epoch 17 Batch 180: Loss 1.4671745300292969
Model None Epoch 17 Batch 181: Loss 1.3823388814926147
Model None Epoch 17 Batch 182: Loss 1.3876975774765015
Model None Epoch 17 Batch 183: Loss 1.3772860765457153
Model None Epoch 17 Batch 184: Loss 1.376193881034851
Model None Epoch 17 Batch 185: Loss 1.5178546905517578
Model None Epoch 17 Batch 186: Loss 1.4559745788574219
Model None Epoch 17 Batch 187: Loss 1.505735993385315
Model None Epoch 17 Batch 188: Loss 1.359803318977356
Model None Epoch 17 Batch 189: Loss 1.4667623043060303
Model None Epoch 17 Batch 190: Loss 1.4173903465270996
Model None Epoch 17 Batch 191: Loss 1.3753302097320557
Model None Epoch 17 Batch 192: Loss 1.4702489376068115
Model None Epoch 17 Batch 193: Loss 1.4185497760772705
Model None Epoch 17 Batch 194: Loss 1.3993628025054932
Model None Epoch 17 Batch 195: Loss 1.477428913116455

 Downstream Train loss: 1.4336225323531093 Acc: 0.5431
Downstream Train Epoch: 18 [0/50000 (0%)]	Loss: 1.403841
Model None Epoch 18 Batch 0: Loss 1.4038407802581787
Model None Epoch 18 Batch 1: Loss 1.4575388431549072
Model None Epoch 18 Batch 2: Loss 1.366739273071289
Model None Epoch 18 Batch 3: Loss 1.4385970830917358
Model None Epoch 18 Batch 4: Loss 1.4304208755493164
Model None Epoch 18 Batch 5: Loss 1.3564378023147583
Model None Epoch 18 Batch 6: Loss 1.5024752616882324
Model None Epoch 18 Batch 7: Loss 1.4468811750411987
Model None Epoch 18 Batch 8: Loss 1.4400511980056763
Model None Epoch 18 Batch 9: Loss 1.4305741786956787
Model None Epoch 18 Batch 10: Loss 1.4671955108642578
Model None Epoch 18 Batch 11: Loss 1.4760788679122925
Model None Epoch 18 Batch 12: Loss 1.4299159049987793
Model None Epoch 18 Batch 13: Loss 1.413618803024292
Model None Epoch 18 Batch 14: Loss 1.4803528785705566
Model None Epoch 18 Batch 15: Loss 1.4703032970428467
Model None Epoch 18 Batch 16: Loss 1.537811279296875
Model None Epoch 18 Batch 17: Loss 1.387315034866333
Model None Epoch 18 Batch 18: Loss 1.5032716989517212
Model None Epoch 18 Batch 19: Loss 1.49946928024292
Model None Epoch 18 Batch 20: Loss 1.322204351425171
Model None Epoch 18 Batch 21: Loss 1.4695338010787964
Model None Epoch 18 Batch 22: Loss 1.3991116285324097
Model None Epoch 18 Batch 23: Loss 1.312582015991211
Model None Epoch 18 Batch 24: Loss 1.3614552021026611
Model None Epoch 18 Batch 25: Loss 1.3351597785949707
Model None Epoch 18 Batch 26: Loss 1.4530584812164307
Model None Epoch 18 Batch 27: Loss 1.3453326225280762
Model None Epoch 18 Batch 28: Loss 1.4820364713668823
Model None Epoch 18 Batch 29: Loss 1.4589965343475342
Model None Epoch 18 Batch 30: Loss 1.3910731077194214
Model None Epoch 18 Batch 31: Loss 1.4576292037963867
Model None Epoch 18 Batch 32: Loss 1.4480259418487549
Model None Epoch 18 Batch 33: Loss 1.4794179201126099
Model None Epoch 18 Batch 34: Loss 1.442130208015442
Model None Epoch 18 Batch 35: Loss 1.387978434562683
Model None Epoch 18 Batch 36: Loss 1.496189832687378
Model None Epoch 18 Batch 37: Loss 1.501659870147705
Model None Epoch 18 Batch 38: Loss 1.5089597702026367
Model None Epoch 18 Batch 39: Loss 1.489302635192871
Model None Epoch 18 Batch 40: Loss 1.4580665826797485
Model None Epoch 18 Batch 41: Loss 1.4700102806091309
Model None Epoch 18 Batch 42: Loss 1.5025163888931274
Model None Epoch 18 Batch 43: Loss 1.4132107496261597
Model None Epoch 18 Batch 44: Loss 1.383094310760498
Model None Epoch 18 Batch 45: Loss 1.372127890586853
Model None Epoch 18 Batch 46: Loss 1.4664075374603271
Model None Epoch 18 Batch 47: Loss 1.4598804712295532
Model None Epoch 18 Batch 48: Loss 1.521679162979126
Model None Epoch 18 Batch 49: Loss 1.5003960132598877
Downstream Train Epoch: 18 [12800/50000 (26%)]	Loss: 1.348078
Model None Epoch 18 Batch 50: Loss 1.3480782508850098
Model None Epoch 18 Batch 51: Loss 1.3814977407455444
Model None Epoch 18 Batch 52: Loss 1.5250002145767212
Model None Epoch 18 Batch 53: Loss 1.3909803628921509
Model None Epoch 18 Batch 54: Loss 1.383901834487915
Model None Epoch 18 Batch 55: Loss 1.4378749132156372
Model None Epoch 18 Batch 56: Loss 1.445706844329834
Model None Epoch 18 Batch 57: Loss 1.502145767211914
Model None Epoch 18 Batch 58: Loss 1.3305532932281494
Model None Epoch 18 Batch 59: Loss 1.4731730222702026
Model None Epoch 18 Batch 60: Loss 1.3363368511199951
Model None Epoch 18 Batch 61: Loss 1.4267979860305786
Model None Epoch 18 Batch 62: Loss 1.45735764503479
Model None Epoch 18 Batch 63: Loss 1.3473145961761475
Model None Epoch 18 Batch 64: Loss 1.395924687385559
Model None Epoch 18 Batch 65: Loss 1.495827555656433
Model None Epoch 18 Batch 66: Loss 1.4589793682098389
Model None Epoch 18 Batch 67: Loss 1.4420889616012573
Model None Epoch 18 Batch 68: Loss 1.3953125476837158
Model None Epoch 18 Batch 69: Loss 1.375587821006775
Model None Epoch 18 Batch 70: Loss 1.4344217777252197
Model None Epoch 18 Batch 71: Loss 1.5597469806671143
Model None Epoch 18 Batch 72: Loss 1.4124374389648438
Model None Epoch 18 Batch 73: Loss 1.5397913455963135
Model None Epoch 18 Batch 74: Loss 1.413404941558838
Model None Epoch 18 Batch 75: Loss 1.3750017881393433
Model None Epoch 18 Batch 76: Loss 1.5354970693588257
Model None Epoch 18 Batch 77: Loss 1.4120057821273804
Model None Epoch 18 Batch 78: Loss 1.417541742324829
Model None Epoch 18 Batch 79: Loss 1.3677161931991577
Model None Epoch 18 Batch 80: Loss 1.458147644996643
Model None Epoch 18 Batch 81: Loss 1.418474555015564
Model None Epoch 18 Batch 82: Loss 1.3833894729614258
Model None Epoch 18 Batch 83: Loss 1.5104639530181885
Model None Epoch 18 Batch 84: Loss 1.3708641529083252
Model None Epoch 18 Batch 85: Loss 1.4684020280838013
Model None Epoch 18 Batch 86: Loss 1.311126470565796
Model None Epoch 18 Batch 87: Loss 1.3593275547027588
Model None Epoch 18 Batch 88: Loss 1.5150907039642334
Model None Epoch 18 Batch 89: Loss 1.5063763856887817
Model None Epoch 18 Batch 90: Loss 1.2838077545166016
Model None Epoch 18 Batch 91: Loss 1.395395278930664
Model None Epoch 18 Batch 92: Loss 1.4516117572784424
Model None Epoch 18 Batch 93: Loss 1.5520787239074707
Model None Epoch 18 Batch 94: Loss 1.4537826776504517
Model None Epoch 18 Batch 95: Loss 1.2918775081634521
Model None Epoch 18 Batch 96: Loss 1.5040080547332764
Model None Epoch 18 Batch 97: Loss 1.4374104738235474
Model None Epoch 18 Batch 98: Loss 1.3588109016418457
Model None Epoch 18 Batch 99: Loss 1.4000346660614014
Downstream Train Epoch: 18 [25600/50000 (51%)]	Loss: 1.428004
Model None Epoch 18 Batch 100: Loss 1.4280043840408325
Model None Epoch 18 Batch 101: Loss 1.509055256843567
Model None Epoch 18 Batch 102: Loss 1.3453158140182495
Model None Epoch 18 Batch 103: Loss 1.3315809965133667
Model None Epoch 18 Batch 104: Loss 1.494261622428894
Model None Epoch 18 Batch 105: Loss 1.3747010231018066
Model None Epoch 18 Batch 106: Loss 1.4884957075119019
Model None Epoch 18 Batch 107: Loss 1.3369120359420776
Model None Epoch 18 Batch 108: Loss 1.3827917575836182
Model None Epoch 18 Batch 109: Loss 1.4115540981292725
Model None Epoch 18 Batch 110: Loss 1.3957029581069946
Model None Epoch 18 Batch 111: Loss 1.4989210367202759
Model None Epoch 18 Batch 112: Loss 1.3842418193817139
Model None Epoch 18 Batch 113: Loss 1.3110682964324951
Model None Epoch 18 Batch 114: Loss 1.389822244644165
Model None Epoch 18 Batch 115: Loss 1.5012807846069336
Model None Epoch 18 Batch 116: Loss 1.52704918384552
Model None Epoch 18 Batch 117: Loss 1.4449363946914673
Model None Epoch 18 Batch 118: Loss 1.5372198820114136
Model None Epoch 18 Batch 119: Loss 1.462278962135315
Model None Epoch 18 Batch 120: Loss 1.4629580974578857
Model None Epoch 18 Batch 121: Loss 1.4013311862945557
Model None Epoch 18 Batch 122: Loss 1.4194258451461792
Model None Epoch 18 Batch 123: Loss 1.4861629009246826
Model None Epoch 18 Batch 124: Loss 1.5506839752197266
Model None Epoch 18 Batch 125: Loss 1.4429022073745728
Model None Epoch 18 Batch 126: Loss 1.5288920402526855
Model None Epoch 18 Batch 127: Loss 1.3990001678466797
Model None Epoch 18 Batch 128: Loss 1.4683362245559692
Model None Epoch 18 Batch 129: Loss 1.4278405904769897
Model None Epoch 18 Batch 130: Loss 1.3940343856811523
Model None Epoch 18 Batch 131: Loss 1.4499722719192505
Model None Epoch 18 Batch 132: Loss 1.381316065788269
Model None Epoch 18 Batch 133: Loss 1.3522861003875732
Model None Epoch 18 Batch 134: Loss 1.430053949356079
Model None Epoch 18 Batch 135: Loss 1.4866359233856201
Model None Epoch 18 Batch 136: Loss 1.426945686340332
Model None Epoch 18 Batch 137: Loss 1.3274548053741455
Model None Epoch 18 Batch 138: Loss 1.4007408618927002
Model None Epoch 18 Batch 139: Loss 1.5586938858032227
Model None Epoch 18 Batch 140: Loss 1.3997427225112915
Model None Epoch 18 Batch 141: Loss 1.57867431640625
Model None Epoch 18 Batch 142: Loss 1.4212017059326172
Model None Epoch 18 Batch 143: Loss 1.587446689605713
Model None Epoch 18 Batch 144: Loss 1.4821662902832031
Model None Epoch 18 Batch 145: Loss 1.4455493688583374
Model None Epoch 18 Batch 146: Loss 1.4552571773529053
Model None Epoch 18 Batch 147: Loss 1.4163683652877808
Model None Epoch 18 Batch 148: Loss 1.4706069231033325
Model None Epoch 18 Batch 149: Loss 1.3347829580307007
Downstream Train Epoch: 18 [38400/50000 (77%)]	Loss: 1.395385
Model None Epoch 18 Batch 150: Loss 1.3953851461410522
Model None Epoch 18 Batch 151: Loss 1.3332760334014893
Model None Epoch 18 Batch 152: Loss 1.4615017175674438
Model None Epoch 18 Batch 153: Loss 1.461713194847107
Model None Epoch 18 Batch 154: Loss 1.5122835636138916
Model None Epoch 18 Batch 155: Loss 1.418164849281311
Model None Epoch 18 Batch 156: Loss 1.4800187349319458
Model None Epoch 18 Batch 157: Loss 1.4961413145065308
Model None Epoch 18 Batch 158: Loss 1.5075763463974
Model None Epoch 18 Batch 159: Loss 1.4702273607254028
Model None Epoch 18 Batch 160: Loss 1.5198509693145752
Model None Epoch 18 Batch 161: Loss 1.3329986333847046
Model None Epoch 18 Batch 162: Loss 1.3897773027420044
Model None Epoch 18 Batch 163: Loss 1.5463536977767944
Model None Epoch 18 Batch 164: Loss 1.4831714630126953
Model None Epoch 18 Batch 165: Loss 1.4095797538757324
Model None Epoch 18 Batch 166: Loss 1.4315125942230225
Model None Epoch 18 Batch 167: Loss 1.4415868520736694
Model None Epoch 18 Batch 168: Loss 1.4316774606704712
Model None Epoch 18 Batch 169: Loss 1.444810152053833
Model None Epoch 18 Batch 170: Loss 1.4782726764678955
Model None Epoch 18 Batch 171: Loss 1.4470752477645874
Model None Epoch 18 Batch 172: Loss 1.452212929725647
Model None Epoch 18 Batch 173: Loss 1.4354090690612793
Model None Epoch 18 Batch 174: Loss 1.402031660079956
Model None Epoch 18 Batch 175: Loss 1.3815938234329224
Model None Epoch 18 Batch 176: Loss 1.4031529426574707
Model None Epoch 18 Batch 177: Loss 1.36909019947052
Model None Epoch 18 Batch 178: Loss 1.490506649017334
Model None Epoch 18 Batch 179: Loss 1.4034972190856934
Model None Epoch 18 Batch 180: Loss 1.460193157196045
Model None Epoch 18 Batch 181: Loss 1.4669657945632935
Model None Epoch 18 Batch 182: Loss 1.4851678609848022
Model None Epoch 18 Batch 183: Loss 1.3374665975570679
Model None Epoch 18 Batch 184: Loss 1.3164749145507812
Model None Epoch 18 Batch 185: Loss 1.4230772256851196
Model None Epoch 18 Batch 186: Loss 1.5078767538070679
Model None Epoch 18 Batch 187: Loss 1.325758934020996
Model None Epoch 18 Batch 188: Loss 1.4109060764312744
Model None Epoch 18 Batch 189: Loss 1.411096453666687
Model None Epoch 18 Batch 190: Loss 1.4096674919128418
Model None Epoch 18 Batch 191: Loss 1.3648872375488281
Model None Epoch 18 Batch 192: Loss 1.433959722518921
Model None Epoch 18 Batch 193: Loss 1.537146806716919
Model None Epoch 18 Batch 194: Loss 1.3005805015563965
Model None Epoch 18 Batch 195: Loss 1.450622320175171

 Downstream Train loss: 1.4341406001120198 Acc: 0.5431
Downstream Train Epoch: 19 [0/50000 (0%)]	Loss: 1.438188
Model None Epoch 19 Batch 0: Loss 1.4381883144378662
Model None Epoch 19 Batch 1: Loss 1.3811700344085693
Model None Epoch 19 Batch 2: Loss 1.4951292276382446
Model None Epoch 19 Batch 3: Loss 1.538334846496582
Model None Epoch 19 Batch 4: Loss 1.3129435777664185
Model None Epoch 19 Batch 5: Loss 1.4514970779418945
Model None Epoch 19 Batch 6: Loss 1.486673355102539
Model None Epoch 19 Batch 7: Loss 1.3385125398635864
Model None Epoch 19 Batch 8: Loss 1.3911666870117188
Model None Epoch 19 Batch 9: Loss 1.3917319774627686
Model None Epoch 19 Batch 10: Loss 1.3407158851623535
Model None Epoch 19 Batch 11: Loss 1.4974693059921265
Model None Epoch 19 Batch 12: Loss 1.4857007265090942
Model None Epoch 19 Batch 13: Loss 1.4643222093582153
Model None Epoch 19 Batch 14: Loss 1.4635796546936035
Model None Epoch 19 Batch 15: Loss 1.403065800666809
Model None Epoch 19 Batch 16: Loss 1.3599190711975098
Model None Epoch 19 Batch 17: Loss 1.486670732498169
Model None Epoch 19 Batch 18: Loss 1.4529165029525757
Model None Epoch 19 Batch 19: Loss 1.4008299112319946
Model None Epoch 19 Batch 20: Loss 1.4788126945495605
Model None Epoch 19 Batch 21: Loss 1.3443050384521484
Model None Epoch 19 Batch 22: Loss 1.4412282705307007
Model None Epoch 19 Batch 23: Loss 1.4882365465164185
Model None Epoch 19 Batch 24: Loss 1.4127967357635498
Model None Epoch 19 Batch 25: Loss 1.3797687292099
Model None Epoch 19 Batch 26: Loss 1.4263161420822144
Model None Epoch 19 Batch 27: Loss 1.3915926218032837
Model None Epoch 19 Batch 28: Loss 1.3781458139419556
Model None Epoch 19 Batch 29: Loss 1.4650717973709106
Model None Epoch 19 Batch 30: Loss 1.4672958850860596
Model None Epoch 19 Batch 31: Loss 1.4953476190567017
Model None Epoch 19 Batch 32: Loss 1.4437073469161987
Model None Epoch 19 Batch 33: Loss 1.4237147569656372
Model None Epoch 19 Batch 34: Loss 1.354974389076233
Model None Epoch 19 Batch 35: Loss 1.4952107667922974
Model None Epoch 19 Batch 36: Loss 1.418054461479187
Model None Epoch 19 Batch 37: Loss 1.4434919357299805
Model None Epoch 19 Batch 38: Loss 1.4720368385314941
Model None Epoch 19 Batch 39: Loss 1.4516314268112183
Model None Epoch 19 Batch 40: Loss 1.4947378635406494
Model None Epoch 19 Batch 41: Loss 1.4071619510650635
Model None Epoch 19 Batch 42: Loss 1.4730868339538574
Model None Epoch 19 Batch 43: Loss 1.4086841344833374
Model None Epoch 19 Batch 44: Loss 1.3756214380264282
Model None Epoch 19 Batch 45: Loss 1.3616701364517212
Model None Epoch 19 Batch 46: Loss 1.4287259578704834
Model None Epoch 19 Batch 47: Loss 1.3115181922912598
Model None Epoch 19 Batch 48: Loss 1.3327306509017944
Model None Epoch 19 Batch 49: Loss 1.5173914432525635
Downstream Train Epoch: 19 [12800/50000 (26%)]	Loss: 1.421206
Model None Epoch 19 Batch 50: Loss 1.4212055206298828
Model None Epoch 19 Batch 51: Loss 1.4437601566314697
Model None Epoch 19 Batch 52: Loss 1.4646551609039307
Model None Epoch 19 Batch 53: Loss 1.4946805238723755
Model None Epoch 19 Batch 54: Loss 1.5310163497924805
Model None Epoch 19 Batch 55: Loss 1.3140170574188232
Model None Epoch 19 Batch 56: Loss 1.4301408529281616
Model None Epoch 19 Batch 57: Loss 1.3809840679168701
Model None Epoch 19 Batch 58: Loss 1.3408420085906982
Model None Epoch 19 Batch 59: Loss 1.3948142528533936
Model None Epoch 19 Batch 60: Loss 1.4438095092773438
Model None Epoch 19 Batch 61: Loss 1.4704461097717285
Model None Epoch 19 Batch 62: Loss 1.4344487190246582
Model None Epoch 19 Batch 63: Loss 1.4578722715377808
Model None Epoch 19 Batch 64: Loss 1.4487265348434448
Model None Epoch 19 Batch 65: Loss 1.4213587045669556
Model None Epoch 19 Batch 66: Loss 1.4337553977966309
Model None Epoch 19 Batch 67: Loss 1.4545989036560059
Model None Epoch 19 Batch 68: Loss 1.4982035160064697
Model None Epoch 19 Batch 69: Loss 1.5746241807937622
Model None Epoch 19 Batch 70: Loss 1.358780026435852
Model None Epoch 19 Batch 71: Loss 1.370076060295105
Model None Epoch 19 Batch 72: Loss 1.4878631830215454
Model None Epoch 19 Batch 73: Loss 1.3434213399887085
Model None Epoch 19 Batch 74: Loss 1.5060714483261108
Model None Epoch 19 Batch 75: Loss 1.4804431200027466
Model None Epoch 19 Batch 76: Loss 1.4902844429016113
Model None Epoch 19 Batch 77: Loss 1.3842180967330933
Model None Epoch 19 Batch 78: Loss 1.382360577583313
Model None Epoch 19 Batch 79: Loss 1.4701523780822754
Model None Epoch 19 Batch 80: Loss 1.447067141532898
Model None Epoch 19 Batch 81: Loss 1.6347213983535767
Model None Epoch 19 Batch 82: Loss 1.5153871774673462
Model None Epoch 19 Batch 83: Loss 1.2955782413482666
Model None Epoch 19 Batch 84: Loss 1.5040353536605835
Model None Epoch 19 Batch 85: Loss 1.4236313104629517
Model None Epoch 19 Batch 86: Loss 1.343589425086975
Model None Epoch 19 Batch 87: Loss 1.416548728942871
Model None Epoch 19 Batch 88: Loss 1.3820927143096924
Model None Epoch 19 Batch 89: Loss 1.502711534500122
Model None Epoch 19 Batch 90: Loss 1.4285609722137451
Model None Epoch 19 Batch 91: Loss 1.3837069272994995
Model None Epoch 19 Batch 92: Loss 1.3942996263504028
Model None Epoch 19 Batch 93: Loss 1.4409829378128052
Model None Epoch 19 Batch 94: Loss 1.5077710151672363
Model None Epoch 19 Batch 95: Loss 1.3696600198745728
Model None Epoch 19 Batch 96: Loss 1.3614611625671387
Model None Epoch 19 Batch 97: Loss 1.4263951778411865
Model None Epoch 19 Batch 98: Loss 1.3729349374771118
Model None Epoch 19 Batch 99: Loss 1.4117190837860107
Downstream Train Epoch: 19 [25600/50000 (51%)]	Loss: 1.436945
Model None Epoch 19 Batch 100: Loss 1.436944842338562
Model None Epoch 19 Batch 101: Loss 1.4461963176727295
Model None Epoch 19 Batch 102: Loss 1.4083375930786133
Model None Epoch 19 Batch 103: Loss 1.3806464672088623
Model None Epoch 19 Batch 104: Loss 1.4899014234542847
Model None Epoch 19 Batch 105: Loss 1.5308305025100708
Model None Epoch 19 Batch 106: Loss 1.498570203781128
Model None Epoch 19 Batch 107: Loss 1.4412075281143188
Model None Epoch 19 Batch 108: Loss 1.4906771183013916
Model None Epoch 19 Batch 109: Loss 1.5364702939987183
Model None Epoch 19 Batch 110: Loss 1.4576712846755981
Model None Epoch 19 Batch 111: Loss 1.5045686960220337
Model None Epoch 19 Batch 112: Loss 1.5022975206375122
Model None Epoch 19 Batch 113: Loss 1.4871090650558472
Model None Epoch 19 Batch 114: Loss 1.4112131595611572
Model None Epoch 19 Batch 115: Loss 1.4106343984603882
Model None Epoch 19 Batch 116: Loss 1.4795093536376953
Model None Epoch 19 Batch 117: Loss 1.3997567892074585
Model None Epoch 19 Batch 118: Loss 1.406606912612915
Model None Epoch 19 Batch 119: Loss 1.4045629501342773
Model None Epoch 19 Batch 120: Loss 1.4117603302001953
Model None Epoch 19 Batch 121: Loss 1.427558422088623
Model None Epoch 19 Batch 122: Loss 1.4560247659683228
Model None Epoch 19 Batch 123: Loss 1.4666179418563843
Model None Epoch 19 Batch 124: Loss 1.4756417274475098
Model None Epoch 19 Batch 125: Loss 1.561715006828308
Model None Epoch 19 Batch 126: Loss 1.382885217666626
Model None Epoch 19 Batch 127: Loss 1.4183779954910278
Model None Epoch 19 Batch 128: Loss 1.412296175956726
Model None Epoch 19 Batch 129: Loss 1.3949483633041382
Model None Epoch 19 Batch 130: Loss 1.475678563117981
Model None Epoch 19 Batch 131: Loss 1.4426945447921753
Model None Epoch 19 Batch 132: Loss 1.4215103387832642
Model None Epoch 19 Batch 133: Loss 1.4124746322631836
Model None Epoch 19 Batch 134: Loss 1.4545398950576782
Model None Epoch 19 Batch 135: Loss 1.4849226474761963
Model None Epoch 19 Batch 136: Loss 1.4962682723999023
Model None Epoch 19 Batch 137: Loss 1.399835467338562
Model None Epoch 19 Batch 138: Loss 1.4954816102981567
Model None Epoch 19 Batch 139: Loss 1.483215093612671
Model None Epoch 19 Batch 140: Loss 1.4283090829849243
Model None Epoch 19 Batch 141: Loss 1.4848697185516357
Model None Epoch 19 Batch 142: Loss 1.3921260833740234
Model None Epoch 19 Batch 143: Loss 1.4039249420166016
Model None Epoch 19 Batch 144: Loss 1.4856996536254883
Model None Epoch 19 Batch 145: Loss 1.3941923379898071
Model None Epoch 19 Batch 146: Loss 1.383846640586853
Model None Epoch 19 Batch 147: Loss 1.4227350950241089
Model None Epoch 19 Batch 148: Loss 1.5443720817565918
Model None Epoch 19 Batch 149: Loss 1.4138262271881104
Downstream Train Epoch: 19 [38400/50000 (77%)]	Loss: 1.314158
Model None Epoch 19 Batch 150: Loss 1.3141580820083618
Model None Epoch 19 Batch 151: Loss 1.3716983795166016
Model None Epoch 19 Batch 152: Loss 1.4511886835098267
Model None Epoch 19 Batch 153: Loss 1.3554741144180298
Model None Epoch 19 Batch 154: Loss 1.590743064880371
Model None Epoch 19 Batch 155: Loss 1.402353048324585
Model None Epoch 19 Batch 156: Loss 1.4777077436447144
Model None Epoch 19 Batch 157: Loss 1.5368338823318481
Model None Epoch 19 Batch 158: Loss 1.307755947113037
Model None Epoch 19 Batch 159: Loss 1.4761569499969482
Model None Epoch 19 Batch 160: Loss 1.4856696128845215
Model None Epoch 19 Batch 161: Loss 1.3163347244262695
Model None Epoch 19 Batch 162: Loss 1.4419444799423218
Model None Epoch 19 Batch 163: Loss 1.384755253791809
Model None Epoch 19 Batch 164: Loss 1.4530110359191895
Model None Epoch 19 Batch 165: Loss 1.4335212707519531
Model None Epoch 19 Batch 166: Loss 1.460972785949707
Model None Epoch 19 Batch 167: Loss 1.310312271118164
Model None Epoch 19 Batch 168: Loss 1.455223560333252
Model None Epoch 19 Batch 169: Loss 1.466296911239624
Model None Epoch 19 Batch 170: Loss 1.474010944366455
Model None Epoch 19 Batch 171: Loss 1.497797966003418
Model None Epoch 19 Batch 172: Loss 1.3747341632843018
Model None Epoch 19 Batch 173: Loss 1.3157565593719482
Model None Epoch 19 Batch 174: Loss 1.399393081665039
Model None Epoch 19 Batch 175: Loss 1.46357262134552
Model None Epoch 19 Batch 176: Loss 1.4101061820983887
Model None Epoch 19 Batch 177: Loss 1.5023329257965088
Model None Epoch 19 Batch 178: Loss 1.4776109457015991
Model None Epoch 19 Batch 179: Loss 1.4935475587844849
Model None Epoch 19 Batch 180: Loss 1.5202847719192505
Model None Epoch 19 Batch 181: Loss 1.4452143907546997
Model None Epoch 19 Batch 182: Loss 1.4538499116897583
Model None Epoch 19 Batch 183: Loss 1.361133098602295
Model None Epoch 19 Batch 184: Loss 1.3499765396118164
Model None Epoch 19 Batch 185: Loss 1.4378318786621094
Model None Epoch 19 Batch 186: Loss 1.465105414390564
Model None Epoch 19 Batch 187: Loss 1.5658003091812134
Model None Epoch 19 Batch 188: Loss 1.459626317024231
Model None Epoch 19 Batch 189: Loss 1.4353306293487549
Model None Epoch 19 Batch 190: Loss 1.4929476976394653
Model None Epoch 19 Batch 191: Loss 1.4594496488571167
Model None Epoch 19 Batch 192: Loss 1.3077208995819092
Model None Epoch 19 Batch 193: Loss 1.4743238687515259
Model None Epoch 19 Batch 194: Loss 1.5877959728240967
Model None Epoch 19 Batch 195: Loss 1.5615383386611938

 Downstream Train loss: 1.4376788619829683 Acc: 0.5431
Downstream Train Epoch: 20 [0/50000 (0%)]	Loss: 1.477675
Model None Epoch 20 Batch 0: Loss 1.4776746034622192
Model None Epoch 20 Batch 1: Loss 1.3755451440811157
Model None Epoch 20 Batch 2: Loss 1.4626648426055908
Model None Epoch 20 Batch 3: Loss 1.4599661827087402
Model None Epoch 20 Batch 4: Loss 1.3668310642242432
Model None Epoch 20 Batch 5: Loss 1.410894751548767
Model None Epoch 20 Batch 6: Loss 1.480682373046875
Model None Epoch 20 Batch 7: Loss 1.4150866270065308
Model None Epoch 20 Batch 8: Loss 1.5760339498519897
Model None Epoch 20 Batch 9: Loss 1.485197901725769
Model None Epoch 20 Batch 10: Loss 1.4389328956604004
Model None Epoch 20 Batch 11: Loss 1.5382004976272583
Model None Epoch 20 Batch 12: Loss 1.298622488975525
Model None Epoch 20 Batch 13: Loss 1.2998114824295044
Model None Epoch 20 Batch 14: Loss 1.380294680595398
Model None Epoch 20 Batch 15: Loss 1.407183289527893
Model None Epoch 20 Batch 16: Loss 1.471190094947815
Model None Epoch 20 Batch 17: Loss 1.4696964025497437
Model None Epoch 20 Batch 18: Loss 1.3865528106689453
Model None Epoch 20 Batch 19: Loss 1.4130077362060547
Model None Epoch 20 Batch 20: Loss 1.3524386882781982
Model None Epoch 20 Batch 21: Loss 1.413160800933838
Model None Epoch 20 Batch 22: Loss 1.4248979091644287
Model None Epoch 20 Batch 23: Loss 1.3980884552001953
Model None Epoch 20 Batch 24: Loss 1.3638087511062622
Model None Epoch 20 Batch 25: Loss 1.4431222677230835
Model None Epoch 20 Batch 26: Loss 1.4246370792388916
Model None Epoch 20 Batch 27: Loss 1.4675182104110718
Model None Epoch 20 Batch 28: Loss 1.3781321048736572
Model None Epoch 20 Batch 29: Loss 1.3798434734344482
Model None Epoch 20 Batch 30: Loss 1.501037836074829
Model None Epoch 20 Batch 31: Loss 1.4411355257034302
Model None Epoch 20 Batch 32: Loss 1.4430752992630005
Model None Epoch 20 Batch 33: Loss 1.505165934562683
Model None Epoch 20 Batch 34: Loss 1.457270860671997
Model None Epoch 20 Batch 35: Loss 1.4834038019180298
Model None Epoch 20 Batch 36: Loss 1.3860044479370117
Model None Epoch 20 Batch 37: Loss 1.3542420864105225
Model None Epoch 20 Batch 38: Loss 1.409805417060852
Model None Epoch 20 Batch 39: Loss 1.363602876663208
Model None Epoch 20 Batch 40: Loss 1.4803781509399414
Model None Epoch 20 Batch 41: Loss 1.309212565422058
Model None Epoch 20 Batch 42: Loss 1.4205693006515503
Model None Epoch 20 Batch 43: Loss 1.4694386720657349
Model None Epoch 20 Batch 44: Loss 1.5004923343658447
Model None Epoch 20 Batch 45: Loss 1.2960026264190674
Model None Epoch 20 Batch 46: Loss 1.3913850784301758
Model None Epoch 20 Batch 47: Loss 1.406826376914978
Model None Epoch 20 Batch 48: Loss 1.43660569190979
Model None Epoch 20 Batch 49: Loss 1.4010531902313232
Downstream Train Epoch: 20 [12800/50000 (26%)]	Loss: 1.466073
Model None Epoch 20 Batch 50: Loss 1.4660727977752686
Model None Epoch 20 Batch 51: Loss 1.4745142459869385
Model None Epoch 20 Batch 52: Loss 1.3256758451461792
Model None Epoch 20 Batch 53: Loss 1.5125271081924438
Model None Epoch 20 Batch 54: Loss 1.3684067726135254
Model None Epoch 20 Batch 55: Loss 1.4754672050476074
Model None Epoch 20 Batch 56: Loss 1.2890743017196655
Model None Epoch 20 Batch 57: Loss 1.5813908576965332
Model None Epoch 20 Batch 58: Loss 1.3816691637039185
Model None Epoch 20 Batch 59: Loss 1.4733328819274902
Model None Epoch 20 Batch 60: Loss 1.3803305625915527
Model None Epoch 20 Batch 61: Loss 1.5309293270111084
Model None Epoch 20 Batch 62: Loss 1.4023302793502808
Model None Epoch 20 Batch 63: Loss 1.4109383821487427
Model None Epoch 20 Batch 64: Loss 1.3683702945709229
Model None Epoch 20 Batch 65: Loss 1.4080249071121216
Model None Epoch 20 Batch 66: Loss 1.352885365486145
Model None Epoch 20 Batch 67: Loss 1.3788425922393799
Model None Epoch 20 Batch 68: Loss 1.5084166526794434
Model None Epoch 20 Batch 69: Loss 1.4533979892730713
Model None Epoch 20 Batch 70: Loss 1.3578041791915894
Model None Epoch 20 Batch 71: Loss 1.3731716871261597
Model None Epoch 20 Batch 72: Loss 1.4166719913482666
Model None Epoch 20 Batch 73: Loss 1.4075177907943726
Model None Epoch 20 Batch 74: Loss 1.3363451957702637
Model None Epoch 20 Batch 75: Loss 1.3756513595581055
Model None Epoch 20 Batch 76: Loss 1.5264818668365479
Model None Epoch 20 Batch 77: Loss 1.60561203956604
Model None Epoch 20 Batch 78: Loss 1.4675512313842773
Model None Epoch 20 Batch 79: Loss 1.3829314708709717
Model None Epoch 20 Batch 80: Loss 1.3647801876068115
Model None Epoch 20 Batch 81: Loss 1.355033040046692
Model None Epoch 20 Batch 82: Loss 1.3657746315002441
Model None Epoch 20 Batch 83: Loss 1.5142464637756348
Model None Epoch 20 Batch 84: Loss 1.3884037733078003
Model None Epoch 20 Batch 85: Loss 1.4873031377792358
Model None Epoch 20 Batch 86: Loss 1.5386203527450562
Model None Epoch 20 Batch 87: Loss 1.448577880859375
Model None Epoch 20 Batch 88: Loss 1.3381909132003784
Model None Epoch 20 Batch 89: Loss 1.2901729345321655
Model None Epoch 20 Batch 90: Loss 1.353982925415039
Model None Epoch 20 Batch 91: Loss 1.4037777185440063
Model None Epoch 20 Batch 92: Loss 1.4058655500411987
Model None Epoch 20 Batch 93: Loss 1.4829957485198975
Model None Epoch 20 Batch 94: Loss 1.4410303831100464
Model None Epoch 20 Batch 95: Loss 1.4127336740493774
Model None Epoch 20 Batch 96: Loss 1.3797270059585571
Model None Epoch 20 Batch 97: Loss 1.418703556060791
Model None Epoch 20 Batch 98: Loss 1.4688793420791626
Model None Epoch 20 Batch 99: Loss 1.4353474378585815
Downstream Train Epoch: 20 [25600/50000 (51%)]	Loss: 1.423947
Model None Epoch 20 Batch 100: Loss 1.4239470958709717
Model None Epoch 20 Batch 101: Loss 1.3748879432678223
Model None Epoch 20 Batch 102: Loss 1.4330755472183228
Model None Epoch 20 Batch 103: Loss 1.4112703800201416
Model None Epoch 20 Batch 104: Loss 1.4515998363494873
Model None Epoch 20 Batch 105: Loss 1.3783905506134033
Model None Epoch 20 Batch 106: Loss 1.3759933710098267
Model None Epoch 20 Batch 107: Loss 1.2659672498703003
Model None Epoch 20 Batch 108: Loss 1.3433358669281006
Model None Epoch 20 Batch 109: Loss 1.4477033615112305
Model None Epoch 20 Batch 110: Loss 1.4903919696807861
Model None Epoch 20 Batch 111: Loss 1.48325514793396
Model None Epoch 20 Batch 112: Loss 1.379913568496704
Model None Epoch 20 Batch 113: Loss 1.4555288553237915
Model None Epoch 20 Batch 114: Loss 1.441477656364441
Model None Epoch 20 Batch 115: Loss 1.5109716653823853
Model None Epoch 20 Batch 116: Loss 1.4591645002365112
Model None Epoch 20 Batch 117: Loss 1.3465019464492798
Model None Epoch 20 Batch 118: Loss 1.4080073833465576
Model None Epoch 20 Batch 119: Loss 1.4531139135360718
Model None Epoch 20 Batch 120: Loss 1.6160095930099487
Model None Epoch 20 Batch 121: Loss 1.5153546333312988
Model None Epoch 20 Batch 122: Loss 1.4823836088180542
Model None Epoch 20 Batch 123: Loss 1.4834685325622559
Model None Epoch 20 Batch 124: Loss 1.4414440393447876
Model None Epoch 20 Batch 125: Loss 1.4801249504089355
Model None Epoch 20 Batch 126: Loss 1.5069502592086792
Model None Epoch 20 Batch 127: Loss 1.4414142370224
Model None Epoch 20 Batch 128: Loss 1.512222170829773
Model None Epoch 20 Batch 129: Loss 1.4427344799041748
Model None Epoch 20 Batch 130: Loss 1.4279382228851318
Model None Epoch 20 Batch 131: Loss 1.422245740890503
Model None Epoch 20 Batch 132: Loss 1.3589906692504883
Model None Epoch 20 Batch 133: Loss 1.4247184991836548
Model None Epoch 20 Batch 134: Loss 1.415069818496704
Model None Epoch 20 Batch 135: Loss 1.4134435653686523
Model None Epoch 20 Batch 136: Loss 1.4947137832641602
Model None Epoch 20 Batch 137: Loss 1.3660197257995605
Model None Epoch 20 Batch 138: Loss 1.4044667482376099
Model None Epoch 20 Batch 139: Loss 1.413867712020874
Model None Epoch 20 Batch 140: Loss 1.4817826747894287
Model None Epoch 20 Batch 141: Loss 1.4016929864883423
Model None Epoch 20 Batch 142: Loss 1.4114168882369995
Model None Epoch 20 Batch 143: Loss 1.3762686252593994
Model None Epoch 20 Batch 144: Loss 1.4796172380447388
Model None Epoch 20 Batch 145: Loss 1.4366295337677002
Model None Epoch 20 Batch 146: Loss 1.448953628540039
Model None Epoch 20 Batch 147: Loss 1.465597152709961
Model None Epoch 20 Batch 148: Loss 1.5625064373016357
Model None Epoch 20 Batch 149: Loss 1.4376311302185059
Downstream Train Epoch: 20 [38400/50000 (77%)]	Loss: 1.361381
Model None Epoch 20 Batch 150: Loss 1.3613805770874023
Model None Epoch 20 Batch 151: Loss 1.5436127185821533
Model None Epoch 20 Batch 152: Loss 1.3992507457733154
Model None Epoch 20 Batch 153: Loss 1.3091559410095215
Model None Epoch 20 Batch 154: Loss 1.3848909139633179
Model None Epoch 20 Batch 155: Loss 1.3269001245498657
Model None Epoch 20 Batch 156: Loss 1.5090094804763794
Model None Epoch 20 Batch 157: Loss 1.4576202630996704
Model None Epoch 20 Batch 158: Loss 1.3778754472732544
Model None Epoch 20 Batch 159: Loss 1.404121994972229
Model None Epoch 20 Batch 160: Loss 1.4059957265853882
Model None Epoch 20 Batch 161: Loss 1.3704179525375366
Model None Epoch 20 Batch 162: Loss 1.5448157787322998
Model None Epoch 20 Batch 163: Loss 1.4172255992889404
Model None Epoch 20 Batch 164: Loss 1.4163119792938232
Model None Epoch 20 Batch 165: Loss 1.3119781017303467
Model None Epoch 20 Batch 166: Loss 1.4970982074737549
Model None Epoch 20 Batch 167: Loss 1.4106733798980713
Model None Epoch 20 Batch 168: Loss 1.5230107307434082
Model None Epoch 20 Batch 169: Loss 1.5038889646530151
Model None Epoch 20 Batch 170: Loss 1.4604452848434448
Model None Epoch 20 Batch 171: Loss 1.4859683513641357
Model None Epoch 20 Batch 172: Loss 1.425789713859558
Model None Epoch 20 Batch 173: Loss 1.4027247428894043
Model None Epoch 20 Batch 174: Loss 1.422980785369873
Model None Epoch 20 Batch 175: Loss 1.4340534210205078
Model None Epoch 20 Batch 176: Loss 1.417665719985962
Model None Epoch 20 Batch 177: Loss 1.4566913843154907
Model None Epoch 20 Batch 178: Loss 1.42495858669281
Model None Epoch 20 Batch 179: Loss 1.529459834098816
Model None Epoch 20 Batch 180: Loss 1.5176608562469482
Model None Epoch 20 Batch 181: Loss 1.4285303354263306
Model None Epoch 20 Batch 182: Loss 1.4209403991699219
Model None Epoch 20 Batch 183: Loss 1.5012813806533813
Model None Epoch 20 Batch 184: Loss 1.3996069431304932
Model None Epoch 20 Batch 185: Loss 1.407983422279358
Model None Epoch 20 Batch 186: Loss 1.4111335277557373
Model None Epoch 20 Batch 187: Loss 1.5400207042694092
Model None Epoch 20 Batch 188: Loss 1.6219786405563354
Model None Epoch 20 Batch 189: Loss 1.5378544330596924
Model None Epoch 20 Batch 190: Loss 1.5358359813690186
Model None Epoch 20 Batch 191: Loss 1.52150297164917
Model None Epoch 20 Batch 192: Loss 1.4735777378082275
Model None Epoch 20 Batch 193: Loss 1.3597849607467651
Model None Epoch 20 Batch 194: Loss 1.4567821025848389
Model None Epoch 20 Batch 195: Loss 1.4144377708435059

 Downstream Train loss: 1.4315712670890652 Acc: 0.5516
Downstream Train Epoch: 21 [0/50000 (0%)]	Loss: 1.440233
Model None Epoch 21 Batch 0: Loss 1.4402329921722412
Model None Epoch 21 Batch 1: Loss 1.4957149028778076
Model None Epoch 21 Batch 2: Loss 1.322072148323059
Model None Epoch 21 Batch 3: Loss 1.5198171138763428
Model None Epoch 21 Batch 4: Loss 1.3821628093719482
Model None Epoch 21 Batch 5: Loss 1.3704829216003418
Model None Epoch 21 Batch 6: Loss 1.462119460105896
Model None Epoch 21 Batch 7: Loss 1.487398624420166
Model None Epoch 21 Batch 8: Loss 1.5173248052597046
Model None Epoch 21 Batch 9: Loss 1.3547656536102295
Model None Epoch 21 Batch 10: Loss 1.3951489925384521
Model None Epoch 21 Batch 11: Loss 1.3950051069259644
Model None Epoch 21 Batch 12: Loss 1.4608832597732544
Model None Epoch 21 Batch 13: Loss 1.4287009239196777
Model None Epoch 21 Batch 14: Loss 1.4196382761001587
Model None Epoch 21 Batch 15: Loss 1.4227656126022339
Model None Epoch 21 Batch 16: Loss 1.5887510776519775
Model None Epoch 21 Batch 17: Loss 1.460263967514038
Model None Epoch 21 Batch 18: Loss 1.3847609758377075
Model None Epoch 21 Batch 19: Loss 1.462497353553772
Model None Epoch 21 Batch 20: Loss 1.4191585779190063
Model None Epoch 21 Batch 21: Loss 1.3496309518814087
Model None Epoch 21 Batch 22: Loss 1.496064305305481
Model None Epoch 21 Batch 23: Loss 1.4704902172088623
Model None Epoch 21 Batch 24: Loss 1.397480845451355
Model None Epoch 21 Batch 25: Loss 1.482279896736145
Model None Epoch 21 Batch 26: Loss 1.4180370569229126
Model None Epoch 21 Batch 27: Loss 1.453163981437683
Model None Epoch 21 Batch 28: Loss 1.3599507808685303
Model None Epoch 21 Batch 29: Loss 1.4429471492767334
Model None Epoch 21 Batch 30: Loss 1.5097686052322388
Model None Epoch 21 Batch 31: Loss 1.3724069595336914
Model None Epoch 21 Batch 32: Loss 1.490469217300415
Model None Epoch 21 Batch 33: Loss 1.5389747619628906
Model None Epoch 21 Batch 34: Loss 1.3671438694000244
Model None Epoch 21 Batch 35: Loss 1.5094352960586548
Model None Epoch 21 Batch 36: Loss 1.4316879510879517
Model None Epoch 21 Batch 37: Loss 1.4357128143310547
Model None Epoch 21 Batch 38: Loss 1.5386284589767456
Model None Epoch 21 Batch 39: Loss 1.4210838079452515
Model None Epoch 21 Batch 40: Loss 1.5256130695343018
Model None Epoch 21 Batch 41: Loss 1.4243301153182983
Model None Epoch 21 Batch 42: Loss 1.3497929573059082
Model None Epoch 21 Batch 43: Loss 1.4908833503723145
Model None Epoch 21 Batch 44: Loss 1.4752857685089111
Model None Epoch 21 Batch 45: Loss 1.3876088857650757
Model None Epoch 21 Batch 46: Loss 1.4941855669021606
Model None Epoch 21 Batch 47: Loss 1.4430553913116455
Model None Epoch 21 Batch 48: Loss 1.4371745586395264
Model None Epoch 21 Batch 49: Loss 1.429305076599121
Downstream Train Epoch: 21 [12800/50000 (26%)]	Loss: 1.342829
Model None Epoch 21 Batch 50: Loss 1.342828631401062
Model None Epoch 21 Batch 51: Loss 1.4613240957260132
Model None Epoch 21 Batch 52: Loss 1.4075719118118286
Model None Epoch 21 Batch 53: Loss 1.2311458587646484
Model None Epoch 21 Batch 54: Loss 1.5321431159973145
Model None Epoch 21 Batch 55: Loss 1.378366470336914
Model None Epoch 21 Batch 56: Loss 1.3876599073410034
Model None Epoch 21 Batch 57: Loss 1.4369494915008545
Model None Epoch 21 Batch 58: Loss 1.3595367670059204
Model None Epoch 21 Batch 59: Loss 1.391101360321045
Model None Epoch 21 Batch 60: Loss 1.4560835361480713
Model None Epoch 21 Batch 61: Loss 1.3409316539764404
Model None Epoch 21 Batch 62: Loss 1.5300322771072388
Model None Epoch 21 Batch 63: Loss 1.3442928791046143
Model None Epoch 21 Batch 64: Loss 1.4543620347976685
Model None Epoch 21 Batch 65: Loss 1.4344063997268677
Model None Epoch 21 Batch 66: Loss 1.5030450820922852
Model None Epoch 21 Batch 67: Loss 1.4270727634429932
Model None Epoch 21 Batch 68: Loss 1.4131261110305786
Model None Epoch 21 Batch 69: Loss 1.5234034061431885
Model None Epoch 21 Batch 70: Loss 1.4182199239730835
Model None Epoch 21 Batch 71: Loss 1.430160403251648
Model None Epoch 21 Batch 72: Loss 1.2136290073394775
Model None Epoch 21 Batch 73: Loss 1.5115594863891602
Model None Epoch 21 Batch 74: Loss 1.4253218173980713
Model None Epoch 21 Batch 75: Loss 1.3898131847381592
Model None Epoch 21 Batch 76: Loss 1.4942235946655273
Model None Epoch 21 Batch 77: Loss 1.4090220928192139
Model None Epoch 21 Batch 78: Loss 1.4008979797363281
Model None Epoch 21 Batch 79: Loss 1.3974463939666748
Model None Epoch 21 Batch 80: Loss 1.430916428565979
Model None Epoch 21 Batch 81: Loss 1.3344894647598267
Model None Epoch 21 Batch 82: Loss 1.4831569194793701
Model None Epoch 21 Batch 83: Loss 1.3910613059997559
Model None Epoch 21 Batch 84: Loss 1.5465868711471558
Model None Epoch 21 Batch 85: Loss 1.4006208181381226
Model None Epoch 21 Batch 86: Loss 1.4837161302566528
Model None Epoch 21 Batch 87: Loss 1.411271095275879
Model None Epoch 21 Batch 88: Loss 1.426472544670105
Model None Epoch 21 Batch 89: Loss 1.3730359077453613
Model None Epoch 21 Batch 90: Loss 1.3600023984909058
Model None Epoch 21 Batch 91: Loss 1.500187873840332
Model None Epoch 21 Batch 92: Loss 1.4727932214736938
Model None Epoch 21 Batch 93: Loss 1.4204511642456055
Model None Epoch 21 Batch 94: Loss 1.3949087858200073
Model None Epoch 21 Batch 95: Loss 1.4474961757659912
Model None Epoch 21 Batch 96: Loss 1.3584449291229248
Model None Epoch 21 Batch 97: Loss 1.40143883228302
Model None Epoch 21 Batch 98: Loss 1.4338854551315308
Model None Epoch 21 Batch 99: Loss 1.4550166130065918
Downstream Train Epoch: 21 [25600/50000 (51%)]	Loss: 1.482616
Model None Epoch 21 Batch 100: Loss 1.4826161861419678
Model None Epoch 21 Batch 101: Loss 1.4186921119689941
Model None Epoch 21 Batch 102: Loss 1.4397602081298828
Model None Epoch 21 Batch 103: Loss 1.4099743366241455
Model None Epoch 21 Batch 104: Loss 1.4250257015228271
Model None Epoch 21 Batch 105: Loss 1.5242440700531006
Model None Epoch 21 Batch 106: Loss 1.458364486694336
Model None Epoch 21 Batch 107: Loss 1.5919636487960815
Model None Epoch 21 Batch 108: Loss 1.4157323837280273
Model None Epoch 21 Batch 109: Loss 1.3412771224975586
Model None Epoch 21 Batch 110: Loss 1.4312477111816406
Model None Epoch 21 Batch 111: Loss 1.5015672445297241
Model None Epoch 21 Batch 112: Loss 1.4116350412368774
Model None Epoch 21 Batch 113: Loss 1.4370741844177246
Model None Epoch 21 Batch 114: Loss 1.3771573305130005
Model None Epoch 21 Batch 115: Loss 1.44777512550354
Model None Epoch 21 Batch 116: Loss 1.388813853263855
Model None Epoch 21 Batch 117: Loss 1.385905146598816
Model None Epoch 21 Batch 118: Loss 1.442765474319458
Model None Epoch 21 Batch 119: Loss 1.3718500137329102
Model None Epoch 21 Batch 120: Loss 1.4015445709228516
Model None Epoch 21 Batch 121: Loss 1.6191692352294922
Model None Epoch 21 Batch 122: Loss 1.4244166612625122
Model None Epoch 21 Batch 123: Loss 1.4413527250289917
Model None Epoch 21 Batch 124: Loss 1.4302407503128052
Model None Epoch 21 Batch 125: Loss 1.571658968925476
Model None Epoch 21 Batch 126: Loss 1.4928476810455322
Model None Epoch 21 Batch 127: Loss 1.4754705429077148
Model None Epoch 21 Batch 128: Loss 1.3798506259918213
Model None Epoch 21 Batch 129: Loss 1.4650861024856567
Model None Epoch 21 Batch 130: Loss 1.4475781917572021
Model None Epoch 21 Batch 131: Loss 1.3964924812316895
Model None Epoch 21 Batch 132: Loss 1.3803839683532715
Model None Epoch 21 Batch 133: Loss 1.3734058141708374
Model None Epoch 21 Batch 134: Loss 1.5370784997940063
Model None Epoch 21 Batch 135: Loss 1.422558069229126
Model None Epoch 21 Batch 136: Loss 1.3982195854187012
Model None Epoch 21 Batch 137: Loss 1.51285719871521
Model None Epoch 21 Batch 138: Loss 1.5126162767410278
Model None Epoch 21 Batch 139: Loss 1.402026653289795
Model None Epoch 21 Batch 140: Loss 1.334069013595581
Model None Epoch 21 Batch 141: Loss 1.4676461219787598
Model None Epoch 21 Batch 142: Loss 1.462339997291565
Model None Epoch 21 Batch 143: Loss 1.4971753358840942
Model None Epoch 21 Batch 144: Loss 1.3776936531066895
Model None Epoch 21 Batch 145: Loss 1.4895871877670288
Model None Epoch 21 Batch 146: Loss 1.4248039722442627
Model None Epoch 21 Batch 147: Loss 1.3186110258102417
Model None Epoch 21 Batch 148: Loss 1.4972236156463623
Model None Epoch 21 Batch 149: Loss 1.4706374406814575
Downstream Train Epoch: 21 [38400/50000 (77%)]	Loss: 1.373232
Model None Epoch 21 Batch 150: Loss 1.3732317686080933
Model None Epoch 21 Batch 151: Loss 1.4681512117385864
Model None Epoch 21 Batch 152: Loss 1.44733726978302
Model None Epoch 21 Batch 153: Loss 1.3185169696807861
Model None Epoch 21 Batch 154: Loss 1.3743656873703003
Model None Epoch 21 Batch 155: Loss 1.3444205522537231
Model None Epoch 21 Batch 156: Loss 1.4860602617263794
Model None Epoch 21 Batch 157: Loss 1.530425786972046
Model None Epoch 21 Batch 158: Loss 1.489431381225586
Model None Epoch 21 Batch 159: Loss 1.4633225202560425
Model None Epoch 21 Batch 160: Loss 1.4159694910049438
Model None Epoch 21 Batch 161: Loss 1.3928534984588623
Model None Epoch 21 Batch 162: Loss 1.4928570985794067
Model None Epoch 21 Batch 163: Loss 1.3448562622070312
Model None Epoch 21 Batch 164: Loss 1.4834644794464111
Model None Epoch 21 Batch 165: Loss 1.4058068990707397
Model None Epoch 21 Batch 166: Loss 1.3193732500076294
Model None Epoch 21 Batch 167: Loss 1.536376953125
Model None Epoch 21 Batch 168: Loss 1.4490532875061035
Model None Epoch 21 Batch 169: Loss 1.436000943183899
Model None Epoch 21 Batch 170: Loss 1.4561251401901245
Model None Epoch 21 Batch 171: Loss 1.2874526977539062
Model None Epoch 21 Batch 172: Loss 1.4093621969223022
Model None Epoch 21 Batch 173: Loss 1.385801911354065
Model None Epoch 21 Batch 174: Loss 1.4681183099746704
Model None Epoch 21 Batch 175: Loss 1.4408619403839111
Model None Epoch 21 Batch 176: Loss 1.391326665878296
Model None Epoch 21 Batch 177: Loss 1.3308361768722534
Model None Epoch 21 Batch 178: Loss 1.4262369871139526
Model None Epoch 21 Batch 179: Loss 1.404056191444397
Model None Epoch 21 Batch 180: Loss 1.3579760789871216
Model None Epoch 21 Batch 181: Loss 1.4335033893585205
Model None Epoch 21 Batch 182: Loss 1.4506680965423584
Model None Epoch 21 Batch 183: Loss 1.4792510271072388
Model None Epoch 21 Batch 184: Loss 1.4222105741500854
Model None Epoch 21 Batch 185: Loss 1.3962129354476929
Model None Epoch 21 Batch 186: Loss 1.4118518829345703
Model None Epoch 21 Batch 187: Loss 1.5769505500793457
Model None Epoch 21 Batch 188: Loss 1.5303478240966797
Model None Epoch 21 Batch 189: Loss 1.5025027990341187
Model None Epoch 21 Batch 190: Loss 1.4390157461166382
Model None Epoch 21 Batch 191: Loss 1.3894646167755127
Model None Epoch 21 Batch 192: Loss 1.444765567779541
Model None Epoch 21 Batch 193: Loss 1.4388631582260132
Model None Epoch 21 Batch 194: Loss 1.5028462409973145
Model None Epoch 21 Batch 195: Loss 1.4331684112548828

 Downstream Train loss: 1.4337429787431444 Acc: 0.5516
Downstream Train Epoch: 22 [0/50000 (0%)]	Loss: 1.485434
Model None Epoch 22 Batch 0: Loss 1.48543381690979
Model None Epoch 22 Batch 1: Loss 1.388403296470642
Model None Epoch 22 Batch 2: Loss 1.4738900661468506
Model None Epoch 22 Batch 3: Loss 1.5390676259994507
Model None Epoch 22 Batch 4: Loss 1.5180656909942627
Model None Epoch 22 Batch 5: Loss 1.4954097270965576
Model None Epoch 22 Batch 6: Loss 1.392329454421997
Model None Epoch 22 Batch 7: Loss 1.308752179145813
Model None Epoch 22 Batch 8: Loss 1.5054887533187866
Model None Epoch 22 Batch 9: Loss 1.4642102718353271
Model None Epoch 22 Batch 10: Loss 1.4488754272460938
Model None Epoch 22 Batch 11: Loss 1.4545127153396606
Model None Epoch 22 Batch 12: Loss 1.343147873878479
Model None Epoch 22 Batch 13: Loss 1.440971851348877
Model None Epoch 22 Batch 14: Loss 1.40033757686615
Model None Epoch 22 Batch 15: Loss 1.4160336256027222
Model None Epoch 22 Batch 16: Loss 1.4054725170135498
Model None Epoch 22 Batch 17: Loss 1.4116239547729492
Model None Epoch 22 Batch 18: Loss 1.4382749795913696
Model None Epoch 22 Batch 19: Loss 1.4861116409301758
Model None Epoch 22 Batch 20: Loss 1.3416578769683838
Model None Epoch 22 Batch 21: Loss 1.4348520040512085
Model None Epoch 22 Batch 22: Loss 1.467620611190796
Model None Epoch 22 Batch 23: Loss 1.371192216873169
Model None Epoch 22 Batch 24: Loss 1.3788893222808838
Model None Epoch 22 Batch 25: Loss 1.352620005607605
Model None Epoch 22 Batch 26: Loss 1.4571471214294434
Model None Epoch 22 Batch 27: Loss 1.470717191696167
Model None Epoch 22 Batch 28: Loss 1.3800511360168457
Model None Epoch 22 Batch 29: Loss 1.4261044263839722
Model None Epoch 22 Batch 30: Loss 1.5097888708114624
Model None Epoch 22 Batch 31: Loss 1.5366458892822266
Model None Epoch 22 Batch 32: Loss 1.5712575912475586
Model None Epoch 22 Batch 33: Loss 1.3919965028762817
Model None Epoch 22 Batch 34: Loss 1.3378949165344238
Model None Epoch 22 Batch 35: Loss 1.4914530515670776
Model None Epoch 22 Batch 36: Loss 1.3555645942687988
Model None Epoch 22 Batch 37: Loss 1.4664583206176758
Model None Epoch 22 Batch 38: Loss 1.6349453926086426
Model None Epoch 22 Batch 39: Loss 1.4593186378479004
Model None Epoch 22 Batch 40: Loss 1.4873692989349365
Model None Epoch 22 Batch 41: Loss 1.503363013267517
Model None Epoch 22 Batch 42: Loss 1.327580213546753
Model None Epoch 22 Batch 43: Loss 1.4559823274612427
Model None Epoch 22 Batch 44: Loss 1.3384413719177246
Model None Epoch 22 Batch 45: Loss 1.403046727180481
Model None Epoch 22 Batch 46: Loss 1.3741323947906494
Model None Epoch 22 Batch 47: Loss 1.387694001197815
Model None Epoch 22 Batch 48: Loss 1.3225517272949219
Model None Epoch 22 Batch 49: Loss 1.339043378829956
Downstream Train Epoch: 22 [12800/50000 (26%)]	Loss: 1.408811
Model None Epoch 22 Batch 50: Loss 1.4088114500045776
Model None Epoch 22 Batch 51: Loss 1.449054479598999
Model None Epoch 22 Batch 52: Loss 1.501412034034729
Model None Epoch 22 Batch 53: Loss 1.3920525312423706
Model None Epoch 22 Batch 54: Loss 1.3829753398895264
Model None Epoch 22 Batch 55: Loss 1.3793200254440308
Model None Epoch 22 Batch 56: Loss 1.4028332233428955
Model None Epoch 22 Batch 57: Loss 1.4812015295028687
Model None Epoch 22 Batch 58: Loss 1.2924453020095825
Model None Epoch 22 Batch 59: Loss 1.4254862070083618
Model None Epoch 22 Batch 60: Loss 1.4487738609313965
Model None Epoch 22 Batch 61: Loss 1.4284896850585938
Model None Epoch 22 Batch 62: Loss 1.462682843208313
Model None Epoch 22 Batch 63: Loss 1.3374804258346558
Model None Epoch 22 Batch 64: Loss 1.4451721906661987
Model None Epoch 22 Batch 65: Loss 1.4521948099136353
Model None Epoch 22 Batch 66: Loss 1.4913359880447388
Model None Epoch 22 Batch 67: Loss 1.4338576793670654
Model None Epoch 22 Batch 68: Loss 1.3161698579788208
Model None Epoch 22 Batch 69: Loss 1.27667236328125
Model None Epoch 22 Batch 70: Loss 1.2842917442321777
Model None Epoch 22 Batch 71: Loss 1.417715072631836
Model None Epoch 22 Batch 72: Loss 1.4215795993804932
Model None Epoch 22 Batch 73: Loss 1.4150190353393555
Model None Epoch 22 Batch 74: Loss 1.4338340759277344
Model None Epoch 22 Batch 75: Loss 1.3763129711151123
Model None Epoch 22 Batch 76: Loss 1.4805363416671753
Model None Epoch 22 Batch 77: Loss 1.3762314319610596
Model None Epoch 22 Batch 78: Loss 1.5198187828063965
Model None Epoch 22 Batch 79: Loss 1.4382176399230957
Model None Epoch 22 Batch 80: Loss 1.4253363609313965
Model None Epoch 22 Batch 81: Loss 1.4508607387542725
Model None Epoch 22 Batch 82: Loss 1.399202585220337
Model None Epoch 22 Batch 83: Loss 1.4380333423614502
Model None Epoch 22 Batch 84: Loss 1.4454638957977295
Model None Epoch 22 Batch 85: Loss 1.5126430988311768
Model None Epoch 22 Batch 86: Loss 1.29877769947052
Model None Epoch 22 Batch 87: Loss 1.4468711614608765
Model None Epoch 22 Batch 88: Loss 1.5019571781158447
Model None Epoch 22 Batch 89: Loss 1.4490903615951538
Model None Epoch 22 Batch 90: Loss 1.4997833967208862
Model None Epoch 22 Batch 91: Loss 1.3099420070648193
Model None Epoch 22 Batch 92: Loss 1.4951035976409912
Model None Epoch 22 Batch 93: Loss 1.4989442825317383
Model None Epoch 22 Batch 94: Loss 1.342952847480774
Model None Epoch 22 Batch 95: Loss 1.5021545886993408
Model None Epoch 22 Batch 96: Loss 1.4527701139450073
Model None Epoch 22 Batch 97: Loss 1.394200086593628
Model None Epoch 22 Batch 98: Loss 1.447003960609436
Model None Epoch 22 Batch 99: Loss 1.440638780593872
Downstream Train Epoch: 22 [25600/50000 (51%)]	Loss: 1.452348
Model None Epoch 22 Batch 100: Loss 1.4523476362228394
Model None Epoch 22 Batch 101: Loss 1.3998081684112549
Model None Epoch 22 Batch 102: Loss 1.3814029693603516
Model None Epoch 22 Batch 103: Loss 1.451229453086853
Model None Epoch 22 Batch 104: Loss 1.6257622241973877
Model None Epoch 22 Batch 105: Loss 1.3336626291275024
Model None Epoch 22 Batch 106: Loss 1.3348065614700317
Model None Epoch 22 Batch 107: Loss 1.4367477893829346
Model None Epoch 22 Batch 108: Loss 1.3777579069137573
Model None Epoch 22 Batch 109: Loss 1.3687255382537842
Model None Epoch 22 Batch 110: Loss 1.411738395690918
Model None Epoch 22 Batch 111: Loss 1.5073574781417847
Model None Epoch 22 Batch 112: Loss 1.3992458581924438
Model None Epoch 22 Batch 113: Loss 1.4126015901565552
Model None Epoch 22 Batch 114: Loss 1.2819751501083374
Model None Epoch 22 Batch 115: Loss 1.4491461515426636
Model None Epoch 22 Batch 116: Loss 1.4972752332687378
Model None Epoch 22 Batch 117: Loss 1.4557642936706543
Model None Epoch 22 Batch 118: Loss 1.484684944152832
Model None Epoch 22 Batch 119: Loss 1.3654712438583374
Model None Epoch 22 Batch 120: Loss 1.3559837341308594
Model None Epoch 22 Batch 121: Loss 1.4155524969100952
Model None Epoch 22 Batch 122: Loss 1.5410405397415161
Model None Epoch 22 Batch 123: Loss 1.431532859802246
Model None Epoch 22 Batch 124: Loss 1.4568510055541992
Model None Epoch 22 Batch 125: Loss 1.4172120094299316
Model None Epoch 22 Batch 126: Loss 1.352286696434021
Model None Epoch 22 Batch 127: Loss 1.4661729335784912
Model None Epoch 22 Batch 128: Loss 1.385871171951294
Model None Epoch 22 Batch 129: Loss 1.458501935005188
Model None Epoch 22 Batch 130: Loss 1.3430631160736084
Model None Epoch 22 Batch 131: Loss 1.3750566244125366
Model None Epoch 22 Batch 132: Loss 1.317203402519226
Model None Epoch 22 Batch 133: Loss 1.5022212266921997
Model None Epoch 22 Batch 134: Loss 1.464198350906372
Model None Epoch 22 Batch 135: Loss 1.4086573123931885
Model None Epoch 22 Batch 136: Loss 1.461536169052124
Model None Epoch 22 Batch 137: Loss 1.3255600929260254
Model None Epoch 22 Batch 138: Loss 1.3713806867599487
Model None Epoch 22 Batch 139: Loss 1.492672085762024
Model None Epoch 22 Batch 140: Loss 1.564908742904663
Model None Epoch 22 Batch 141: Loss 1.4896150827407837
Model None Epoch 22 Batch 142: Loss 1.4711343050003052
Model None Epoch 22 Batch 143: Loss 1.4789869785308838
Model None Epoch 22 Batch 144: Loss 1.4750499725341797
Model None Epoch 22 Batch 145: Loss 1.4204615354537964
Model None Epoch 22 Batch 146: Loss 1.2999731302261353
Model None Epoch 22 Batch 147: Loss 1.4534484148025513
Model None Epoch 22 Batch 148: Loss 1.4600350856781006
Model None Epoch 22 Batch 149: Loss 1.4677165746688843
Downstream Train Epoch: 22 [38400/50000 (77%)]	Loss: 1.423349
Model None Epoch 22 Batch 150: Loss 1.4233494997024536
Model None Epoch 22 Batch 151: Loss 1.5236109495162964
Model None Epoch 22 Batch 152: Loss 1.325491189956665
Model None Epoch 22 Batch 153: Loss 1.3708083629608154
Model None Epoch 22 Batch 154: Loss 1.426845908164978
Model None Epoch 22 Batch 155: Loss 1.3594352006912231
Model None Epoch 22 Batch 156: Loss 1.4443399906158447
Model None Epoch 22 Batch 157: Loss 1.3890888690948486
Model None Epoch 22 Batch 158: Loss 1.4791080951690674
Model None Epoch 22 Batch 159: Loss 1.5560859441757202
Model None Epoch 22 Batch 160: Loss 1.3321489095687866
Model None Epoch 22 Batch 161: Loss 1.432815432548523
Model None Epoch 22 Batch 162: Loss 1.2850338220596313
Model None Epoch 22 Batch 163: Loss 1.4990633726119995
Model None Epoch 22 Batch 164: Loss 1.4678691625595093
Model None Epoch 22 Batch 165: Loss 1.516136884689331
Model None Epoch 22 Batch 166: Loss 1.3426254987716675
Model None Epoch 22 Batch 167: Loss 1.470726728439331
Model None Epoch 22 Batch 168: Loss 1.4356991052627563
Model None Epoch 22 Batch 169: Loss 1.4533461332321167
Model None Epoch 22 Batch 170: Loss 1.370783805847168
Model None Epoch 22 Batch 171: Loss 1.4321261644363403
Model None Epoch 22 Batch 172: Loss 1.4252071380615234
Model None Epoch 22 Batch 173: Loss 1.3599275350570679
Model None Epoch 22 Batch 174: Loss 1.6057910919189453
Model None Epoch 22 Batch 175: Loss 1.4250659942626953
Model None Epoch 22 Batch 176: Loss 1.4438209533691406
Model None Epoch 22 Batch 177: Loss 1.5699741840362549
Model None Epoch 22 Batch 178: Loss 1.4649430513381958
Model None Epoch 22 Batch 179: Loss 1.464876651763916
Model None Epoch 22 Batch 180: Loss 1.5071032047271729
Model None Epoch 22 Batch 181: Loss 1.4210628271102905
Model None Epoch 22 Batch 182: Loss 1.3809840679168701
Model None Epoch 22 Batch 183: Loss 1.3971896171569824
Model None Epoch 22 Batch 184: Loss 1.4103682041168213
Model None Epoch 22 Batch 185: Loss 1.442215085029602
Model None Epoch 22 Batch 186: Loss 1.5221226215362549
Model None Epoch 22 Batch 187: Loss 1.4467235803604126
Model None Epoch 22 Batch 188: Loss 1.4401679039001465
Model None Epoch 22 Batch 189: Loss 1.5195232629776
Model None Epoch 22 Batch 190: Loss 1.4331707954406738
Model None Epoch 22 Batch 191: Loss 1.409986972808838
Model None Epoch 22 Batch 192: Loss 1.378316044807434
Model None Epoch 22 Batch 193: Loss 1.4744867086410522
Model None Epoch 22 Batch 194: Loss 1.370957612991333
Model None Epoch 22 Batch 195: Loss 1.5308562517166138

 Downstream Train loss: 1.4298381513478804 Acc: 0.5519
Downstream Train Epoch: 23 [0/50000 (0%)]	Loss: 1.462516
Model None Epoch 23 Batch 0: Loss 1.4625157117843628
Model None Epoch 23 Batch 1: Loss 1.4180004596710205
Model None Epoch 23 Batch 2: Loss 1.4909753799438477
Model None Epoch 23 Batch 3: Loss 1.5563154220581055
Model None Epoch 23 Batch 4: Loss 1.4893913269042969
Model None Epoch 23 Batch 5: Loss 1.4145046472549438
Model None Epoch 23 Batch 6: Loss 1.5022826194763184
Model None Epoch 23 Batch 7: Loss 1.3666571378707886
Model None Epoch 23 Batch 8: Loss 1.3419848680496216
Model None Epoch 23 Batch 9: Loss 1.3692561388015747
Model None Epoch 23 Batch 10: Loss 1.3931884765625
Model None Epoch 23 Batch 11: Loss 1.4762132167816162
Model None Epoch 23 Batch 12: Loss 1.4059579372406006
Model None Epoch 23 Batch 13: Loss 1.477564811706543
Model None Epoch 23 Batch 14: Loss 1.5588384866714478
Model None Epoch 23 Batch 15: Loss 1.4044184684753418
Model None Epoch 23 Batch 16: Loss 1.3959004878997803
Model None Epoch 23 Batch 17: Loss 1.4567707777023315
Model None Epoch 23 Batch 18: Loss 1.5811971426010132
Model None Epoch 23 Batch 19: Loss 1.4621281623840332
Model None Epoch 23 Batch 20: Loss 1.435638427734375
Model None Epoch 23 Batch 21: Loss 1.3849215507507324
Model None Epoch 23 Batch 22: Loss 1.4262360334396362
Model None Epoch 23 Batch 23: Loss 1.3936822414398193
Model None Epoch 23 Batch 24: Loss 1.281424641609192
Model None Epoch 23 Batch 25: Loss 1.4326366186141968
Model None Epoch 23 Batch 26: Loss 1.4815698862075806
Model None Epoch 23 Batch 27: Loss 1.5025146007537842
Model None Epoch 23 Batch 28: Loss 1.329302430152893
Model None Epoch 23 Batch 29: Loss 1.3624681234359741
Model None Epoch 23 Batch 30: Loss 1.3928383588790894
Model None Epoch 23 Batch 31: Loss 1.3786838054656982
Model None Epoch 23 Batch 32: Loss 1.475054144859314
Model None Epoch 23 Batch 33: Loss 1.3888918161392212
Model None Epoch 23 Batch 34: Loss 1.4050087928771973
Model None Epoch 23 Batch 35: Loss 1.4911041259765625
Model None Epoch 23 Batch 36: Loss 1.418310523033142
Model None Epoch 23 Batch 37: Loss 1.3198137283325195
Model None Epoch 23 Batch 38: Loss 1.5249594449996948
Model None Epoch 23 Batch 39: Loss 1.5069174766540527
Model None Epoch 23 Batch 40: Loss 1.5386258363723755
Model None Epoch 23 Batch 41: Loss 1.395578384399414
Model None Epoch 23 Batch 42: Loss 1.4425424337387085
Model None Epoch 23 Batch 43: Loss 1.464712381362915
Model None Epoch 23 Batch 44: Loss 1.5729278326034546
Model None Epoch 23 Batch 45: Loss 1.4478179216384888
Model None Epoch 23 Batch 46: Loss 1.4235581159591675
Model None Epoch 23 Batch 47: Loss 1.4885958433151245
Model None Epoch 23 Batch 48: Loss 1.582987904548645
Model None Epoch 23 Batch 49: Loss 1.4128367900848389
Downstream Train Epoch: 23 [12800/50000 (26%)]	Loss: 1.421181
Model None Epoch 23 Batch 50: Loss 1.4211809635162354
Model None Epoch 23 Batch 51: Loss 1.5585966110229492
Model None Epoch 23 Batch 52: Loss 1.337037205696106
Model None Epoch 23 Batch 53: Loss 1.4608370065689087
Model None Epoch 23 Batch 54: Loss 1.371718406677246
Model None Epoch 23 Batch 55: Loss 1.4842671155929565
Model None Epoch 23 Batch 56: Loss 1.3496472835540771
Model None Epoch 23 Batch 57: Loss 1.4544057846069336
Model None Epoch 23 Batch 58: Loss 1.4409468173980713
Model None Epoch 23 Batch 59: Loss 1.5140939950942993
Model None Epoch 23 Batch 60: Loss 1.43368661403656
Model None Epoch 23 Batch 61: Loss 1.462483286857605
Model None Epoch 23 Batch 62: Loss 1.436091661453247
Model None Epoch 23 Batch 63: Loss 1.5378825664520264
Model None Epoch 23 Batch 64: Loss 1.4964118003845215
Model None Epoch 23 Batch 65: Loss 1.3891385793685913
Model None Epoch 23 Batch 66: Loss 1.5105904340744019
Model None Epoch 23 Batch 67: Loss 1.383980393409729
Model None Epoch 23 Batch 68: Loss 1.5336027145385742
Model None Epoch 23 Batch 69: Loss 1.385485053062439
Model None Epoch 23 Batch 70: Loss 1.3753376007080078
Model None Epoch 23 Batch 71: Loss 1.4173600673675537
Model None Epoch 23 Batch 72: Loss 1.4177806377410889
Model None Epoch 23 Batch 73: Loss 1.407181739807129
Model None Epoch 23 Batch 74: Loss 1.3199174404144287
Model None Epoch 23 Batch 75: Loss 1.37300443649292
Model None Epoch 23 Batch 76: Loss 1.3600311279296875
Model None Epoch 23 Batch 77: Loss 1.4577007293701172
Model None Epoch 23 Batch 78: Loss 1.3552308082580566
Model None Epoch 23 Batch 79: Loss 1.4554872512817383
Model None Epoch 23 Batch 80: Loss 1.4229968786239624
Model None Epoch 23 Batch 81: Loss 1.3555433750152588
Model None Epoch 23 Batch 82: Loss 1.4176281690597534
Model None Epoch 23 Batch 83: Loss 1.4389852285385132
Model None Epoch 23 Batch 84: Loss 1.508812427520752
Model None Epoch 23 Batch 85: Loss 1.3902597427368164
Model None Epoch 23 Batch 86: Loss 1.2904784679412842
Model None Epoch 23 Batch 87: Loss 1.4044564962387085
Model None Epoch 23 Batch 88: Loss 1.4784390926361084
Model None Epoch 23 Batch 89: Loss 1.4055427312850952
Model None Epoch 23 Batch 90: Loss 1.394524097442627
Model None Epoch 23 Batch 91: Loss 1.4751050472259521
Model None Epoch 23 Batch 92: Loss 1.359296441078186
Model None Epoch 23 Batch 93: Loss 1.3492358922958374
Model None Epoch 23 Batch 94: Loss 1.4215450286865234
Model None Epoch 23 Batch 95: Loss 1.2867451906204224
Model None Epoch 23 Batch 96: Loss 1.3918461799621582
Model None Epoch 23 Batch 97: Loss 1.529309868812561
Model None Epoch 23 Batch 98: Loss 1.350921869277954
Model None Epoch 23 Batch 99: Loss 1.4022070169448853
Downstream Train Epoch: 23 [25600/50000 (51%)]	Loss: 1.431046
Model None Epoch 23 Batch 100: Loss 1.431046485900879
Model None Epoch 23 Batch 101: Loss 1.408049464225769
Model None Epoch 23 Batch 102: Loss 1.4901487827301025
Model None Epoch 23 Batch 103: Loss 1.4568837881088257
Model None Epoch 23 Batch 104: Loss 1.4461649656295776
Model None Epoch 23 Batch 105: Loss 1.55039381980896
Model None Epoch 23 Batch 106: Loss 1.4294790029525757
Model None Epoch 23 Batch 107: Loss 1.5288424491882324
Model None Epoch 23 Batch 108: Loss 1.4982669353485107
Model None Epoch 23 Batch 109: Loss 1.3651134967803955
Model None Epoch 23 Batch 110: Loss 1.4616371393203735
Model None Epoch 23 Batch 111: Loss 1.411872148513794
Model None Epoch 23 Batch 112: Loss 1.468650221824646
Model None Epoch 23 Batch 113: Loss 1.3695851564407349
Model None Epoch 23 Batch 114: Loss 1.4695295095443726
Model None Epoch 23 Batch 115: Loss 1.5307879447937012
Model None Epoch 23 Batch 116: Loss 1.4977651834487915
Model None Epoch 23 Batch 117: Loss 1.3724359273910522
Model None Epoch 23 Batch 118: Loss 1.3851768970489502
Model None Epoch 23 Batch 119: Loss 1.410338044166565
Model None Epoch 23 Batch 120: Loss 1.338567852973938
Model None Epoch 23 Batch 121: Loss 1.492912769317627
Model None Epoch 23 Batch 122: Loss 1.446250557899475
Model None Epoch 23 Batch 123: Loss 1.46401846408844
Model None Epoch 23 Batch 124: Loss 1.4071314334869385
Model None Epoch 23 Batch 125: Loss 1.4265238046646118
Model None Epoch 23 Batch 126: Loss 1.4698853492736816
Model None Epoch 23 Batch 127: Loss 1.5031437873840332
Model None Epoch 23 Batch 128: Loss 1.42732572555542
Model None Epoch 23 Batch 129: Loss 1.4584189653396606
Model None Epoch 23 Batch 130: Loss 1.4420708417892456
Model None Epoch 23 Batch 131: Loss 1.2943010330200195
Model None Epoch 23 Batch 132: Loss 1.4533591270446777
Model None Epoch 23 Batch 133: Loss 1.4163645505905151
Model None Epoch 23 Batch 134: Loss 1.3933268785476685
Model None Epoch 23 Batch 135: Loss 1.4472593069076538
Model None Epoch 23 Batch 136: Loss 1.4391077756881714
Model None Epoch 23 Batch 137: Loss 1.4757611751556396
Model None Epoch 23 Batch 138: Loss 1.3660178184509277
Model None Epoch 23 Batch 139: Loss 1.5363789796829224
Model None Epoch 23 Batch 140: Loss 1.4474380016326904
Model None Epoch 23 Batch 141: Loss 1.4199118614196777
Model None Epoch 23 Batch 142: Loss 1.3483514785766602
Model None Epoch 23 Batch 143: Loss 1.3742210865020752
Model None Epoch 23 Batch 144: Loss 1.455859899520874
Model None Epoch 23 Batch 145: Loss 1.479884386062622
Model None Epoch 23 Batch 146: Loss 1.3805875778198242
Model None Epoch 23 Batch 147: Loss 1.3842328786849976
Model None Epoch 23 Batch 148: Loss 1.4637746810913086
Model None Epoch 23 Batch 149: Loss 1.3660914897918701
Downstream Train Epoch: 23 [38400/50000 (77%)]	Loss: 1.463071
Model None Epoch 23 Batch 150: Loss 1.4630712270736694
Model None Epoch 23 Batch 151: Loss 1.3922392129898071
Model None Epoch 23 Batch 152: Loss 1.3973028659820557
Model None Epoch 23 Batch 153: Loss 1.4717520475387573
Model None Epoch 23 Batch 154: Loss 1.3905285596847534
Model None Epoch 23 Batch 155: Loss 1.4329283237457275
Model None Epoch 23 Batch 156: Loss 1.413485050201416
Model None Epoch 23 Batch 157: Loss 1.5049858093261719
Model None Epoch 23 Batch 158: Loss 1.4052945375442505
Model None Epoch 23 Batch 159: Loss 1.5371036529541016
Model None Epoch 23 Batch 160: Loss 1.3370462656021118
Model None Epoch 23 Batch 161: Loss 1.3778247833251953
Model None Epoch 23 Batch 162: Loss 1.4412723779678345
Model None Epoch 23 Batch 163: Loss 1.4320975542068481
Model None Epoch 23 Batch 164: Loss 1.393364667892456
Model None Epoch 23 Batch 165: Loss 1.4708497524261475
Model None Epoch 23 Batch 166: Loss 1.3401892185211182
Model None Epoch 23 Batch 167: Loss 1.3350348472595215
Model None Epoch 23 Batch 168: Loss 1.4109033346176147
Model None Epoch 23 Batch 169: Loss 1.3703773021697998
Model None Epoch 23 Batch 170: Loss 1.3672022819519043
Model None Epoch 23 Batch 171: Loss 1.3884223699569702
Model None Epoch 23 Batch 172: Loss 1.5393385887145996
Model None Epoch 23 Batch 173: Loss 1.4183919429779053
Model None Epoch 23 Batch 174: Loss 1.425998330116272
Model None Epoch 23 Batch 175: Loss 1.4293488264083862
Model None Epoch 23 Batch 176: Loss 1.5101535320281982
Model None Epoch 23 Batch 177: Loss 1.4337668418884277
Model None Epoch 23 Batch 178: Loss 1.48626708984375
Model None Epoch 23 Batch 179: Loss 1.5206520557403564
Model None Epoch 23 Batch 180: Loss 1.441955804824829
Model None Epoch 23 Batch 181: Loss 1.4968699216842651
Model None Epoch 23 Batch 182: Loss 1.3996740579605103
Model None Epoch 23 Batch 183: Loss 1.349678635597229
Model None Epoch 23 Batch 184: Loss 1.4142473936080933
Model None Epoch 23 Batch 185: Loss 1.5354599952697754
Model None Epoch 23 Batch 186: Loss 1.4537854194641113
Model None Epoch 23 Batch 187: Loss 1.388722538948059
Model None Epoch 23 Batch 188: Loss 1.383065104484558
Model None Epoch 23 Batch 189: Loss 1.461348295211792
Model None Epoch 23 Batch 190: Loss 1.4251445531845093
Model None Epoch 23 Batch 191: Loss 1.3419761657714844
Model None Epoch 23 Batch 192: Loss 1.4201722145080566
Model None Epoch 23 Batch 193: Loss 1.4593968391418457
Model None Epoch 23 Batch 194: Loss 1.389981746673584
Model None Epoch 23 Batch 195: Loss 1.416616439819336

 Downstream Train loss: 1.4312099619787566 Acc: 0.5519
Downstream Train Epoch: 24 [0/50000 (0%)]	Loss: 1.433242
Model None Epoch 24 Batch 0: Loss 1.4332420825958252
Model None Epoch 24 Batch 1: Loss 1.520258903503418
Model None Epoch 24 Batch 2: Loss 1.45021390914917
Model None Epoch 24 Batch 3: Loss 1.4611741304397583
Model None Epoch 24 Batch 4: Loss 1.3486043214797974
Model None Epoch 24 Batch 5: Loss 1.2690891027450562
Model None Epoch 24 Batch 6: Loss 1.4599899053573608
Model None Epoch 24 Batch 7: Loss 1.4764344692230225
Model None Epoch 24 Batch 8: Loss 1.4843597412109375
Model None Epoch 24 Batch 9: Loss 1.47472083568573
Model None Epoch 24 Batch 10: Loss 1.448136806488037
Model None Epoch 24 Batch 11: Loss 1.4275405406951904
Model None Epoch 24 Batch 12: Loss 1.5116066932678223
Model None Epoch 24 Batch 13: Loss 1.5550916194915771
Model None Epoch 24 Batch 14: Loss 1.4368489980697632
Model None Epoch 24 Batch 15: Loss 1.4628276824951172
Model None Epoch 24 Batch 16: Loss 1.320228099822998
Model None Epoch 24 Batch 17: Loss 1.3492366075515747
Model None Epoch 24 Batch 18: Loss 1.3479148149490356
Model None Epoch 24 Batch 19: Loss 1.334414005279541
Model None Epoch 24 Batch 20: Loss 1.373018741607666
Model None Epoch 24 Batch 21: Loss 1.4628616571426392
Model None Epoch 24 Batch 22: Loss 1.4784168004989624
Model None Epoch 24 Batch 23: Loss 1.4650546312332153
Model None Epoch 24 Batch 24: Loss 1.389840006828308
Model None Epoch 24 Batch 25: Loss 1.3772603273391724
Model None Epoch 24 Batch 26: Loss 1.470104455947876
Model None Epoch 24 Batch 27: Loss 1.3863669633865356
Model None Epoch 24 Batch 28: Loss 1.3542110919952393
Model None Epoch 24 Batch 29: Loss 1.4662879705429077
Model None Epoch 24 Batch 30: Loss 1.4139333963394165
Model None Epoch 24 Batch 31: Loss 1.4784303903579712
Model None Epoch 24 Batch 32: Loss 1.4924507141113281
Model None Epoch 24 Batch 33: Loss 1.5965970754623413
Model None Epoch 24 Batch 34: Loss 1.3621470928192139
Model None Epoch 24 Batch 35: Loss 1.4623020887374878
Model None Epoch 24 Batch 36: Loss 1.3308414220809937
Model None Epoch 24 Batch 37: Loss 1.4914288520812988
Model None Epoch 24 Batch 38: Loss 1.4672259092330933
Model None Epoch 24 Batch 39: Loss 1.415934681892395
Model None Epoch 24 Batch 40: Loss 1.309108853340149
Model None Epoch 24 Batch 41: Loss 1.4533350467681885
Model None Epoch 24 Batch 42: Loss 1.462066888809204
Model None Epoch 24 Batch 43: Loss 1.474765419960022
Model None Epoch 24 Batch 44: Loss 1.371191143989563
Model None Epoch 24 Batch 45: Loss 1.3697186708450317
Model None Epoch 24 Batch 46: Loss 1.4796106815338135
Model None Epoch 24 Batch 47: Loss 1.417130708694458
Model None Epoch 24 Batch 48: Loss 1.2847341299057007
Model None Epoch 24 Batch 49: Loss 1.4570530652999878
Downstream Train Epoch: 24 [12800/50000 (26%)]	Loss: 1.417721
Model None Epoch 24 Batch 50: Loss 1.4177206754684448
Model None Epoch 24 Batch 51: Loss 1.5192300081253052
Model None Epoch 24 Batch 52: Loss 1.5185543298721313
Model None Epoch 24 Batch 53: Loss 1.5352078676223755
Model None Epoch 24 Batch 54: Loss 1.3627339601516724
Model None Epoch 24 Batch 55: Loss 1.311075210571289
Model None Epoch 24 Batch 56: Loss 1.3418980836868286
Model None Epoch 24 Batch 57: Loss 1.4258838891983032
Model None Epoch 24 Batch 58: Loss 1.3462655544281006
Model None Epoch 24 Batch 59: Loss 1.3771708011627197
Model None Epoch 24 Batch 60: Loss 1.5203094482421875
Model None Epoch 24 Batch 61: Loss 1.4333001375198364
Model None Epoch 24 Batch 62: Loss 1.4143788814544678
Model None Epoch 24 Batch 63: Loss 1.3641455173492432
Model None Epoch 24 Batch 64: Loss 1.5209858417510986
Model None Epoch 24 Batch 65: Loss 1.4235963821411133
Model None Epoch 24 Batch 66: Loss 1.4322097301483154
Model None Epoch 24 Batch 67: Loss 1.3187294006347656
Model None Epoch 24 Batch 68: Loss 1.3559691905975342
Model None Epoch 24 Batch 69: Loss 1.3724116086959839
Model None Epoch 24 Batch 70: Loss 1.417866826057434
Model None Epoch 24 Batch 71: Loss 1.3818899393081665
Model None Epoch 24 Batch 72: Loss 1.3074997663497925
Model None Epoch 24 Batch 73: Loss 1.443649172782898
Model None Epoch 24 Batch 74: Loss 1.4338597059249878
Model None Epoch 24 Batch 75: Loss 1.3703749179840088
Model None Epoch 24 Batch 76: Loss 1.3619238138198853
Model None Epoch 24 Batch 77: Loss 1.534617304801941
Model None Epoch 24 Batch 78: Loss 1.4480090141296387
Model None Epoch 24 Batch 79: Loss 1.4425843954086304
Model None Epoch 24 Batch 80: Loss 1.434731364250183
Model None Epoch 24 Batch 81: Loss 1.4719481468200684
Model None Epoch 24 Batch 82: Loss 1.353531837463379
Model None Epoch 24 Batch 83: Loss 1.423779010772705
Model None Epoch 24 Batch 84: Loss 1.3647937774658203
Model None Epoch 24 Batch 85: Loss 1.341506838798523
Model None Epoch 24 Batch 86: Loss 1.4891334772109985
Model None Epoch 24 Batch 87: Loss 1.504108190536499
Model None Epoch 24 Batch 88: Loss 1.4101670980453491
Model None Epoch 24 Batch 89: Loss 1.3890390396118164
Model None Epoch 24 Batch 90: Loss 1.3455696105957031
Model None Epoch 24 Batch 91: Loss 1.4512197971343994
Model None Epoch 24 Batch 92: Loss 1.3686538934707642
Model None Epoch 24 Batch 93: Loss 1.4402438402175903
Model None Epoch 24 Batch 94: Loss 1.4348106384277344
Model None Epoch 24 Batch 95: Loss 1.4604064226150513
Model None Epoch 24 Batch 96: Loss 1.421218752861023
Model None Epoch 24 Batch 97: Loss 1.3087714910507202
Model None Epoch 24 Batch 98: Loss 1.318276286125183
Model None Epoch 24 Batch 99: Loss 1.514701008796692
Downstream Train Epoch: 24 [25600/50000 (51%)]	Loss: 1.404624
Model None Epoch 24 Batch 100: Loss 1.404624104499817
Model None Epoch 24 Batch 101: Loss 1.4343411922454834
Model None Epoch 24 Batch 102: Loss 1.4933346509933472
Model None Epoch 24 Batch 103: Loss 1.4048508405685425
Model None Epoch 24 Batch 104: Loss 1.3278340101242065
Model None Epoch 24 Batch 105: Loss 1.5499149560928345
Model None Epoch 24 Batch 106: Loss 1.3273578882217407
Model None Epoch 24 Batch 107: Loss 1.428903579711914
Model None Epoch 24 Batch 108: Loss 1.343332052230835
Model None Epoch 24 Batch 109: Loss 1.3160721063613892
Model None Epoch 24 Batch 110: Loss 1.5358493328094482
Model None Epoch 24 Batch 111: Loss 1.591662049293518
Model None Epoch 24 Batch 112: Loss 1.3597997426986694
Model None Epoch 24 Batch 113: Loss 1.373905897140503
Model None Epoch 24 Batch 114: Loss 1.4212385416030884
Model None Epoch 24 Batch 115: Loss 1.5480543375015259
Model None Epoch 24 Batch 116: Loss 1.4310545921325684
Model None Epoch 24 Batch 117: Loss 1.581467866897583
Model None Epoch 24 Batch 118: Loss 1.3961211442947388
Model None Epoch 24 Batch 119: Loss 1.499671220779419
Model None Epoch 24 Batch 120: Loss 1.4793332815170288
Model None Epoch 24 Batch 121: Loss 1.438852310180664
Model None Epoch 24 Batch 122: Loss 1.390071153640747
Model None Epoch 24 Batch 123: Loss 1.4885892868041992
Model None Epoch 24 Batch 124: Loss 1.4279996156692505
Model None Epoch 24 Batch 125: Loss 1.4422357082366943
Model None Epoch 24 Batch 126: Loss 1.37087082862854
Model None Epoch 24 Batch 127: Loss 1.309487223625183
Model None Epoch 24 Batch 128: Loss 1.450884461402893
Model None Epoch 24 Batch 129: Loss 1.388932704925537
Model None Epoch 24 Batch 130: Loss 1.4057555198669434
Model None Epoch 24 Batch 131: Loss 1.4487494230270386
Model None Epoch 24 Batch 132: Loss 1.5444632768630981
Model None Epoch 24 Batch 133: Loss 1.466476321220398
Model None Epoch 24 Batch 134: Loss 1.2707469463348389
Model None Epoch 24 Batch 135: Loss 1.4765392541885376
Model None Epoch 24 Batch 136: Loss 1.4329009056091309
Model None Epoch 24 Batch 137: Loss 1.3832871913909912
Model None Epoch 24 Batch 138: Loss 1.4228464365005493
Model None Epoch 24 Batch 139: Loss 1.4034327268600464
Model None Epoch 24 Batch 140: Loss 1.398307204246521
Model None Epoch 24 Batch 141: Loss 1.4806846380233765
Model None Epoch 24 Batch 142: Loss 1.4135336875915527
Model None Epoch 24 Batch 143: Loss 1.336552619934082
Model None Epoch 24 Batch 144: Loss 1.4609767198562622
Model None Epoch 24 Batch 145: Loss 1.3903073072433472
Model None Epoch 24 Batch 146: Loss 1.5520695447921753
Model None Epoch 24 Batch 147: Loss 1.4745491743087769
Model None Epoch 24 Batch 148: Loss 1.5024515390396118
Model None Epoch 24 Batch 149: Loss 1.4265995025634766
Downstream Train Epoch: 24 [38400/50000 (77%)]	Loss: 1.382942
Model None Epoch 24 Batch 150: Loss 1.3829421997070312
Model None Epoch 24 Batch 151: Loss 1.5704132318496704
Model None Epoch 24 Batch 152: Loss 1.4455063343048096
Model None Epoch 24 Batch 153: Loss 1.4079394340515137
Model None Epoch 24 Batch 154: Loss 1.4985485076904297
Model None Epoch 24 Batch 155: Loss 1.4196897745132446
Model None Epoch 24 Batch 156: Loss 1.4610856771469116
Model None Epoch 24 Batch 157: Loss 1.3571573495864868
Model None Epoch 24 Batch 158: Loss 1.4812536239624023
Model None Epoch 24 Batch 159: Loss 1.4412989616394043
Model None Epoch 24 Batch 160: Loss 1.4721165895462036
Model None Epoch 24 Batch 161: Loss 1.4115357398986816
Model None Epoch 24 Batch 162: Loss 1.4352624416351318
Model None Epoch 24 Batch 163: Loss 1.449366807937622
Model None Epoch 24 Batch 164: Loss 1.4913634061813354
Model None Epoch 24 Batch 165: Loss 1.4543347358703613
Model None Epoch 24 Batch 166: Loss 1.38346529006958
Model None Epoch 24 Batch 167: Loss 1.383247971534729
Model None Epoch 24 Batch 168: Loss 1.388498067855835
Model None Epoch 24 Batch 169: Loss 1.3309866189956665
Model None Epoch 24 Batch 170: Loss 1.4468979835510254
Model None Epoch 24 Batch 171: Loss 1.3261332511901855
Model None Epoch 24 Batch 172: Loss 1.4922337532043457
Model None Epoch 24 Batch 173: Loss 1.411461591720581
Model None Epoch 24 Batch 174: Loss 1.4461963176727295
Model None Epoch 24 Batch 175: Loss 1.3563686609268188
Model None Epoch 24 Batch 176: Loss 1.4556514024734497
Model None Epoch 24 Batch 177: Loss 1.3745312690734863
Model None Epoch 24 Batch 178: Loss 1.4475607872009277
Model None Epoch 24 Batch 179: Loss 1.4050993919372559
Model None Epoch 24 Batch 180: Loss 1.4322465658187866
Model None Epoch 24 Batch 181: Loss 1.2962453365325928
Model None Epoch 24 Batch 182: Loss 1.4414801597595215
Model None Epoch 24 Batch 183: Loss 1.4207534790039062
Model None Epoch 24 Batch 184: Loss 1.3812298774719238
Model None Epoch 24 Batch 185: Loss 1.3740657567977905
Model None Epoch 24 Batch 186: Loss 1.3868625164031982
Model None Epoch 24 Batch 187: Loss 1.4234232902526855
Model None Epoch 24 Batch 188: Loss 1.4210165739059448
Model None Epoch 24 Batch 189: Loss 1.4322962760925293
Model None Epoch 24 Batch 190: Loss 1.3917546272277832
Model None Epoch 24 Batch 191: Loss 1.4494072198867798
Model None Epoch 24 Batch 192: Loss 1.4961916208267212
Model None Epoch 24 Batch 193: Loss 1.371977686882019
Model None Epoch 24 Batch 194: Loss 1.5026975870132446
Model None Epoch 24 Batch 195: Loss 1.5365692377090454

 Downstream Train loss: 1.4251033961772919 Acc: 0.5614
Downstream Train Epoch: 25 [0/50000 (0%)]	Loss: 1.385391
Model None Epoch 25 Batch 0: Loss 1.3853914737701416
Model None Epoch 25 Batch 1: Loss 1.509735345840454
Model None Epoch 25 Batch 2: Loss 1.3992213010787964
Model None Epoch 25 Batch 3: Loss 1.3600422143936157
Model None Epoch 25 Batch 4: Loss 1.5044299364089966
Model None Epoch 25 Batch 5: Loss 1.340262770652771
Model None Epoch 25 Batch 6: Loss 1.4512003660202026
Model None Epoch 25 Batch 7: Loss 1.4229165315628052
Model None Epoch 25 Batch 8: Loss 1.4651446342468262
Model None Epoch 25 Batch 9: Loss 1.465903401374817
Model None Epoch 25 Batch 10: Loss 1.4814280271530151
Model None Epoch 25 Batch 11: Loss 1.294294834136963
Model None Epoch 25 Batch 12: Loss 1.503240704536438
Model None Epoch 25 Batch 13: Loss 1.3590770959854126
Model None Epoch 25 Batch 14: Loss 1.4013586044311523
Model None Epoch 25 Batch 15: Loss 1.4790477752685547
Model None Epoch 25 Batch 16: Loss 1.3241201639175415
Model None Epoch 25 Batch 17: Loss 1.3281620740890503
Model None Epoch 25 Batch 18: Loss 1.443205714225769
Model None Epoch 25 Batch 19: Loss 1.3724881410598755
Model None Epoch 25 Batch 20: Loss 1.4419203996658325
Model None Epoch 25 Batch 21: Loss 1.3531783819198608
Model None Epoch 25 Batch 22: Loss 1.513914942741394
Model None Epoch 25 Batch 23: Loss 1.4080544710159302
Model None Epoch 25 Batch 24: Loss 1.4456472396850586
Model None Epoch 25 Batch 25: Loss 1.4386935234069824
Model None Epoch 25 Batch 26: Loss 1.4083043336868286
Model None Epoch 25 Batch 27: Loss 1.3681191205978394
Model None Epoch 25 Batch 28: Loss 1.4236031770706177
Model None Epoch 25 Batch 29: Loss 1.31106698513031
Model None Epoch 25 Batch 30: Loss 1.4257904291152954
Model None Epoch 25 Batch 31: Loss 1.5026073455810547
Model None Epoch 25 Batch 32: Loss 1.4760644435882568
Model None Epoch 25 Batch 33: Loss 1.3266253471374512
Model None Epoch 25 Batch 34: Loss 1.4131289720535278
Model None Epoch 25 Batch 35: Loss 1.5101783275604248
Model None Epoch 25 Batch 36: Loss 1.3885682821273804
Model None Epoch 25 Batch 37: Loss 1.3030893802642822
Model None Epoch 25 Batch 38: Loss 1.4248474836349487
Model None Epoch 25 Batch 39: Loss 1.4262062311172485
Model None Epoch 25 Batch 40: Loss 1.285807490348816
Model None Epoch 25 Batch 41: Loss 1.3701155185699463
Model None Epoch 25 Batch 42: Loss 1.5038756132125854
Model None Epoch 25 Batch 43: Loss 1.4599988460540771
Model None Epoch 25 Batch 44: Loss 1.445953130722046
Model None Epoch 25 Batch 45: Loss 1.3908531665802002
Model None Epoch 25 Batch 46: Loss 1.4113552570343018
Model None Epoch 25 Batch 47: Loss 1.3641606569290161
Model None Epoch 25 Batch 48: Loss 1.482788324356079
Model None Epoch 25 Batch 49: Loss 1.4575181007385254
Downstream Train Epoch: 25 [12800/50000 (26%)]	Loss: 1.582653
Model None Epoch 25 Batch 50: Loss 1.5826529264450073
Model None Epoch 25 Batch 51: Loss 1.4191982746124268
Model None Epoch 25 Batch 52: Loss 1.4370763301849365
Model None Epoch 25 Batch 53: Loss 1.386662244796753
Model None Epoch 25 Batch 54: Loss 1.424556851387024
Model None Epoch 25 Batch 55: Loss 1.3713849782943726
Model None Epoch 25 Batch 56: Loss 1.4176337718963623
Model None Epoch 25 Batch 57: Loss 1.4034384489059448
Model None Epoch 25 Batch 58: Loss 1.3991355895996094
Model None Epoch 25 Batch 59: Loss 1.4086289405822754
Model None Epoch 25 Batch 60: Loss 1.3651752471923828
Model None Epoch 25 Batch 61: Loss 1.4387794733047485
Model None Epoch 25 Batch 62: Loss 1.277783751487732
Model None Epoch 25 Batch 63: Loss 1.4940704107284546
Model None Epoch 25 Batch 64: Loss 1.297228455543518
Model None Epoch 25 Batch 65: Loss 1.3748104572296143
Model None Epoch 25 Batch 66: Loss 1.492562174797058
Model None Epoch 25 Batch 67: Loss 1.4178906679153442
Model None Epoch 25 Batch 68: Loss 1.4173986911773682
Model None Epoch 25 Batch 69: Loss 1.3519829511642456
Model None Epoch 25 Batch 70: Loss 1.5069950819015503
Model None Epoch 25 Batch 71: Loss 1.4435147047042847
Model None Epoch 25 Batch 72: Loss 1.196285367012024
Model None Epoch 25 Batch 73: Loss 1.4957265853881836
Model None Epoch 25 Batch 74: Loss 1.4299063682556152
Model None Epoch 25 Batch 75: Loss 1.5169373750686646
Model None Epoch 25 Batch 76: Loss 1.40743887424469
Model None Epoch 25 Batch 77: Loss 1.4291609525680542
Model None Epoch 25 Batch 78: Loss 1.4309818744659424
Model None Epoch 25 Batch 79: Loss 1.4143060445785522
Model None Epoch 25 Batch 80: Loss 1.4003392457962036
Model None Epoch 25 Batch 81: Loss 1.494363784790039
Model None Epoch 25 Batch 82: Loss 1.453229308128357
Model None Epoch 25 Batch 83: Loss 1.4201178550720215
Model None Epoch 25 Batch 84: Loss 1.5857822895050049
Model None Epoch 25 Batch 85: Loss 1.5297297239303589
Model None Epoch 25 Batch 86: Loss 1.3717373609542847
Model None Epoch 25 Batch 87: Loss 1.3151671886444092
Model None Epoch 25 Batch 88: Loss 1.5545381307601929
Model None Epoch 25 Batch 89: Loss 1.5778533220291138
Model None Epoch 25 Batch 90: Loss 1.403525471687317
Model None Epoch 25 Batch 91: Loss 1.4704787731170654
Model None Epoch 25 Batch 92: Loss 1.3625926971435547
Model None Epoch 25 Batch 93: Loss 1.552234411239624
Model None Epoch 25 Batch 94: Loss 1.3920961618423462
Model None Epoch 25 Batch 95: Loss 1.5239379405975342
Model None Epoch 25 Batch 96: Loss 1.3885424137115479
Model None Epoch 25 Batch 97: Loss 1.382981777191162
Model None Epoch 25 Batch 98: Loss 1.387089729309082
Model None Epoch 25 Batch 99: Loss 1.522378921508789
Downstream Train Epoch: 25 [25600/50000 (51%)]	Loss: 1.401817
Model None Epoch 25 Batch 100: Loss 1.4018168449401855
Model None Epoch 25 Batch 101: Loss 1.3985307216644287
Model None Epoch 25 Batch 102: Loss 1.4038783311843872
Model None Epoch 25 Batch 103: Loss 1.3598591089248657
Model None Epoch 25 Batch 104: Loss 1.3996386528015137
Model None Epoch 25 Batch 105: Loss 1.370362401008606
Model None Epoch 25 Batch 106: Loss 1.5509347915649414
Model None Epoch 25 Batch 107: Loss 1.5781546831130981
Model None Epoch 25 Batch 108: Loss 1.5082682371139526
Model None Epoch 25 Batch 109: Loss 1.434994101524353
Model None Epoch 25 Batch 110: Loss 1.4916051626205444
Model None Epoch 25 Batch 111: Loss 1.3719393014907837
Model None Epoch 25 Batch 112: Loss 1.495683193206787
Model None Epoch 25 Batch 113: Loss 1.3929634094238281
Model None Epoch 25 Batch 114: Loss 1.5442472696304321
Model None Epoch 25 Batch 115: Loss 1.3799461126327515
Model None Epoch 25 Batch 116: Loss 1.4172581434249878
Model None Epoch 25 Batch 117: Loss 1.352589726448059
Model None Epoch 25 Batch 118: Loss 1.4656171798706055
Model None Epoch 25 Batch 119: Loss 1.4490176439285278
Model None Epoch 25 Batch 120: Loss 1.3096810579299927
Model None Epoch 25 Batch 121: Loss 1.3524885177612305
Model None Epoch 25 Batch 122: Loss 1.402555227279663
Model None Epoch 25 Batch 123: Loss 1.4564886093139648
Model None Epoch 25 Batch 124: Loss 1.4962818622589111
Model None Epoch 25 Batch 125: Loss 1.3335009813308716
Model None Epoch 25 Batch 126: Loss 1.3408241271972656
Model None Epoch 25 Batch 127: Loss 1.4849247932434082
Model None Epoch 25 Batch 128: Loss 1.4035917520523071
Model None Epoch 25 Batch 129: Loss 1.4206897020339966
Model None Epoch 25 Batch 130: Loss 1.4106148481369019
Model None Epoch 25 Batch 131: Loss 1.5257320404052734
Model None Epoch 25 Batch 132: Loss 1.332456350326538
Model None Epoch 25 Batch 133: Loss 1.366815209388733
Model None Epoch 25 Batch 134: Loss 1.438474178314209
Model None Epoch 25 Batch 135: Loss 1.3724976778030396
Model None Epoch 25 Batch 136: Loss 1.4180233478546143
Model None Epoch 25 Batch 137: Loss 1.4337824583053589
Model None Epoch 25 Batch 138: Loss 1.2543638944625854
Model None Epoch 25 Batch 139: Loss 1.4469258785247803
Model None Epoch 25 Batch 140: Loss 1.4319329261779785
Model None Epoch 25 Batch 141: Loss 1.474809169769287
Model None Epoch 25 Batch 142: Loss 1.333644151687622
Model None Epoch 25 Batch 143: Loss 1.523142695426941
Model None Epoch 25 Batch 144: Loss 1.3628815412521362
Model None Epoch 25 Batch 145: Loss 1.5485000610351562
Model None Epoch 25 Batch 146: Loss 1.4290974140167236
Model None Epoch 25 Batch 147: Loss 1.438003420829773
Model None Epoch 25 Batch 148: Loss 1.3946806192398071
Model None Epoch 25 Batch 149: Loss 1.4794423580169678
Downstream Train Epoch: 25 [38400/50000 (77%)]	Loss: 1.387710
Model None Epoch 25 Batch 150: Loss 1.3877102136611938
Model None Epoch 25 Batch 151: Loss 1.3549127578735352
Model None Epoch 25 Batch 152: Loss 1.2783907651901245
Model None Epoch 25 Batch 153: Loss 1.415766716003418
Model None Epoch 25 Batch 154: Loss 1.3652002811431885
Model None Epoch 25 Batch 155: Loss 1.3633254766464233
Model None Epoch 25 Batch 156: Loss 1.5206507444381714
Model None Epoch 25 Batch 157: Loss 1.5600143671035767
Model None Epoch 25 Batch 158: Loss 1.3650760650634766
Model None Epoch 25 Batch 159: Loss 1.3586909770965576
Model None Epoch 25 Batch 160: Loss 1.4568917751312256
Model None Epoch 25 Batch 161: Loss 1.4314473867416382
Model None Epoch 25 Batch 162: Loss 1.4875332117080688
Model None Epoch 25 Batch 163: Loss 1.5150113105773926
Model None Epoch 25 Batch 164: Loss 1.468255877494812
Model None Epoch 25 Batch 165: Loss 1.4622989892959595
Model None Epoch 25 Batch 166: Loss 1.4068483114242554
Model None Epoch 25 Batch 167: Loss 1.3522260189056396
Model None Epoch 25 Batch 168: Loss 1.4666849374771118
Model None Epoch 25 Batch 169: Loss 1.5725854635238647
Model None Epoch 25 Batch 170: Loss 1.4386590719223022
Model None Epoch 25 Batch 171: Loss 1.5376276969909668
Model None Epoch 25 Batch 172: Loss 1.4300309419631958
Model None Epoch 25 Batch 173: Loss 1.3687156438827515
Model None Epoch 25 Batch 174: Loss 1.326154112815857
Model None Epoch 25 Batch 175: Loss 1.4496114253997803
Model None Epoch 25 Batch 176: Loss 1.4053311347961426
Model None Epoch 25 Batch 177: Loss 1.4860758781433105
Model None Epoch 25 Batch 178: Loss 1.517128348350525
Model None Epoch 25 Batch 179: Loss 1.3211530447006226
Model None Epoch 25 Batch 180: Loss 1.3403480052947998
Model None Epoch 25 Batch 181: Loss 1.4192767143249512
Model None Epoch 25 Batch 182: Loss 1.3695591688156128
Model None Epoch 25 Batch 183: Loss 1.2966760396957397
Model None Epoch 25 Batch 184: Loss 1.4830915927886963
Model None Epoch 25 Batch 185: Loss 1.454819917678833
Model None Epoch 25 Batch 186: Loss 1.3971710205078125
Model None Epoch 25 Batch 187: Loss 1.3586348295211792
Model None Epoch 25 Batch 188: Loss 1.3806277513504028
Model None Epoch 25 Batch 189: Loss 1.3876903057098389
Model None Epoch 25 Batch 190: Loss 1.454960584640503
Model None Epoch 25 Batch 191: Loss 1.5810973644256592
Model None Epoch 25 Batch 192: Loss 1.3345316648483276
Model None Epoch 25 Batch 193: Loss 1.41908597946167
Model None Epoch 25 Batch 194: Loss 1.4682378768920898
Model None Epoch 25 Batch 195: Loss 1.3227251768112183

 Downstream Train loss: 1.4226194960730416 Acc: 0.5614
Downstream Train Epoch: 26 [0/50000 (0%)]	Loss: 1.397674
Model None Epoch 26 Batch 0: Loss 1.3976738452911377
Model None Epoch 26 Batch 1: Loss 1.426792025566101
Model None Epoch 26 Batch 2: Loss 1.3362104892730713
Model None Epoch 26 Batch 3: Loss 1.4304378032684326
Model None Epoch 26 Batch 4: Loss 1.371751308441162
Model None Epoch 26 Batch 5: Loss 1.4121427536010742
Model None Epoch 26 Batch 6: Loss 1.4443875551223755
Model None Epoch 26 Batch 7: Loss 1.427734136581421
Model None Epoch 26 Batch 8: Loss 1.4313178062438965
Model None Epoch 26 Batch 9: Loss 1.261433482170105
Model None Epoch 26 Batch 10: Loss 1.4053232669830322
Model None Epoch 26 Batch 11: Loss 1.3505887985229492
Model None Epoch 26 Batch 12: Loss 1.415855884552002
Model None Epoch 26 Batch 13: Loss 1.4430348873138428
Model None Epoch 26 Batch 14: Loss 1.468953251838684
Model None Epoch 26 Batch 15: Loss 1.4811580181121826
Model None Epoch 26 Batch 16: Loss 1.3583605289459229
Model None Epoch 26 Batch 17: Loss 1.4959698915481567
Model None Epoch 26 Batch 18: Loss 1.3223687410354614
Model None Epoch 26 Batch 19: Loss 1.3766387701034546
Model None Epoch 26 Batch 20: Loss 1.5161908864974976
Model None Epoch 26 Batch 21: Loss 1.4505813121795654
Model None Epoch 26 Batch 22: Loss 1.4073925018310547
Model None Epoch 26 Batch 23: Loss 1.4126425981521606
Model None Epoch 26 Batch 24: Loss 1.4817510843276978
Model None Epoch 26 Batch 25: Loss 1.5029033422470093
Model None Epoch 26 Batch 26: Loss 1.360107660293579
Model None Epoch 26 Batch 27: Loss 1.4202995300292969
Model None Epoch 26 Batch 28: Loss 1.3790366649627686
Model None Epoch 26 Batch 29: Loss 1.3695440292358398
Model None Epoch 26 Batch 30: Loss 1.4110051393508911
Model None Epoch 26 Batch 31: Loss 1.393250823020935
Model None Epoch 26 Batch 32: Loss 1.4117001295089722
Model None Epoch 26 Batch 33: Loss 1.4255270957946777
Model None Epoch 26 Batch 34: Loss 1.3966312408447266
Model None Epoch 26 Batch 35: Loss 1.5477070808410645
Model None Epoch 26 Batch 36: Loss 1.368492603302002
Model None Epoch 26 Batch 37: Loss 1.3985782861709595
Model None Epoch 26 Batch 38: Loss 1.3051787614822388
Model None Epoch 26 Batch 39: Loss 1.4741909503936768
Model None Epoch 26 Batch 40: Loss 1.4554522037506104
Model None Epoch 26 Batch 41: Loss 1.4738572835922241
Model None Epoch 26 Batch 42: Loss 1.402965784072876
Model None Epoch 26 Batch 43: Loss 1.475777506828308
Model None Epoch 26 Batch 44: Loss 1.4963663816452026
Model None Epoch 26 Batch 45: Loss 1.4910296201705933
Model None Epoch 26 Batch 46: Loss 1.5882303714752197
Model None Epoch 26 Batch 47: Loss 1.4969227313995361
Model None Epoch 26 Batch 48: Loss 1.4476909637451172
Model None Epoch 26 Batch 49: Loss 1.507674217224121
Downstream Train Epoch: 26 [12800/50000 (26%)]	Loss: 1.537506
Model None Epoch 26 Batch 50: Loss 1.537505865097046
Model None Epoch 26 Batch 51: Loss 1.4146873950958252
Model None Epoch 26 Batch 52: Loss 1.3824142217636108
Model None Epoch 26 Batch 53: Loss 1.4258359670639038
Model None Epoch 26 Batch 54: Loss 1.4479191303253174
Model None Epoch 26 Batch 55: Loss 1.4187045097351074
Model None Epoch 26 Batch 56: Loss 1.461050271987915
Model None Epoch 26 Batch 57: Loss 1.5207642316818237
Model None Epoch 26 Batch 58: Loss 1.5558573007583618
Model None Epoch 26 Batch 59: Loss 1.4462902545928955
Model None Epoch 26 Batch 60: Loss 1.3879224061965942
Model None Epoch 26 Batch 61: Loss 1.4168245792388916
Model None Epoch 26 Batch 62: Loss 1.3481872081756592
Model None Epoch 26 Batch 63: Loss 1.4059783220291138
Model None Epoch 26 Batch 64: Loss 1.4066708087921143
Model None Epoch 26 Batch 65: Loss 1.313247799873352
Model None Epoch 26 Batch 66: Loss 1.3708966970443726
Model None Epoch 26 Batch 67: Loss 1.4481576681137085
Model None Epoch 26 Batch 68: Loss 1.4533706903457642
Model None Epoch 26 Batch 69: Loss 1.3695852756500244
Model None Epoch 26 Batch 70: Loss 1.440677285194397
Model None Epoch 26 Batch 71: Loss 1.3434596061706543
Model None Epoch 26 Batch 72: Loss 1.5278303623199463
Model None Epoch 26 Batch 73: Loss 1.366328239440918
Model None Epoch 26 Batch 74: Loss 1.4689537286758423
Model None Epoch 26 Batch 75: Loss 1.3985260725021362
Model None Epoch 26 Batch 76: Loss 1.4608794450759888
Model None Epoch 26 Batch 77: Loss 1.306403398513794
Model None Epoch 26 Batch 78: Loss 1.3338371515274048
Model None Epoch 26 Batch 79: Loss 1.3656357526779175
Model None Epoch 26 Batch 80: Loss 1.4412689208984375
Model None Epoch 26 Batch 81: Loss 1.4931482076644897
Model None Epoch 26 Batch 82: Loss 1.4030648469924927
Model None Epoch 26 Batch 83: Loss 1.3475080728530884
Model None Epoch 26 Batch 84: Loss 1.4330415725708008
Model None Epoch 26 Batch 85: Loss 1.3331670761108398
Model None Epoch 26 Batch 86: Loss 1.4884555339813232
Model None Epoch 26 Batch 87: Loss 1.396769642829895
Model None Epoch 26 Batch 88: Loss 1.3951971530914307
Model None Epoch 26 Batch 89: Loss 1.3752870559692383
Model None Epoch 26 Batch 90: Loss 1.484858751296997
Model None Epoch 26 Batch 91: Loss 1.4463398456573486
Model None Epoch 26 Batch 92: Loss 1.4864716529846191
Model None Epoch 26 Batch 93: Loss 1.452890157699585
Model None Epoch 26 Batch 94: Loss 1.409998893737793
Model None Epoch 26 Batch 95: Loss 1.3814281225204468
Model None Epoch 26 Batch 96: Loss 1.4369434118270874
Model None Epoch 26 Batch 97: Loss 1.456618070602417
Model None Epoch 26 Batch 98: Loss 1.4495095014572144
Model None Epoch 26 Batch 99: Loss 1.3143904209136963
Downstream Train Epoch: 26 [25600/50000 (51%)]	Loss: 1.417475
Model None Epoch 26 Batch 100: Loss 1.4174753427505493
Model None Epoch 26 Batch 101: Loss 1.3692086935043335
Model None Epoch 26 Batch 102: Loss 1.451412320137024
Model None Epoch 26 Batch 103: Loss 1.4276560544967651
Model None Epoch 26 Batch 104: Loss 1.4232641458511353
Model None Epoch 26 Batch 105: Loss 1.4653297662734985
Model None Epoch 26 Batch 106: Loss 1.4820382595062256
Model None Epoch 26 Batch 107: Loss 1.4537161588668823
Model None Epoch 26 Batch 108: Loss 1.3383145332336426
Model None Epoch 26 Batch 109: Loss 1.4295532703399658
Model None Epoch 26 Batch 110: Loss 1.427380084991455
Model None Epoch 26 Batch 111: Loss 1.4336788654327393
Model None Epoch 26 Batch 112: Loss 1.4502058029174805
Model None Epoch 26 Batch 113: Loss 1.4950367212295532
Model None Epoch 26 Batch 114: Loss 1.4764517545700073
Model None Epoch 26 Batch 115: Loss 1.542716145515442
Model None Epoch 26 Batch 116: Loss 1.3873602151870728
Model None Epoch 26 Batch 117: Loss 1.3409433364868164
Model None Epoch 26 Batch 118: Loss 1.393403172492981
Model None Epoch 26 Batch 119: Loss 1.4820195436477661
Model None Epoch 26 Batch 120: Loss 1.5116634368896484
Model None Epoch 26 Batch 121: Loss 1.382651448249817
Model None Epoch 26 Batch 122: Loss 1.392494797706604
Model None Epoch 26 Batch 123: Loss 1.5299423933029175
Model None Epoch 26 Batch 124: Loss 1.3201066255569458
Model None Epoch 26 Batch 125: Loss 1.4054408073425293
Model None Epoch 26 Batch 126: Loss 1.451675295829773
Model None Epoch 26 Batch 127: Loss 1.3226029872894287
Model None Epoch 26 Batch 128: Loss 1.3650485277175903
Model None Epoch 26 Batch 129: Loss 1.403458833694458
Model None Epoch 26 Batch 130: Loss 1.4101805686950684
Model None Epoch 26 Batch 131: Loss 1.4129172563552856
Model None Epoch 26 Batch 132: Loss 1.3900010585784912
Model None Epoch 26 Batch 133: Loss 1.4360367059707642
Model None Epoch 26 Batch 134: Loss 1.4172236919403076
Model None Epoch 26 Batch 135: Loss 1.4271212816238403
Model None Epoch 26 Batch 136: Loss 1.4426089525222778
Model None Epoch 26 Batch 137: Loss 1.4845939874649048
Model None Epoch 26 Batch 138: Loss 1.53568434715271
Model None Epoch 26 Batch 139: Loss 1.3540878295898438
Model None Epoch 26 Batch 140: Loss 1.3641260862350464
Model None Epoch 26 Batch 141: Loss 1.3833112716674805
Model None Epoch 26 Batch 142: Loss 1.4110767841339111
Model None Epoch 26 Batch 143: Loss 1.402299404144287
Model None Epoch 26 Batch 144: Loss 1.547789216041565
Model None Epoch 26 Batch 145: Loss 1.4386646747589111
Model None Epoch 26 Batch 146: Loss 1.4751397371292114
Model None Epoch 26 Batch 147: Loss 1.4493944644927979
Model None Epoch 26 Batch 148: Loss 1.3998076915740967
Model None Epoch 26 Batch 149: Loss 1.4947681427001953
Downstream Train Epoch: 26 [38400/50000 (77%)]	Loss: 1.446000
Model None Epoch 26 Batch 150: Loss 1.4460004568099976
Model None Epoch 26 Batch 151: Loss 1.401193618774414
Model None Epoch 26 Batch 152: Loss 1.5086488723754883
Model None Epoch 26 Batch 153: Loss 1.599294900894165
Model None Epoch 26 Batch 154: Loss 1.3306777477264404
Model None Epoch 26 Batch 155: Loss 1.50236976146698
Model None Epoch 26 Batch 156: Loss 1.3882782459259033
Model None Epoch 26 Batch 157: Loss 1.3966361284255981
Model None Epoch 26 Batch 158: Loss 1.4580371379852295
Model None Epoch 26 Batch 159: Loss 1.4402432441711426
Model None Epoch 26 Batch 160: Loss 1.4786227941513062
Model None Epoch 26 Batch 161: Loss 1.4595563411712646
Model None Epoch 26 Batch 162: Loss 1.2957744598388672
Model None Epoch 26 Batch 163: Loss 1.400848388671875
Model None Epoch 26 Batch 164: Loss 1.561071515083313
Model None Epoch 26 Batch 165: Loss 1.4211512804031372
Model None Epoch 26 Batch 166: Loss 1.3408966064453125
Model None Epoch 26 Batch 167: Loss 1.4889357089996338
Model None Epoch 26 Batch 168: Loss 1.5093801021575928
Model None Epoch 26 Batch 169: Loss 1.3605514764785767
Model None Epoch 26 Batch 170: Loss 1.3227384090423584
Model None Epoch 26 Batch 171: Loss 1.428595781326294
Model None Epoch 26 Batch 172: Loss 1.46829354763031
Model None Epoch 26 Batch 173: Loss 1.376974105834961
Model None Epoch 26 Batch 174: Loss 1.4155910015106201
Model None Epoch 26 Batch 175: Loss 1.5689314603805542
Model None Epoch 26 Batch 176: Loss 1.3960086107254028
Model None Epoch 26 Batch 177: Loss 1.2772328853607178
Model None Epoch 26 Batch 178: Loss 1.5403858423233032
Model None Epoch 26 Batch 179: Loss 1.3204857110977173
Model None Epoch 26 Batch 180: Loss 1.3325999975204468
Model None Epoch 26 Batch 181: Loss 1.456189513206482
Model None Epoch 26 Batch 182: Loss 1.4431496858596802
Model None Epoch 26 Batch 183: Loss 1.4237358570098877
Model None Epoch 26 Batch 184: Loss 1.4563273191452026
Model None Epoch 26 Batch 185: Loss 1.5102277994155884
Model None Epoch 26 Batch 186: Loss 1.4258257150650024
Model None Epoch 26 Batch 187: Loss 1.3518604040145874
Model None Epoch 26 Batch 188: Loss 1.3782259225845337
Model None Epoch 26 Batch 189: Loss 1.552204966545105
Model None Epoch 26 Batch 190: Loss 1.4540897607803345
Model None Epoch 26 Batch 191: Loss 1.3501341342926025
Model None Epoch 26 Batch 192: Loss 1.3977031707763672
Model None Epoch 26 Batch 193: Loss 1.39846932888031
Model None Epoch 26 Batch 194: Loss 1.3842378854751587
Model None Epoch 26 Batch 195: Loss 1.4746043682098389

 Downstream Train loss: 1.4257124747548784 Acc: 0.5614
Downstream Train Epoch: 27 [0/50000 (0%)]	Loss: 1.418886
Model None Epoch 27 Batch 0: Loss 1.4188857078552246
Model None Epoch 27 Batch 1: Loss 1.396842360496521
Model None Epoch 27 Batch 2: Loss 1.3850386142730713
Model None Epoch 27 Batch 3: Loss 1.4312292337417603
Model None Epoch 27 Batch 4: Loss 1.3253644704818726
Model None Epoch 27 Batch 5: Loss 1.390228509902954
Model None Epoch 27 Batch 6: Loss 1.4044489860534668
Model None Epoch 27 Batch 7: Loss 1.4744157791137695
Model None Epoch 27 Batch 8: Loss 1.3916094303131104
Model None Epoch 27 Batch 9: Loss 1.4089516401290894
Model None Epoch 27 Batch 10: Loss 1.3421282768249512
Model None Epoch 27 Batch 11: Loss 1.2658225297927856
Model None Epoch 27 Batch 12: Loss 1.4487624168395996
Model None Epoch 27 Batch 13: Loss 1.3382060527801514
Model None Epoch 27 Batch 14: Loss 1.3778955936431885
Model None Epoch 27 Batch 15: Loss 1.482500433921814
Model None Epoch 27 Batch 16: Loss 1.3180198669433594
Model None Epoch 27 Batch 17: Loss 1.3754159212112427
Model None Epoch 27 Batch 18: Loss 1.3850505352020264
Model None Epoch 27 Batch 19: Loss 1.3802250623703003
Model None Epoch 27 Batch 20: Loss 1.4360567331314087
Model None Epoch 27 Batch 21: Loss 1.4229851961135864
Model None Epoch 27 Batch 22: Loss 1.36896812915802
Model None Epoch 27 Batch 23: Loss 1.4344285726547241
Model None Epoch 27 Batch 24: Loss 1.3973559141159058
Model None Epoch 27 Batch 25: Loss 1.4222677946090698
Model None Epoch 27 Batch 26: Loss 1.4031453132629395
Model None Epoch 27 Batch 27: Loss 1.383437991142273
Model None Epoch 27 Batch 28: Loss 1.412590742111206
Model None Epoch 27 Batch 29: Loss 1.3884449005126953
Model None Epoch 27 Batch 30: Loss 1.4915345907211304
Model None Epoch 27 Batch 31: Loss 1.3880988359451294
Model None Epoch 27 Batch 32: Loss 1.5193185806274414
Model None Epoch 27 Batch 33: Loss 1.4318779706954956
Model None Epoch 27 Batch 34: Loss 1.4365955591201782
Model None Epoch 27 Batch 35: Loss 1.5284113883972168
Model None Epoch 27 Batch 36: Loss 1.3959159851074219
Model None Epoch 27 Batch 37: Loss 1.382704496383667
Model None Epoch 27 Batch 38: Loss 1.489250898361206
Model None Epoch 27 Batch 39: Loss 1.4078116416931152
Model None Epoch 27 Batch 40: Loss 1.361269474029541
Model None Epoch 27 Batch 41: Loss 1.3152679204940796
Model None Epoch 27 Batch 42: Loss 1.4004437923431396
Model None Epoch 27 Batch 43: Loss 1.5610238313674927
Model None Epoch 27 Batch 44: Loss 1.4595458507537842
Model None Epoch 27 Batch 45: Loss 1.3867716789245605
Model None Epoch 27 Batch 46: Loss 1.3849637508392334
Model None Epoch 27 Batch 47: Loss 1.466676950454712
Model None Epoch 27 Batch 48: Loss 1.6215542554855347
Model None Epoch 27 Batch 49: Loss 1.4497530460357666
Downstream Train Epoch: 27 [12800/50000 (26%)]	Loss: 1.390024
Model None Epoch 27 Batch 50: Loss 1.3900235891342163
Model None Epoch 27 Batch 51: Loss 1.507737636566162
Model None Epoch 27 Batch 52: Loss 1.426802158355713
Model None Epoch 27 Batch 53: Loss 1.4650142192840576
Model None Epoch 27 Batch 54: Loss 1.4254062175750732
Model None Epoch 27 Batch 55: Loss 1.503976821899414
Model None Epoch 27 Batch 56: Loss 1.529349684715271
Model None Epoch 27 Batch 57: Loss 1.4734803438186646
Model None Epoch 27 Batch 58: Loss 1.3564116954803467
Model None Epoch 27 Batch 59: Loss 1.5097800493240356
Model None Epoch 27 Batch 60: Loss 1.4578735828399658
Model None Epoch 27 Batch 61: Loss 1.4631636142730713
Model None Epoch 27 Batch 62: Loss 1.5042674541473389
Model None Epoch 27 Batch 63: Loss 1.395011305809021
Model None Epoch 27 Batch 64: Loss 1.3349896669387817
Model None Epoch 27 Batch 65: Loss 1.4266753196716309
Model None Epoch 27 Batch 66: Loss 1.3951256275177002
Model None Epoch 27 Batch 67: Loss 1.4117324352264404
Model None Epoch 27 Batch 68: Loss 1.383132815361023
Model None Epoch 27 Batch 69: Loss 1.4982417821884155
Model None Epoch 27 Batch 70: Loss 1.3285243511199951
Model None Epoch 27 Batch 71: Loss 1.5150599479675293
Model None Epoch 27 Batch 72: Loss 1.4638736248016357
Model None Epoch 27 Batch 73: Loss 1.4061297178268433
Model None Epoch 27 Batch 74: Loss 1.3766233921051025
Model None Epoch 27 Batch 75: Loss 1.371911883354187
Model None Epoch 27 Batch 76: Loss 1.4578005075454712
Model None Epoch 27 Batch 77: Loss 1.4320613145828247
Model None Epoch 27 Batch 78: Loss 1.4588375091552734
Model None Epoch 27 Batch 79: Loss 1.443548321723938
Model None Epoch 27 Batch 80: Loss 1.5075379610061646
Model None Epoch 27 Batch 81: Loss 1.402200698852539
Model None Epoch 27 Batch 82: Loss 1.4873508214950562
Model None Epoch 27 Batch 83: Loss 1.4860060214996338
Model None Epoch 27 Batch 84: Loss 1.4155346155166626
Model None Epoch 27 Batch 85: Loss 1.317252516746521
Model None Epoch 27 Batch 86: Loss 1.4755194187164307
Model None Epoch 27 Batch 87: Loss 1.3498663902282715
Model None Epoch 27 Batch 88: Loss 1.31282377243042
Model None Epoch 27 Batch 89: Loss 1.3657732009887695
Model None Epoch 27 Batch 90: Loss 1.48121976852417
Model None Epoch 27 Batch 91: Loss 1.432592511177063
Model None Epoch 27 Batch 92: Loss 1.40244722366333
Model None Epoch 27 Batch 93: Loss 1.3364278078079224
Model None Epoch 27 Batch 94: Loss 1.4298895597457886
Model None Epoch 27 Batch 95: Loss 1.3605161905288696
Model None Epoch 27 Batch 96: Loss 1.5148155689239502
Model None Epoch 27 Batch 97: Loss 1.4913228750228882
Model None Epoch 27 Batch 98: Loss 1.3489619493484497
Model None Epoch 27 Batch 99: Loss 1.4098623991012573
Downstream Train Epoch: 27 [25600/50000 (51%)]	Loss: 1.469404
Model None Epoch 27 Batch 100: Loss 1.4694042205810547
Model None Epoch 27 Batch 101: Loss 1.4497705698013306
Model None Epoch 27 Batch 102: Loss 1.3836770057678223
Model None Epoch 27 Batch 103: Loss 1.4979064464569092
Model None Epoch 27 Batch 104: Loss 1.4574378728866577
Model None Epoch 27 Batch 105: Loss 1.5354374647140503
Model None Epoch 27 Batch 106: Loss 1.4836273193359375
Model None Epoch 27 Batch 107: Loss 1.4408574104309082
Model None Epoch 27 Batch 108: Loss 1.3899383544921875
Model None Epoch 27 Batch 109: Loss 1.369916558265686
Model None Epoch 27 Batch 110: Loss 1.3294095993041992
Model None Epoch 27 Batch 111: Loss 1.577131748199463
Model None Epoch 27 Batch 112: Loss 1.4556196928024292
Model None Epoch 27 Batch 113: Loss 1.3748291730880737
Model None Epoch 27 Batch 114: Loss 1.3556568622589111
Model None Epoch 27 Batch 115: Loss 1.4342296123504639
Model None Epoch 27 Batch 116: Loss 1.357843279838562
Model None Epoch 27 Batch 117: Loss 1.2845752239227295
Model None Epoch 27 Batch 118: Loss 1.3907160758972168
Model None Epoch 27 Batch 119: Loss 1.4939894676208496
Model None Epoch 27 Batch 120: Loss 1.430139422416687
Model None Epoch 27 Batch 121: Loss 1.3531453609466553
Model None Epoch 27 Batch 122: Loss 1.4220284223556519
Model None Epoch 27 Batch 123: Loss 1.5285730361938477
Model None Epoch 27 Batch 124: Loss 1.4430723190307617
Model None Epoch 27 Batch 125: Loss 1.527590274810791
Model None Epoch 27 Batch 126: Loss 1.3893388509750366
Model None Epoch 27 Batch 127: Loss 1.3681877851486206
Model None Epoch 27 Batch 128: Loss 1.51619553565979
Model None Epoch 27 Batch 129: Loss 1.3568756580352783
Model None Epoch 27 Batch 130: Loss 1.4725754261016846
Model None Epoch 27 Batch 131: Loss 1.383046269416809
Model None Epoch 27 Batch 132: Loss 1.372741460800171
Model None Epoch 27 Batch 133: Loss 1.486594796180725
Model None Epoch 27 Batch 134: Loss 1.4768327474594116
Model None Epoch 27 Batch 135: Loss 1.3976247310638428
Model None Epoch 27 Batch 136: Loss 1.3937832117080688
Model None Epoch 27 Batch 137: Loss 1.4002739191055298
Model None Epoch 27 Batch 138: Loss 1.5190742015838623
Model None Epoch 27 Batch 139: Loss 1.443860650062561
Model None Epoch 27 Batch 140: Loss 1.4543687105178833
Model None Epoch 27 Batch 141: Loss 1.4383984804153442
Model None Epoch 27 Batch 142: Loss 1.4028993844985962
Model None Epoch 27 Batch 143: Loss 1.3975272178649902
Model None Epoch 27 Batch 144: Loss 1.3993263244628906
Model None Epoch 27 Batch 145: Loss 1.4217840433120728
Model None Epoch 27 Batch 146: Loss 1.3887385129928589
Model None Epoch 27 Batch 147: Loss 1.4667880535125732
Model None Epoch 27 Batch 148: Loss 1.4579867124557495
Model None Epoch 27 Batch 149: Loss 1.3602306842803955
Downstream Train Epoch: 27 [38400/50000 (77%)]	Loss: 1.483750
Model None Epoch 27 Batch 150: Loss 1.4837502241134644
Model None Epoch 27 Batch 151: Loss 1.3340998888015747
Model None Epoch 27 Batch 152: Loss 1.3934309482574463
Model None Epoch 27 Batch 153: Loss 1.4073835611343384
Model None Epoch 27 Batch 154: Loss 1.2828291654586792
Model None Epoch 27 Batch 155: Loss 1.5441093444824219
Model None Epoch 27 Batch 156: Loss 1.4941664934158325
Model None Epoch 27 Batch 157: Loss 1.355857491493225
Model None Epoch 27 Batch 158: Loss 1.427443265914917
Model None Epoch 27 Batch 159: Loss 1.4603421688079834
Model None Epoch 27 Batch 160: Loss 1.4733049869537354
Model None Epoch 27 Batch 161: Loss 1.3203610181808472
Model None Epoch 27 Batch 162: Loss 1.4497008323669434
Model None Epoch 27 Batch 163: Loss 1.4135102033615112
Model None Epoch 27 Batch 164: Loss 1.428527593612671
Model None Epoch 27 Batch 165: Loss 1.5946189165115356
Model None Epoch 27 Batch 166: Loss 1.409835934638977
Model None Epoch 27 Batch 167: Loss 1.4609835147857666
Model None Epoch 27 Batch 168: Loss 1.5121519565582275
Model None Epoch 27 Batch 169: Loss 1.3335078954696655
Model None Epoch 27 Batch 170: Loss 1.3555638790130615
Model None Epoch 27 Batch 171: Loss 1.4636166095733643
Model None Epoch 27 Batch 172: Loss 1.5194684267044067
Model None Epoch 27 Batch 173: Loss 1.495633602142334
Model None Epoch 27 Batch 174: Loss 1.3972967863082886
Model None Epoch 27 Batch 175: Loss 1.3762561082839966
Model None Epoch 27 Batch 176: Loss 1.4289288520812988
Model None Epoch 27 Batch 177: Loss 1.3677184581756592
Model None Epoch 27 Batch 178: Loss 1.43696129322052
Model None Epoch 27 Batch 179: Loss 1.4879873991012573
Model None Epoch 27 Batch 180: Loss 1.413450002670288
Model None Epoch 27 Batch 181: Loss 1.5121731758117676
Model None Epoch 27 Batch 182: Loss 1.4185795783996582
Model None Epoch 27 Batch 183: Loss 1.4740937948226929
Model None Epoch 27 Batch 184: Loss 1.4741427898406982
Model None Epoch 27 Batch 185: Loss 1.4734777212142944
Model None Epoch 27 Batch 186: Loss 1.306513786315918
Model None Epoch 27 Batch 187: Loss 1.5015357732772827
Model None Epoch 27 Batch 188: Loss 1.4919884204864502
Model None Epoch 27 Batch 189: Loss 1.5253089666366577
Model None Epoch 27 Batch 190: Loss 1.390845775604248
Model None Epoch 27 Batch 191: Loss 1.5073636770248413
Model None Epoch 27 Batch 192: Loss 1.3772788047790527
Model None Epoch 27 Batch 193: Loss 1.2949268817901611
Model None Epoch 27 Batch 194: Loss 1.3825528621673584
Model None Epoch 27 Batch 195: Loss 1.077380657196045

 Downstream Train loss: 1.423788452635006 Acc: 0.5614
Downstream Train Epoch: 28 [0/50000 (0%)]	Loss: 1.349602
Model None Epoch 28 Batch 0: Loss 1.3496018648147583
Model None Epoch 28 Batch 1: Loss 1.4196583032608032
Model None Epoch 28 Batch 2: Loss 1.3533947467803955
Model None Epoch 28 Batch 3: Loss 1.4788696765899658
Model None Epoch 28 Batch 4: Loss 1.37750244140625
Model None Epoch 28 Batch 5: Loss 1.3663392066955566
Model None Epoch 28 Batch 6: Loss 1.4058760404586792
Model None Epoch 28 Batch 7: Loss 1.4147323369979858
Model None Epoch 28 Batch 8: Loss 1.3831465244293213
Model None Epoch 28 Batch 9: Loss 1.2428971529006958
Model None Epoch 28 Batch 10: Loss 1.317940592765808
Model None Epoch 28 Batch 11: Loss 1.345328688621521
Model None Epoch 28 Batch 12: Loss 1.456071376800537
Model None Epoch 28 Batch 13: Loss 1.5049834251403809
Model None Epoch 28 Batch 14: Loss 1.3769078254699707
Model None Epoch 28 Batch 15: Loss 1.2891418933868408
Model None Epoch 28 Batch 16: Loss 1.3517162799835205
Model None Epoch 28 Batch 17: Loss 1.4142810106277466
Model None Epoch 28 Batch 18: Loss 1.5379389524459839
Model None Epoch 28 Batch 19: Loss 1.5368132591247559
Model None Epoch 28 Batch 20: Loss 1.48075270652771
Model None Epoch 28 Batch 21: Loss 1.3293825387954712
Model None Epoch 28 Batch 22: Loss 1.46055006980896
Model None Epoch 28 Batch 23: Loss 1.445416808128357
Model None Epoch 28 Batch 24: Loss 1.3429700136184692
Model None Epoch 28 Batch 25: Loss 1.3976092338562012
Model None Epoch 28 Batch 26: Loss 1.533943772315979
Model None Epoch 28 Batch 27: Loss 1.3849000930786133
Model None Epoch 28 Batch 28: Loss 1.407494306564331
Model None Epoch 28 Batch 29: Loss 1.3663477897644043
Model None Epoch 28 Batch 30: Loss 1.5354926586151123
Model None Epoch 28 Batch 31: Loss 1.3760449886322021
Model None Epoch 28 Batch 32: Loss 1.3984614610671997
Model None Epoch 28 Batch 33: Loss 1.3504666090011597
Model None Epoch 28 Batch 34: Loss 1.5241568088531494
Model None Epoch 28 Batch 35: Loss 1.4204230308532715
Model None Epoch 28 Batch 36: Loss 1.4542428255081177
Model None Epoch 28 Batch 37: Loss 1.4822102785110474
Model None Epoch 28 Batch 38: Loss 1.4048609733581543
Model None Epoch 28 Batch 39: Loss 1.499923586845398
Model None Epoch 28 Batch 40: Loss 1.4184650182724
Model None Epoch 28 Batch 41: Loss 1.4730325937271118
Model None Epoch 28 Batch 42: Loss 1.3981636762619019
Model None Epoch 28 Batch 43: Loss 1.4044091701507568
Model None Epoch 28 Batch 44: Loss 1.4312881231307983
Model None Epoch 28 Batch 45: Loss 1.3891549110412598
Model None Epoch 28 Batch 46: Loss 1.4223641157150269
Model None Epoch 28 Batch 47: Loss 1.432523250579834
Model None Epoch 28 Batch 48: Loss 1.4794749021530151
Model None Epoch 28 Batch 49: Loss 1.3975683450698853
Downstream Train Epoch: 28 [12800/50000 (26%)]	Loss: 1.317332
Model None Epoch 28 Batch 50: Loss 1.3173316717147827
Model None Epoch 28 Batch 51: Loss 1.4434820413589478
Model None Epoch 28 Batch 52: Loss 1.302865982055664
Model None Epoch 28 Batch 53: Loss 1.4587156772613525
Model None Epoch 28 Batch 54: Loss 1.468477487564087
Model None Epoch 28 Batch 55: Loss 1.3987274169921875
Model None Epoch 28 Batch 56: Loss 1.4117499589920044
Model None Epoch 28 Batch 57: Loss 1.4336743354797363
Model None Epoch 28 Batch 58: Loss 1.4090691804885864
Model None Epoch 28 Batch 59: Loss 1.4458987712860107
Model None Epoch 28 Batch 60: Loss 1.316632628440857
Model None Epoch 28 Batch 61: Loss 1.3933087587356567
Model None Epoch 28 Batch 62: Loss 1.382575511932373
Model None Epoch 28 Batch 63: Loss 1.3965600728988647
Model None Epoch 28 Batch 64: Loss 1.2992547750473022
Model None Epoch 28 Batch 65: Loss 1.416763424873352
Model None Epoch 28 Batch 66: Loss 1.4560133218765259
Model None Epoch 28 Batch 67: Loss 1.507955551147461
Model None Epoch 28 Batch 68: Loss 1.4604002237319946
Model None Epoch 28 Batch 69: Loss 1.4963726997375488
Model None Epoch 28 Batch 70: Loss 1.4406198263168335
Model None Epoch 28 Batch 71: Loss 1.2929015159606934
Model None Epoch 28 Batch 72: Loss 1.4004484415054321
Model None Epoch 28 Batch 73: Loss 1.4713237285614014
Model None Epoch 28 Batch 74: Loss 1.5132741928100586
Model None Epoch 28 Batch 75: Loss 1.4936636686325073
Model None Epoch 28 Batch 76: Loss 1.464267373085022
Model None Epoch 28 Batch 77: Loss 1.4740686416625977
Model None Epoch 28 Batch 78: Loss 1.3778886795043945
Model None Epoch 28 Batch 79: Loss 1.5010017156600952
Model None Epoch 28 Batch 80: Loss 1.499273657798767
Model None Epoch 28 Batch 81: Loss 1.310605764389038
Model None Epoch 28 Batch 82: Loss 1.4360806941986084
Model None Epoch 28 Batch 83: Loss 1.3939722776412964
Model None Epoch 28 Batch 84: Loss 1.4449818134307861
Model None Epoch 28 Batch 85: Loss 1.4922575950622559
Model None Epoch 28 Batch 86: Loss 1.3925045728683472
Model None Epoch 28 Batch 87: Loss 1.3467328548431396
Model None Epoch 28 Batch 88: Loss 1.3770325183868408
Model None Epoch 28 Batch 89: Loss 1.4268649816513062
Model None Epoch 28 Batch 90: Loss 1.4885878562927246
Model None Epoch 28 Batch 91: Loss 1.4567095041275024
Model None Epoch 28 Batch 92: Loss 1.4494819641113281
Model None Epoch 28 Batch 93: Loss 1.2994049787521362
Model None Epoch 28 Batch 94: Loss 1.5141313076019287
Model None Epoch 28 Batch 95: Loss 1.3997037410736084
Model None Epoch 28 Batch 96: Loss 1.4368442296981812
Model None Epoch 28 Batch 97: Loss 1.372433066368103
Model None Epoch 28 Batch 98: Loss 1.361005187034607
Model None Epoch 28 Batch 99: Loss 1.4092037677764893
Downstream Train Epoch: 28 [25600/50000 (51%)]	Loss: 1.442106
Model None Epoch 28 Batch 100: Loss 1.4421064853668213
Model None Epoch 28 Batch 101: Loss 1.4165067672729492
Model None Epoch 28 Batch 102: Loss 1.2727535963058472
Model None Epoch 28 Batch 103: Loss 1.4321743249893188
Model None Epoch 28 Batch 104: Loss 1.3367042541503906
Model None Epoch 28 Batch 105: Loss 1.439743995666504
Model None Epoch 28 Batch 106: Loss 1.5478157997131348
Model None Epoch 28 Batch 107: Loss 1.4702569246292114
Model None Epoch 28 Batch 108: Loss 1.306700348854065
Model None Epoch 28 Batch 109: Loss 1.4050538539886475
Model None Epoch 28 Batch 110: Loss 1.4215065240859985
Model None Epoch 28 Batch 111: Loss 1.460191249847412
Model None Epoch 28 Batch 112: Loss 1.6074063777923584
Model None Epoch 28 Batch 113: Loss 1.351624846458435
Model None Epoch 28 Batch 114: Loss 1.4709309339523315
Model None Epoch 28 Batch 115: Loss 1.382240891456604
Model None Epoch 28 Batch 116: Loss 1.435664176940918
Model None Epoch 28 Batch 117: Loss 1.4309109449386597
Model None Epoch 28 Batch 118: Loss 1.4597959518432617
Model None Epoch 28 Batch 119: Loss 1.4405403137207031
Model None Epoch 28 Batch 120: Loss 1.479134202003479
Model None Epoch 28 Batch 121: Loss 1.3650881052017212
Model None Epoch 28 Batch 122: Loss 1.3860535621643066
Model None Epoch 28 Batch 123: Loss 1.4682984352111816
Model None Epoch 28 Batch 124: Loss 1.4086238145828247
Model None Epoch 28 Batch 125: Loss 1.3839908838272095
Model None Epoch 28 Batch 126: Loss 1.4605727195739746
Model None Epoch 28 Batch 127: Loss 1.4233121871948242
Model None Epoch 28 Batch 128: Loss 1.3371444940567017
Model None Epoch 28 Batch 129: Loss 1.4669075012207031
Model None Epoch 28 Batch 130: Loss 1.4154084920883179
Model None Epoch 28 Batch 131: Loss 1.3769116401672363
Model None Epoch 28 Batch 132: Loss 1.3806527853012085
Model None Epoch 28 Batch 133: Loss 1.411144733428955
Model None Epoch 28 Batch 134: Loss 1.3331928253173828
Model None Epoch 28 Batch 135: Loss 1.5019840002059937
Model None Epoch 28 Batch 136: Loss 1.4825260639190674
Model None Epoch 28 Batch 137: Loss 1.4226211309432983
Model None Epoch 28 Batch 138: Loss 1.3918209075927734
Model None Epoch 28 Batch 139: Loss 1.4441709518432617
Model None Epoch 28 Batch 140: Loss 1.3710386753082275
Model None Epoch 28 Batch 141: Loss 1.3899104595184326
Model None Epoch 28 Batch 142: Loss 1.4936467409133911
Model None Epoch 28 Batch 143: Loss 1.4410678148269653
Model None Epoch 28 Batch 144: Loss 1.4444639682769775
Model None Epoch 28 Batch 145: Loss 1.4770113229751587
Model None Epoch 28 Batch 146: Loss 1.4660813808441162
Model None Epoch 28 Batch 147: Loss 1.3798116445541382
Model None Epoch 28 Batch 148: Loss 1.4327375888824463
Model None Epoch 28 Batch 149: Loss 1.3818628787994385
Downstream Train Epoch: 28 [38400/50000 (77%)]	Loss: 1.377128
Model None Epoch 28 Batch 150: Loss 1.37712824344635
Model None Epoch 28 Batch 151: Loss 1.352321982383728
Model None Epoch 28 Batch 152: Loss 1.3548911809921265
Model None Epoch 28 Batch 153: Loss 1.3983465433120728
Model None Epoch 28 Batch 154: Loss 1.3663098812103271
Model None Epoch 28 Batch 155: Loss 1.5089092254638672
Model None Epoch 28 Batch 156: Loss 1.5254462957382202
Model None Epoch 28 Batch 157: Loss 1.4161818027496338
Model None Epoch 28 Batch 158: Loss 1.4848546981811523
Model None Epoch 28 Batch 159: Loss 1.4161376953125
Model None Epoch 28 Batch 160: Loss 1.3757832050323486
Model None Epoch 28 Batch 161: Loss 1.3817169666290283
Model None Epoch 28 Batch 162: Loss 1.3366283178329468
Model None Epoch 28 Batch 163: Loss 1.356958031654358
Model None Epoch 28 Batch 164: Loss 1.3225243091583252
Model None Epoch 28 Batch 165: Loss 1.3744232654571533
Model None Epoch 28 Batch 166: Loss 1.3945401906967163
Model None Epoch 28 Batch 167: Loss 1.3963381052017212
Model None Epoch 28 Batch 168: Loss 1.3798433542251587
Model None Epoch 28 Batch 169: Loss 1.48267662525177
Model None Epoch 28 Batch 170: Loss 1.3087079524993896
Model None Epoch 28 Batch 171: Loss 1.2669717073440552
Model None Epoch 28 Batch 172: Loss 1.3762954473495483
Model None Epoch 28 Batch 173: Loss 1.4714096784591675
Model None Epoch 28 Batch 174: Loss 1.3544842004776
Model None Epoch 28 Batch 175: Loss 1.4618134498596191
Model None Epoch 28 Batch 176: Loss 1.4487508535385132
Model None Epoch 28 Batch 177: Loss 1.3202580213546753
Model None Epoch 28 Batch 178: Loss 1.493723750114441
Model None Epoch 28 Batch 179: Loss 1.4655523300170898
Model None Epoch 28 Batch 180: Loss 1.4881232976913452
Model None Epoch 28 Batch 181: Loss 1.3323431015014648
Model None Epoch 28 Batch 182: Loss 1.4274678230285645
Model None Epoch 28 Batch 183: Loss 1.3212300539016724
Model None Epoch 28 Batch 184: Loss 1.4460419416427612
Model None Epoch 28 Batch 185: Loss 1.404167652130127
Model None Epoch 28 Batch 186: Loss 1.463734745979309
Model None Epoch 28 Batch 187: Loss 1.5000536441802979
Model None Epoch 28 Batch 188: Loss 1.4190998077392578
Model None Epoch 28 Batch 189: Loss 1.622273564338684
Model None Epoch 28 Batch 190: Loss 1.3575890064239502
Model None Epoch 28 Batch 191: Loss 1.4493460655212402
Model None Epoch 28 Batch 192: Loss 1.3587912321090698
Model None Epoch 28 Batch 193: Loss 1.4117761850357056
Model None Epoch 28 Batch 194: Loss 1.415474534034729
Model None Epoch 28 Batch 195: Loss 1.3812777996063232

 Downstream Train loss: 1.4165044597217016 Acc: 0.5614
Downstream Train Epoch: 29 [0/50000 (0%)]	Loss: 1.457805
Model None Epoch 29 Batch 0: Loss 1.457804799079895
Model None Epoch 29 Batch 1: Loss 1.3855376243591309
Model None Epoch 29 Batch 2: Loss 1.4760890007019043
Model None Epoch 29 Batch 3: Loss 1.398216962814331
Model None Epoch 29 Batch 4: Loss 1.3524806499481201
Model None Epoch 29 Batch 5: Loss 1.4355944395065308
Model None Epoch 29 Batch 6: Loss 1.3226255178451538
Model None Epoch 29 Batch 7: Loss 1.3965750932693481
Model None Epoch 29 Batch 8: Loss 1.4889459609985352
Model None Epoch 29 Batch 9: Loss 1.4186683893203735
Model None Epoch 29 Batch 10: Loss 1.424529790878296
Model None Epoch 29 Batch 11: Loss 1.3171284198760986
Model None Epoch 29 Batch 12: Loss 1.3375065326690674
Model None Epoch 29 Batch 13: Loss 1.4611868858337402
Model None Epoch 29 Batch 14: Loss 1.449221134185791
Model None Epoch 29 Batch 15: Loss 1.4345451593399048
Model None Epoch 29 Batch 16: Loss 1.46720290184021
Model None Epoch 29 Batch 17: Loss 1.4666661024093628
Model None Epoch 29 Batch 18: Loss 1.4877893924713135
Model None Epoch 29 Batch 19: Loss 1.3597369194030762
Model None Epoch 29 Batch 20: Loss 1.3818517923355103
Model None Epoch 29 Batch 21: Loss 1.4380574226379395
Model None Epoch 29 Batch 22: Loss 1.5363458395004272
Model None Epoch 29 Batch 23: Loss 1.3875313997268677
Model None Epoch 29 Batch 24: Loss 1.5799105167388916
Model None Epoch 29 Batch 25: Loss 1.438002347946167
Model None Epoch 29 Batch 26: Loss 1.4921257495880127
Model None Epoch 29 Batch 27: Loss 1.3312807083129883
Model None Epoch 29 Batch 28: Loss 1.4768868684768677
Model None Epoch 29 Batch 29: Loss 1.313963532447815
Model None Epoch 29 Batch 30: Loss 1.4160268306732178
Model None Epoch 29 Batch 31: Loss 1.4677174091339111
Model None Epoch 29 Batch 32: Loss 1.4390840530395508
Model None Epoch 29 Batch 33: Loss 1.403576135635376
Model None Epoch 29 Batch 34: Loss 1.2542667388916016
Model None Epoch 29 Batch 35: Loss 1.395430088043213
Model None Epoch 29 Batch 36: Loss 1.4894636869430542
Model None Epoch 29 Batch 37: Loss 1.3885053396224976
Model None Epoch 29 Batch 38: Loss 1.4738876819610596
Model None Epoch 29 Batch 39: Loss 1.3929113149642944
Model None Epoch 29 Batch 40: Loss 1.5102293491363525
Model None Epoch 29 Batch 41: Loss 1.4605822563171387
Model None Epoch 29 Batch 42: Loss 1.4206331968307495
Model None Epoch 29 Batch 43: Loss 1.5031300783157349
Model None Epoch 29 Batch 44: Loss 1.362393856048584
Model None Epoch 29 Batch 45: Loss 1.39853835105896
Model None Epoch 29 Batch 46: Loss 1.3614739179611206
Model None Epoch 29 Batch 47: Loss 1.403589129447937
Model None Epoch 29 Batch 48: Loss 1.4066808223724365
Model None Epoch 29 Batch 49: Loss 1.4481645822525024
Downstream Train Epoch: 29 [12800/50000 (26%)]	Loss: 1.469161
Model None Epoch 29 Batch 50: Loss 1.4691613912582397
Model None Epoch 29 Batch 51: Loss 1.5218870639801025
Model None Epoch 29 Batch 52: Loss 1.4472060203552246
Model None Epoch 29 Batch 53: Loss 1.4972532987594604
Model None Epoch 29 Batch 54: Loss 1.4489197731018066
Model None Epoch 29 Batch 55: Loss 1.5131453275680542
Model None Epoch 29 Batch 56: Loss 1.4375120401382446
Model None Epoch 29 Batch 57: Loss 1.450778841972351
Model None Epoch 29 Batch 58: Loss 1.3296594619750977
Model None Epoch 29 Batch 59: Loss 1.4367587566375732
Model None Epoch 29 Batch 60: Loss 1.493606448173523
Model None Epoch 29 Batch 61: Loss 1.4244418144226074
Model None Epoch 29 Batch 62: Loss 1.3822568655014038
Model None Epoch 29 Batch 63: Loss 1.477095603942871
Model None Epoch 29 Batch 64: Loss 1.2090016603469849
Model None Epoch 29 Batch 65: Loss 1.4902400970458984
Model None Epoch 29 Batch 66: Loss 1.4453582763671875
Model None Epoch 29 Batch 67: Loss 1.493951439857483
Model None Epoch 29 Batch 68: Loss 1.4373172521591187
Model None Epoch 29 Batch 69: Loss 1.4507296085357666
Model None Epoch 29 Batch 70: Loss 1.50593101978302
Model None Epoch 29 Batch 71: Loss 1.3994990587234497
Model None Epoch 29 Batch 72: Loss 1.4332644939422607
Model None Epoch 29 Batch 73: Loss 1.4094936847686768
Model None Epoch 29 Batch 74: Loss 1.3808565139770508
Model None Epoch 29 Batch 75: Loss 1.4326856136322021
Model None Epoch 29 Batch 76: Loss 1.3935372829437256
Model None Epoch 29 Batch 77: Loss 1.487764835357666
Model None Epoch 29 Batch 78: Loss 1.3680191040039062
Model None Epoch 29 Batch 79: Loss 1.371920108795166
Model None Epoch 29 Batch 80: Loss 1.3728429079055786
Model None Epoch 29 Batch 81: Loss 1.409523606300354
Model None Epoch 29 Batch 82: Loss 1.4208955764770508
Model None Epoch 29 Batch 83: Loss 1.378727674484253
Model None Epoch 29 Batch 84: Loss 1.3891264200210571
Model None Epoch 29 Batch 85: Loss 1.4406381845474243
Model None Epoch 29 Batch 86: Loss 1.4388916492462158
Model None Epoch 29 Batch 87: Loss 1.5677448511123657
Model None Epoch 29 Batch 88: Loss 1.3702332973480225
Model None Epoch 29 Batch 89: Loss 1.40537691116333
Model None Epoch 29 Batch 90: Loss 1.477927803993225
Model None Epoch 29 Batch 91: Loss 1.2845830917358398
Model None Epoch 29 Batch 92: Loss 1.3323894739151
Model None Epoch 29 Batch 93: Loss 1.3968348503112793
Model None Epoch 29 Batch 94: Loss 1.4791666269302368
Model None Epoch 29 Batch 95: Loss 1.40045166015625
Model None Epoch 29 Batch 96: Loss 1.4162273406982422
Model None Epoch 29 Batch 97: Loss 1.468421220779419
Model None Epoch 29 Batch 98: Loss 1.393916368484497
Model None Epoch 29 Batch 99: Loss 1.3776047229766846
Downstream Train Epoch: 29 [25600/50000 (51%)]	Loss: 1.406877
Model None Epoch 29 Batch 100: Loss 1.4068766832351685
Model None Epoch 29 Batch 101: Loss 1.3926175832748413
Model None Epoch 29 Batch 102: Loss 1.283138632774353
Model None Epoch 29 Batch 103: Loss 1.4369922876358032
Model None Epoch 29 Batch 104: Loss 1.5264973640441895
Model None Epoch 29 Batch 105: Loss 1.4260516166687012
Model None Epoch 29 Batch 106: Loss 1.403252124786377
Model None Epoch 29 Batch 107: Loss 1.32803213596344
Model None Epoch 29 Batch 108: Loss 1.3245048522949219
Model None Epoch 29 Batch 109: Loss 1.3915401697158813
Model None Epoch 29 Batch 110: Loss 1.386168360710144
Model None Epoch 29 Batch 111: Loss 1.4132823944091797
Model None Epoch 29 Batch 112: Loss 1.5026698112487793
Model None Epoch 29 Batch 113: Loss 1.501227855682373
Model None Epoch 29 Batch 114: Loss 1.5210940837860107
Model None Epoch 29 Batch 115: Loss 1.5430539846420288
Model None Epoch 29 Batch 116: Loss 1.3573118448257446
Model None Epoch 29 Batch 117: Loss 1.369271993637085
Model None Epoch 29 Batch 118: Loss 1.4345557689666748
Model None Epoch 29 Batch 119: Loss 1.431923747062683
Model None Epoch 29 Batch 120: Loss 1.4119473695755005
Model None Epoch 29 Batch 121: Loss 1.554857611656189
Model None Epoch 29 Batch 122: Loss 1.365851640701294
Model None Epoch 29 Batch 123: Loss 1.5569714307785034
Model None Epoch 29 Batch 124: Loss 1.4097486734390259
Model None Epoch 29 Batch 125: Loss 1.3585504293441772
Model None Epoch 29 Batch 126: Loss 1.417627215385437
Model None Epoch 29 Batch 127: Loss 1.3786487579345703
Model None Epoch 29 Batch 128: Loss 1.4299403429031372
Model None Epoch 29 Batch 129: Loss 1.4514673948287964
Model None Epoch 29 Batch 130: Loss 1.370995044708252
Model None Epoch 29 Batch 131: Loss 1.4138469696044922
Model None Epoch 29 Batch 132: Loss 1.4109740257263184
Model None Epoch 29 Batch 133: Loss 1.3762956857681274
Model None Epoch 29 Batch 134: Loss 1.2603001594543457
Model None Epoch 29 Batch 135: Loss 1.410273790359497
Model None Epoch 29 Batch 136: Loss 1.3873668909072876
Model None Epoch 29 Batch 137: Loss 1.3981322050094604
Model None Epoch 29 Batch 138: Loss 1.4242855310440063
Model None Epoch 29 Batch 139: Loss 1.4240981340408325
Model None Epoch 29 Batch 140: Loss 1.3461300134658813
Model None Epoch 29 Batch 141: Loss 1.4419541358947754
Model None Epoch 29 Batch 142: Loss 1.4803258180618286
Model None Epoch 29 Batch 143: Loss 1.4099959135055542
Model None Epoch 29 Batch 144: Loss 1.4348646402359009
Model None Epoch 29 Batch 145: Loss 1.488319754600525
Model None Epoch 29 Batch 146: Loss 1.5012497901916504
Model None Epoch 29 Batch 147: Loss 1.3944400548934937
Model None Epoch 29 Batch 148: Loss 1.4720003604888916
Model None Epoch 29 Batch 149: Loss 1.4154210090637207
Downstream Train Epoch: 29 [38400/50000 (77%)]	Loss: 1.542967
Model None Epoch 29 Batch 150: Loss 1.542966604232788
Model None Epoch 29 Batch 151: Loss 1.3722970485687256
Model None Epoch 29 Batch 152: Loss 1.2974921464920044
Model None Epoch 29 Batch 153: Loss 1.4321333169937134
Model None Epoch 29 Batch 154: Loss 1.3846858739852905
Model None Epoch 29 Batch 155: Loss 1.3691792488098145
Model None Epoch 29 Batch 156: Loss 1.4147900342941284
Model None Epoch 29 Batch 157: Loss 1.5216732025146484
Model None Epoch 29 Batch 158: Loss 1.4379457235336304
Model None Epoch 29 Batch 159: Loss 1.3910269737243652
Model None Epoch 29 Batch 160: Loss 1.3950130939483643
Model None Epoch 29 Batch 161: Loss 1.472853422164917
Model None Epoch 29 Batch 162: Loss 1.3632417917251587
Model None Epoch 29 Batch 163: Loss 1.377008080482483
Model None Epoch 29 Batch 164: Loss 1.3832981586456299
Model None Epoch 29 Batch 165: Loss 1.2923970222473145
Model None Epoch 29 Batch 166: Loss 1.292988896369934
Model None Epoch 29 Batch 167: Loss 1.4357633590698242
Model None Epoch 29 Batch 168: Loss 1.4108967781066895
Model None Epoch 29 Batch 169: Loss 1.4240185022354126
Model None Epoch 29 Batch 170: Loss 1.4004720449447632
Model None Epoch 29 Batch 171: Loss 1.4452617168426514
Model None Epoch 29 Batch 172: Loss 1.4032795429229736
Model None Epoch 29 Batch 173: Loss 1.4469025135040283
Model None Epoch 29 Batch 174: Loss 1.6165980100631714
Model None Epoch 29 Batch 175: Loss 1.506923794746399
Model None Epoch 29 Batch 176: Loss 1.372733235359192
Model None Epoch 29 Batch 177: Loss 1.4844380617141724
Model None Epoch 29 Batch 178: Loss 1.4184143543243408
Model None Epoch 29 Batch 179: Loss 1.363972783088684
Model None Epoch 29 Batch 180: Loss 1.4680784940719604
Model None Epoch 29 Batch 181: Loss 1.4843614101409912
Model None Epoch 29 Batch 182: Loss 1.4040889739990234
Model None Epoch 29 Batch 183: Loss 1.4162732362747192
Model None Epoch 29 Batch 184: Loss 1.501955509185791
Model None Epoch 29 Batch 185: Loss 1.5241427421569824
Model None Epoch 29 Batch 186: Loss 1.3715553283691406
Model None Epoch 29 Batch 187: Loss 1.4639109373092651
Model None Epoch 29 Batch 188: Loss 1.4272927045822144
Model None Epoch 29 Batch 189: Loss 1.425073504447937
Model None Epoch 29 Batch 190: Loss 1.4247589111328125
Model None Epoch 29 Batch 191: Loss 1.3816170692443848
Model None Epoch 29 Batch 192: Loss 1.403339147567749
Model None Epoch 29 Batch 193: Loss 1.3824658393859863
Model None Epoch 29 Batch 194: Loss 1.2894915342330933
Model None Epoch 29 Batch 195: Loss 1.4450801610946655

 Downstream Train loss: 1.4215926765179148 Acc: 0.5614
Downstream Train Epoch: 30 [0/50000 (0%)]	Loss: 1.472893
Model None Epoch 30 Batch 0: Loss 1.4728931188583374
Model None Epoch 30 Batch 1: Loss 1.417125940322876
Model None Epoch 30 Batch 2: Loss 1.4980665445327759
Model None Epoch 30 Batch 3: Loss 1.4515173435211182
Model None Epoch 30 Batch 4: Loss 1.4052679538726807
Model None Epoch 30 Batch 5: Loss 1.4066798686981201
Model None Epoch 30 Batch 6: Loss 1.3662713766098022
Model None Epoch 30 Batch 7: Loss 1.4051480293273926
Model None Epoch 30 Batch 8: Loss 1.3529709577560425
Model None Epoch 30 Batch 9: Loss 1.4602750539779663
Model None Epoch 30 Batch 10: Loss 1.3721418380737305
Model None Epoch 30 Batch 11: Loss 1.5621167421340942
Model None Epoch 30 Batch 12: Loss 1.4933191537857056
Model None Epoch 30 Batch 13: Loss 1.2761600017547607
Model None Epoch 30 Batch 14: Loss 1.4112086296081543
Model None Epoch 30 Batch 15: Loss 1.473182201385498
Model None Epoch 30 Batch 16: Loss 1.3823033571243286
Model None Epoch 30 Batch 17: Loss 1.3653863668441772
Model None Epoch 30 Batch 18: Loss 1.3825863599777222
Model None Epoch 30 Batch 19: Loss 1.450285792350769
Model None Epoch 30 Batch 20: Loss 1.4528193473815918
Model None Epoch 30 Batch 21: Loss 1.3728666305541992
Model None Epoch 30 Batch 22: Loss 1.3660976886749268
Model None Epoch 30 Batch 23: Loss 1.5414589643478394
Model None Epoch 30 Batch 24: Loss 1.4189832210540771
Model None Epoch 30 Batch 25: Loss 1.3718774318695068
Model None Epoch 30 Batch 26: Loss 1.475425362586975
Model None Epoch 30 Batch 27: Loss 1.3459676504135132
Model None Epoch 30 Batch 28: Loss 1.32054603099823
Model None Epoch 30 Batch 29: Loss 1.3843275308609009
Model None Epoch 30 Batch 30: Loss 1.474366307258606
Model None Epoch 30 Batch 31: Loss 1.42020845413208
Model None Epoch 30 Batch 32: Loss 1.4461076259613037
Model None Epoch 30 Batch 33: Loss 1.3525168895721436
Model None Epoch 30 Batch 34: Loss 1.3595333099365234
Model None Epoch 30 Batch 35: Loss 1.3068263530731201
Model None Epoch 30 Batch 36: Loss 1.3691045045852661
Model None Epoch 30 Batch 37: Loss 1.4225642681121826
Model None Epoch 30 Batch 38: Loss 1.3869035243988037
Model None Epoch 30 Batch 39: Loss 1.4502451419830322
Model None Epoch 30 Batch 40: Loss 1.4360034465789795
Model None Epoch 30 Batch 41: Loss 1.447806715965271
Model None Epoch 30 Batch 42: Loss 1.3782544136047363
Model None Epoch 30 Batch 43: Loss 1.3103898763656616
Model None Epoch 30 Batch 44: Loss 1.460727572441101
Model None Epoch 30 Batch 45: Loss 1.5254732370376587
Model None Epoch 30 Batch 46: Loss 1.367767095565796
Model None Epoch 30 Batch 47: Loss 1.3463789224624634
Model None Epoch 30 Batch 48: Loss 1.4545539617538452
Model None Epoch 30 Batch 49: Loss 1.391279697418213
Downstream Train Epoch: 30 [12800/50000 (26%)]	Loss: 1.404354
Model None Epoch 30 Batch 50: Loss 1.4043538570404053
Model None Epoch 30 Batch 51: Loss 1.3586492538452148
Model None Epoch 30 Batch 52: Loss 1.3795245885849
Model None Epoch 30 Batch 53: Loss 1.4813306331634521
Model None Epoch 30 Batch 54: Loss 1.396078109741211
Model None Epoch 30 Batch 55: Loss 1.30204439163208
Model None Epoch 30 Batch 56: Loss 1.4107247591018677
Model None Epoch 30 Batch 57: Loss 1.4439984560012817
Model None Epoch 30 Batch 58: Loss 1.4776813983917236
Model None Epoch 30 Batch 59: Loss 1.418171763420105
Model None Epoch 30 Batch 60: Loss 1.3984237909317017
Model None Epoch 30 Batch 61: Loss 1.412663459777832
Model None Epoch 30 Batch 62: Loss 1.485606074333191
Model None Epoch 30 Batch 63: Loss 1.3874582052230835
Model None Epoch 30 Batch 64: Loss 1.5280097723007202
Model None Epoch 30 Batch 65: Loss 1.3672401905059814
Model None Epoch 30 Batch 66: Loss 1.5837126970291138
Model None Epoch 30 Batch 67: Loss 1.3049893379211426
Model None Epoch 30 Batch 68: Loss 1.4254602193832397
Model None Epoch 30 Batch 69: Loss 1.4589338302612305
Model None Epoch 30 Batch 70: Loss 1.389617919921875
Model None Epoch 30 Batch 71: Loss 1.4510325193405151
Model None Epoch 30 Batch 72: Loss 1.4115740060806274
Model None Epoch 30 Batch 73: Loss 1.4979791641235352
Model None Epoch 30 Batch 74: Loss 1.5191030502319336
Model None Epoch 30 Batch 75: Loss 1.471071481704712
Model None Epoch 30 Batch 76: Loss 1.4884018898010254
Model None Epoch 30 Batch 77: Loss 1.3620891571044922
Model None Epoch 30 Batch 78: Loss 1.357569694519043
Model None Epoch 30 Batch 79: Loss 1.5206763744354248
Model None Epoch 30 Batch 80: Loss 1.4020590782165527
Model None Epoch 30 Batch 81: Loss 1.4098440408706665
Model None Epoch 30 Batch 82: Loss 1.5016472339630127
Model None Epoch 30 Batch 83: Loss 1.4945902824401855
Model None Epoch 30 Batch 84: Loss 1.3007086515426636
Model None Epoch 30 Batch 85: Loss 1.385223627090454
Model None Epoch 30 Batch 86: Loss 1.528551697731018
Model None Epoch 30 Batch 87: Loss 1.417246699333191
Model None Epoch 30 Batch 88: Loss 1.4063725471496582
Model None Epoch 30 Batch 89: Loss 1.5046827793121338
Model None Epoch 30 Batch 90: Loss 1.4421312808990479
Model None Epoch 30 Batch 91: Loss 1.4395318031311035
Model None Epoch 30 Batch 92: Loss 1.3918946981430054
Model None Epoch 30 Batch 93: Loss 1.4270130395889282
Model None Epoch 30 Batch 94: Loss 1.4429160356521606
Model None Epoch 30 Batch 95: Loss 1.3724068403244019
Model None Epoch 30 Batch 96: Loss 1.4355710744857788
Model None Epoch 30 Batch 97: Loss 1.423702359199524
Model None Epoch 30 Batch 98: Loss 1.345457911491394
Model None Epoch 30 Batch 99: Loss 1.525782585144043
Downstream Train Epoch: 30 [25600/50000 (51%)]	Loss: 1.528773
Model None Epoch 30 Batch 100: Loss 1.5287725925445557
Model None Epoch 30 Batch 101: Loss 1.4044314622879028
Model None Epoch 30 Batch 102: Loss 1.4605412483215332
Model None Epoch 30 Batch 103: Loss 1.4127379655838013
Model None Epoch 30 Batch 104: Loss 1.3724344968795776
Model None Epoch 30 Batch 105: Loss 1.3754634857177734
Model None Epoch 30 Batch 106: Loss 1.5964553356170654
Model None Epoch 30 Batch 107: Loss 1.4726059436798096
Model None Epoch 30 Batch 108: Loss 1.4434943199157715
Model None Epoch 30 Batch 109: Loss 1.3879079818725586
Model None Epoch 30 Batch 110: Loss 1.3851884603500366
Model None Epoch 30 Batch 111: Loss 1.4224987030029297
Model None Epoch 30 Batch 112: Loss 1.2866169214248657
Model None Epoch 30 Batch 113: Loss 1.4840874671936035
Model None Epoch 30 Batch 114: Loss 1.5293498039245605
Model None Epoch 30 Batch 115: Loss 1.3868073225021362
Model None Epoch 30 Batch 116: Loss 1.4057878255844116
Model None Epoch 30 Batch 117: Loss 1.3590693473815918
Model None Epoch 30 Batch 118: Loss 1.4039071798324585
Model None Epoch 30 Batch 119: Loss 1.3489117622375488
Model None Epoch 30 Batch 120: Loss 1.369132161140442
Model None Epoch 30 Batch 121: Loss 1.4515858888626099
Model None Epoch 30 Batch 122: Loss 1.4916694164276123
Model None Epoch 30 Batch 123: Loss 1.4132169485092163
Model None Epoch 30 Batch 124: Loss 1.4887179136276245
Model None Epoch 30 Batch 125: Loss 1.4171544313430786
Model None Epoch 30 Batch 126: Loss 1.433219075202942
Model None Epoch 30 Batch 127: Loss 1.462375283241272
Model None Epoch 30 Batch 128: Loss 1.3920550346374512
Model None Epoch 30 Batch 129: Loss 1.3138244152069092
Model None Epoch 30 Batch 130: Loss 1.3670867681503296
Model None Epoch 30 Batch 131: Loss 1.4429787397384644
Model None Epoch 30 Batch 132: Loss 1.4067491292953491
Model None Epoch 30 Batch 133: Loss 1.458046555519104
Model None Epoch 30 Batch 134: Loss 1.463870882987976
Model None Epoch 30 Batch 135: Loss 1.3782049417495728
Model None Epoch 30 Batch 136: Loss 1.3967955112457275
Model None Epoch 30 Batch 137: Loss 1.4260611534118652
Model None Epoch 30 Batch 138: Loss 1.356994867324829
Model None Epoch 30 Batch 139: Loss 1.370619297027588
Model None Epoch 30 Batch 140: Loss 1.4706259965896606
Model None Epoch 30 Batch 141: Loss 1.4630811214447021
Model None Epoch 30 Batch 142: Loss 1.4892065525054932
Model None Epoch 30 Batch 143: Loss 1.3462634086608887
Model None Epoch 30 Batch 144: Loss 1.4481745958328247
Model None Epoch 30 Batch 145: Loss 1.3803547620773315
Model None Epoch 30 Batch 146: Loss 1.5409324169158936
Model None Epoch 30 Batch 147: Loss 1.4734179973602295
Model None Epoch 30 Batch 148: Loss 1.3560601472854614
Model None Epoch 30 Batch 149: Loss 1.339184284210205
Downstream Train Epoch: 30 [38400/50000 (77%)]	Loss: 1.344562
Model None Epoch 30 Batch 150: Loss 1.3445615768432617
Model None Epoch 30 Batch 151: Loss 1.447364091873169
Model None Epoch 30 Batch 152: Loss 1.4531134366989136
Model None Epoch 30 Batch 153: Loss 1.3986083269119263
Model None Epoch 30 Batch 154: Loss 1.3421396017074585
Model None Epoch 30 Batch 155: Loss 1.3966237306594849
Model None Epoch 30 Batch 156: Loss 1.422083854675293
Model None Epoch 30 Batch 157: Loss 1.4018750190734863
Model None Epoch 30 Batch 158: Loss 1.3145649433135986
Model None Epoch 30 Batch 159: Loss 1.4137959480285645
Model None Epoch 30 Batch 160: Loss 1.4699633121490479
Model None Epoch 30 Batch 161: Loss 1.3691152334213257
Model None Epoch 30 Batch 162: Loss 1.3865643739700317
Model None Epoch 30 Batch 163: Loss 1.3974967002868652
Model None Epoch 30 Batch 164: Loss 1.476182460784912
Model None Epoch 30 Batch 165: Loss 1.365097165107727
Model None Epoch 30 Batch 166: Loss 1.5265637636184692
Model None Epoch 30 Batch 167: Loss 1.3222196102142334
Model None Epoch 30 Batch 168: Loss 1.3964855670928955
Model None Epoch 30 Batch 169: Loss 1.3637949228286743
Model None Epoch 30 Batch 170: Loss 1.3730918169021606
Model None Epoch 30 Batch 171: Loss 1.5501991510391235
Model None Epoch 30 Batch 172: Loss 1.4268308877944946
Model None Epoch 30 Batch 173: Loss 1.4481337070465088
Model None Epoch 30 Batch 174: Loss 1.5264086723327637
Model None Epoch 30 Batch 175: Loss 1.3959356546401978
Model None Epoch 30 Batch 176: Loss 1.393503189086914
Model None Epoch 30 Batch 177: Loss 1.4594955444335938
Model None Epoch 30 Batch 178: Loss 1.4146947860717773
Model None Epoch 30 Batch 179: Loss 1.4118613004684448
Model None Epoch 30 Batch 180: Loss 1.4069833755493164
Model None Epoch 30 Batch 181: Loss 1.5108236074447632
Model None Epoch 30 Batch 182: Loss 1.4667069911956787
Model None Epoch 30 Batch 183: Loss 1.385602355003357
Model None Epoch 30 Batch 184: Loss 1.4943771362304688
Model None Epoch 30 Batch 185: Loss 1.355846881866455
Model None Epoch 30 Batch 186: Loss 1.404676079750061
Model None Epoch 30 Batch 187: Loss 1.4943957328796387
Model None Epoch 30 Batch 188: Loss 1.504136323928833
Model None Epoch 30 Batch 189: Loss 1.5181193351745605
Model None Epoch 30 Batch 190: Loss 1.4260104894638062
Model None Epoch 30 Batch 191: Loss 1.3621121644973755
Model None Epoch 30 Batch 192: Loss 1.5036208629608154
Model None Epoch 30 Batch 193: Loss 1.389573335647583
Model None Epoch 30 Batch 194: Loss 1.308965802192688
Model None Epoch 30 Batch 195: Loss 1.210370421409607

 Downstream Train loss: 1.41979189125859 Acc: 0.5614
Downstream Train Epoch: 31 [0/50000 (0%)]	Loss: 1.481053
Model None Epoch 31 Batch 0: Loss 1.4810528755187988
Model None Epoch 31 Batch 1: Loss 1.3467239141464233
Model None Epoch 31 Batch 2: Loss 1.3432292938232422
Model None Epoch 31 Batch 3: Loss 1.3822513818740845
Model None Epoch 31 Batch 4: Loss 1.4812462329864502
Model None Epoch 31 Batch 5: Loss 1.5010206699371338
Model None Epoch 31 Batch 6: Loss 1.3713001012802124
Model None Epoch 31 Batch 7: Loss 1.402787685394287
Model None Epoch 31 Batch 8: Loss 1.2897677421569824
Model None Epoch 31 Batch 9: Loss 1.4595502614974976
Model None Epoch 31 Batch 10: Loss 1.4732438325881958
Model None Epoch 31 Batch 11: Loss 1.3539410829544067
Model None Epoch 31 Batch 12: Loss 1.3811182975769043
Model None Epoch 31 Batch 13: Loss 1.3886240720748901
Model None Epoch 31 Batch 14: Loss 1.4189592599868774
Model None Epoch 31 Batch 15: Loss 1.4687187671661377
Model None Epoch 31 Batch 16: Loss 1.3791149854660034
Model None Epoch 31 Batch 17: Loss 1.4345829486846924
Model None Epoch 31 Batch 18: Loss 1.4710413217544556
Model None Epoch 31 Batch 19: Loss 1.479936122894287
Model None Epoch 31 Batch 20: Loss 1.4578739404678345
Model None Epoch 31 Batch 21: Loss 1.4500120878219604
Model None Epoch 31 Batch 22: Loss 1.4698641300201416
Model None Epoch 31 Batch 23: Loss 1.4463168382644653
Model None Epoch 31 Batch 24: Loss 1.4219186305999756
Model None Epoch 31 Batch 25: Loss 1.4964995384216309
Model None Epoch 31 Batch 26: Loss 1.437613606452942
Model None Epoch 31 Batch 27: Loss 1.4948478937149048
Model None Epoch 31 Batch 28: Loss 1.4038139581680298
Model None Epoch 31 Batch 29: Loss 1.2823396921157837
Model None Epoch 31 Batch 30: Loss 1.5559314489364624
Model None Epoch 31 Batch 31: Loss 1.459446907043457
Model None Epoch 31 Batch 32: Loss 1.474617838859558
Model None Epoch 31 Batch 33: Loss 1.398148536682129
Model None Epoch 31 Batch 34: Loss 1.371849536895752
Model None Epoch 31 Batch 35: Loss 1.3622334003448486
Model None Epoch 31 Batch 36: Loss 1.3069801330566406
Model None Epoch 31 Batch 37: Loss 1.5268192291259766
Model None Epoch 31 Batch 38: Loss 1.338380217552185
Model None Epoch 31 Batch 39: Loss 1.4044861793518066
Model None Epoch 31 Batch 40: Loss 1.5249965190887451
Model None Epoch 31 Batch 41: Loss 1.4046701192855835
Model None Epoch 31 Batch 42: Loss 1.4371507167816162
Model None Epoch 31 Batch 43: Loss 1.4537174701690674
Model None Epoch 31 Batch 44: Loss 1.304917573928833
Model None Epoch 31 Batch 45: Loss 1.4621732234954834
Model None Epoch 31 Batch 46: Loss 1.4573689699172974
Model None Epoch 31 Batch 47: Loss 1.3887368440628052
Model None Epoch 31 Batch 48: Loss 1.3341914415359497
Model None Epoch 31 Batch 49: Loss 1.445707082748413
Downstream Train Epoch: 31 [12800/50000 (26%)]	Loss: 1.483268
Model None Epoch 31 Batch 50: Loss 1.4832675457000732
Model None Epoch 31 Batch 51: Loss 1.429640531539917
Model None Epoch 31 Batch 52: Loss 1.4303358793258667
Model None Epoch 31 Batch 53: Loss 1.4377448558807373
Model None Epoch 31 Batch 54: Loss 1.3868800401687622
Model None Epoch 31 Batch 55: Loss 1.5178388357162476
Model None Epoch 31 Batch 56: Loss 1.4870996475219727
Model None Epoch 31 Batch 57: Loss 1.3620002269744873
Model None Epoch 31 Batch 58: Loss 1.4770890474319458
Model None Epoch 31 Batch 59: Loss 1.4617060422897339
Model None Epoch 31 Batch 60: Loss 1.3766151666641235
Model None Epoch 31 Batch 61: Loss 1.4141756296157837
Model None Epoch 31 Batch 62: Loss 1.4644749164581299
Model None Epoch 31 Batch 63: Loss 1.3248910903930664
Model None Epoch 31 Batch 64: Loss 1.4441702365875244
Model None Epoch 31 Batch 65: Loss 1.3917677402496338
Model None Epoch 31 Batch 66: Loss 1.3765212297439575
Model None Epoch 31 Batch 67: Loss 1.3489021062850952
Model None Epoch 31 Batch 68: Loss 1.4190713167190552
Model None Epoch 31 Batch 69: Loss 1.4314172267913818
Model None Epoch 31 Batch 70: Loss 1.3816053867340088
Model None Epoch 31 Batch 71: Loss 1.4942026138305664
Model None Epoch 31 Batch 72: Loss 1.4489550590515137
Model None Epoch 31 Batch 73: Loss 1.5108548402786255
Model None Epoch 31 Batch 74: Loss 1.3447144031524658
Model None Epoch 31 Batch 75: Loss 1.4093921184539795
Model None Epoch 31 Batch 76: Loss 1.3442312479019165
Model None Epoch 31 Batch 77: Loss 1.415380597114563
Model None Epoch 31 Batch 78: Loss 1.400588035583496
Model None Epoch 31 Batch 79: Loss 1.3714795112609863
Model None Epoch 31 Batch 80: Loss 1.4134573936462402
Model None Epoch 31 Batch 81: Loss 1.5041508674621582
Model None Epoch 31 Batch 82: Loss 1.4172940254211426
Model None Epoch 31 Batch 83: Loss 1.4694427251815796
Model None Epoch 31 Batch 84: Loss 1.4388049840927124
Model None Epoch 31 Batch 85: Loss 1.4347426891326904
Model None Epoch 31 Batch 86: Loss 1.3907976150512695
Model None Epoch 31 Batch 87: Loss 1.4350160360336304
Model None Epoch 31 Batch 88: Loss 1.41061270236969
Model None Epoch 31 Batch 89: Loss 1.4950770139694214
Model None Epoch 31 Batch 90: Loss 1.4119534492492676
Model None Epoch 31 Batch 91: Loss 1.4330281019210815
Model None Epoch 31 Batch 92: Loss 1.3590999841690063
Model None Epoch 31 Batch 93: Loss 1.450987458229065
Model None Epoch 31 Batch 94: Loss 1.355808973312378
Model None Epoch 31 Batch 95: Loss 1.3121929168701172
Model None Epoch 31 Batch 96: Loss 1.4025932550430298
Model None Epoch 31 Batch 97: Loss 1.3956172466278076
Model None Epoch 31 Batch 98: Loss 1.3982903957366943
Model None Epoch 31 Batch 99: Loss 1.2889981269836426
Downstream Train Epoch: 31 [25600/50000 (51%)]	Loss: 1.440692
Model None Epoch 31 Batch 100: Loss 1.4406919479370117
Model None Epoch 31 Batch 101: Loss 1.3381390571594238
Model None Epoch 31 Batch 102: Loss 1.3166816234588623
Model None Epoch 31 Batch 103: Loss 1.5065821409225464
Model None Epoch 31 Batch 104: Loss 1.4326642751693726
Model None Epoch 31 Batch 105: Loss 1.4862104654312134
Model None Epoch 31 Batch 106: Loss 1.433010220527649
Model None Epoch 31 Batch 107: Loss 1.4719500541687012
Model None Epoch 31 Batch 108: Loss 1.360656499862671
Model None Epoch 31 Batch 109: Loss 1.4206300973892212
Model None Epoch 31 Batch 110: Loss 1.3601430654525757
Model None Epoch 31 Batch 111: Loss 1.5721104145050049
Model None Epoch 31 Batch 112: Loss 1.3792017698287964
Model None Epoch 31 Batch 113: Loss 1.4290854930877686
Model None Epoch 31 Batch 114: Loss 1.40217924118042
Model None Epoch 31 Batch 115: Loss 1.3912197351455688
Model None Epoch 31 Batch 116: Loss 1.4054509401321411
Model None Epoch 31 Batch 117: Loss 1.483681082725525
Model None Epoch 31 Batch 118: Loss 1.4268840551376343
Model None Epoch 31 Batch 119: Loss 1.4261068105697632
Model None Epoch 31 Batch 120: Loss 1.3971166610717773
Model None Epoch 31 Batch 121: Loss 1.4302047491073608
Model None Epoch 31 Batch 122: Loss 1.2199151515960693
Model None Epoch 31 Batch 123: Loss 1.4886109828948975
Model None Epoch 31 Batch 124: Loss 1.3702210187911987
Model None Epoch 31 Batch 125: Loss 1.4086867570877075
Model None Epoch 31 Batch 126: Loss 1.4478235244750977
Model None Epoch 31 Batch 127: Loss 1.4186201095581055
Model None Epoch 31 Batch 128: Loss 1.41805100440979
Model None Epoch 31 Batch 129: Loss 1.3825253248214722
Model None Epoch 31 Batch 130: Loss 1.3999598026275635
Model None Epoch 31 Batch 131: Loss 1.3740043640136719
Model None Epoch 31 Batch 132: Loss 1.3663102388381958
Model None Epoch 31 Batch 133: Loss 1.4268854856491089
Model None Epoch 31 Batch 134: Loss 1.3810911178588867
Model None Epoch 31 Batch 135: Loss 1.4084422588348389
Model None Epoch 31 Batch 136: Loss 1.3066258430480957
Model None Epoch 31 Batch 137: Loss 1.434399127960205
Model None Epoch 31 Batch 138: Loss 1.4192359447479248
Model None Epoch 31 Batch 139: Loss 1.421619176864624
Model None Epoch 31 Batch 140: Loss 1.6469566822052002
Model None Epoch 31 Batch 141: Loss 1.3915605545043945
Model None Epoch 31 Batch 142: Loss 1.436204195022583
Model None Epoch 31 Batch 143: Loss 1.4817719459533691
Model None Epoch 31 Batch 144: Loss 1.3322778940200806
Model None Epoch 31 Batch 145: Loss 1.466510534286499
Model None Epoch 31 Batch 146: Loss 1.3952221870422363
Model None Epoch 31 Batch 147: Loss 1.4107426404953003
Model None Epoch 31 Batch 148: Loss 1.4486902952194214
Model None Epoch 31 Batch 149: Loss 1.3947429656982422
Downstream Train Epoch: 31 [38400/50000 (77%)]	Loss: 1.389753
Model None Epoch 31 Batch 150: Loss 1.3897525072097778
Model None Epoch 31 Batch 151: Loss 1.3572444915771484
Model None Epoch 31 Batch 152: Loss 1.4679163694381714
Model None Epoch 31 Batch 153: Loss 1.4031126499176025
Model None Epoch 31 Batch 154: Loss 1.4376459121704102
Model None Epoch 31 Batch 155: Loss 1.4634116888046265
Model None Epoch 31 Batch 156: Loss 1.321088433265686
Model None Epoch 31 Batch 157: Loss 1.4169998168945312
Model None Epoch 31 Batch 158: Loss 1.3210817575454712
Model None Epoch 31 Batch 159: Loss 1.3174306154251099
Model None Epoch 31 Batch 160: Loss 1.503185510635376
Model None Epoch 31 Batch 161: Loss 1.582082986831665
Model None Epoch 31 Batch 162: Loss 1.4404971599578857
Model None Epoch 31 Batch 163: Loss 1.4032833576202393
Model None Epoch 31 Batch 164: Loss 1.558447241783142
Model None Epoch 31 Batch 165: Loss 1.4340711832046509
Model None Epoch 31 Batch 166: Loss 1.3521543741226196
Model None Epoch 31 Batch 167: Loss 1.3058441877365112
Model None Epoch 31 Batch 168: Loss 1.447011113166809
Model None Epoch 31 Batch 169: Loss 1.457545280456543
Model None Epoch 31 Batch 170: Loss 1.4115320444107056
Model None Epoch 31 Batch 171: Loss 1.4117431640625
Model None Epoch 31 Batch 172: Loss 1.4196738004684448
Model None Epoch 31 Batch 173: Loss 1.4913355112075806
Model None Epoch 31 Batch 174: Loss 1.3030600547790527
Model None Epoch 31 Batch 175: Loss 1.433392882347107
Model None Epoch 31 Batch 176: Loss 1.4014394283294678
Model None Epoch 31 Batch 177: Loss 1.327251672744751
Model None Epoch 31 Batch 178: Loss 1.32272469997406
Model None Epoch 31 Batch 179: Loss 1.3368390798568726
Model None Epoch 31 Batch 180: Loss 1.4105883836746216
Model None Epoch 31 Batch 181: Loss 1.511246681213379
Model None Epoch 31 Batch 182: Loss 1.4315311908721924
Model None Epoch 31 Batch 183: Loss 1.472461223602295
Model None Epoch 31 Batch 184: Loss 1.4453343152999878
Model None Epoch 31 Batch 185: Loss 1.3429982662200928
Model None Epoch 31 Batch 186: Loss 1.351292371749878
Model None Epoch 31 Batch 187: Loss 1.4540290832519531
Model None Epoch 31 Batch 188: Loss 1.497810959815979
Model None Epoch 31 Batch 189: Loss 1.5153812170028687
Model None Epoch 31 Batch 190: Loss 1.505140781402588
Model None Epoch 31 Batch 191: Loss 1.3596771955490112
Model None Epoch 31 Batch 192: Loss 1.397646188735962
Model None Epoch 31 Batch 193: Loss 1.5450845956802368
Model None Epoch 31 Batch 194: Loss 1.4515947103500366
Model None Epoch 31 Batch 195: Loss 1.3454744815826416

 Downstream Train loss: 1.418220468321625 Acc: 0.5614
Downstream Train Epoch: 32 [0/50000 (0%)]	Loss: 1.472199
Model None Epoch 32 Batch 0: Loss 1.4721988439559937
Model None Epoch 32 Batch 1: Loss 1.331947684288025
Model None Epoch 32 Batch 2: Loss 1.399399995803833
Model None Epoch 32 Batch 3: Loss 1.357836127281189
Model None Epoch 32 Batch 4: Loss 1.380715250968933
Model None Epoch 32 Batch 5: Loss 1.4181417226791382
Model None Epoch 32 Batch 6: Loss 1.3911277055740356
Model None Epoch 32 Batch 7: Loss 1.3628329038619995
Model None Epoch 32 Batch 8: Loss 1.323249101638794
Model None Epoch 32 Batch 9: Loss 1.4264497756958008
Model None Epoch 32 Batch 10: Loss 1.4162253141403198
Model None Epoch 32 Batch 11: Loss 1.441340684890747
Model None Epoch 32 Batch 12: Loss 1.3861037492752075
Model None Epoch 32 Batch 13: Loss 1.4900709390640259
Model None Epoch 32 Batch 14: Loss 1.510664463043213
Model None Epoch 32 Batch 15: Loss 1.3840415477752686
Model None Epoch 32 Batch 16: Loss 1.3728116750717163
Model None Epoch 32 Batch 17: Loss 1.4115357398986816
Model None Epoch 32 Batch 18: Loss 1.499833106994629
Model None Epoch 32 Batch 19: Loss 1.4702043533325195
Model None Epoch 32 Batch 20: Loss 1.4843512773513794
Model None Epoch 32 Batch 21: Loss 1.374992847442627
Model None Epoch 32 Batch 22: Loss 1.4136499166488647
Model None Epoch 32 Batch 23: Loss 1.3673954010009766
Model None Epoch 32 Batch 24: Loss 1.4335284233093262
Model None Epoch 32 Batch 25: Loss 1.440481185913086
Model None Epoch 32 Batch 26: Loss 1.457339882850647
Model None Epoch 32 Batch 27: Loss 1.324271559715271
Model None Epoch 32 Batch 28: Loss 1.3361194133758545
Model None Epoch 32 Batch 29: Loss 1.431209921836853
Model None Epoch 32 Batch 30: Loss 1.4735502004623413
Model None Epoch 32 Batch 31: Loss 1.4137479066848755
Model None Epoch 32 Batch 32: Loss 1.4839216470718384
Model None Epoch 32 Batch 33: Loss 1.4174259901046753
Model None Epoch 32 Batch 34: Loss 1.4186794757843018
Model None Epoch 32 Batch 35: Loss 1.4584741592407227
Model None Epoch 32 Batch 36: Loss 1.317578911781311
Model None Epoch 32 Batch 37: Loss 1.4342079162597656
Model None Epoch 32 Batch 38: Loss 1.3770251274108887
Model None Epoch 32 Batch 39: Loss 1.3616669178009033
Model None Epoch 32 Batch 40: Loss 1.4309667348861694
Model None Epoch 32 Batch 41: Loss 1.4431499242782593
Model None Epoch 32 Batch 42: Loss 1.3888095617294312
Model None Epoch 32 Batch 43: Loss 1.3122204542160034
Model None Epoch 32 Batch 44: Loss 1.4774391651153564
Model None Epoch 32 Batch 45: Loss 1.3875267505645752
Model None Epoch 32 Batch 46: Loss 1.455411672592163
Model None Epoch 32 Batch 47: Loss 1.4637715816497803
Model None Epoch 32 Batch 48: Loss 1.3988803625106812
Model None Epoch 32 Batch 49: Loss 1.3758455514907837
Downstream Train Epoch: 32 [12800/50000 (26%)]	Loss: 1.232051
Model None Epoch 32 Batch 50: Loss 1.2320512533187866
Model None Epoch 32 Batch 51: Loss 1.3612500429153442
Model None Epoch 32 Batch 52: Loss 1.3814336061477661
Model None Epoch 32 Batch 53: Loss 1.4310513734817505
Model None Epoch 32 Batch 54: Loss 1.372543215751648
Model None Epoch 32 Batch 55: Loss 1.4024361371994019
Model None Epoch 32 Batch 56: Loss 1.566136360168457
Model None Epoch 32 Batch 57: Loss 1.5270026922225952
Model None Epoch 32 Batch 58: Loss 1.2238895893096924
Model None Epoch 32 Batch 59: Loss 1.425315499305725
Model None Epoch 32 Batch 60: Loss 1.413759708404541
Model None Epoch 32 Batch 61: Loss 1.5567251443862915
Model None Epoch 32 Batch 62: Loss 1.371909737586975
Model None Epoch 32 Batch 63: Loss 1.4102895259857178
Model None Epoch 32 Batch 64: Loss 1.4565831422805786
Model None Epoch 32 Batch 65: Loss 1.3330570459365845
Model None Epoch 32 Batch 66: Loss 1.4681442975997925
Model None Epoch 32 Batch 67: Loss 1.4215153455734253
Model None Epoch 32 Batch 68: Loss 1.3827924728393555
Model None Epoch 32 Batch 69: Loss 1.4011070728302002
Model None Epoch 32 Batch 70: Loss 1.34285569190979
Model None Epoch 32 Batch 71: Loss 1.3626267910003662
Model None Epoch 32 Batch 72: Loss 1.4186642169952393
Model None Epoch 32 Batch 73: Loss 1.4532885551452637
Model None Epoch 32 Batch 74: Loss 1.3192836046218872
Model None Epoch 32 Batch 75: Loss 1.2540143728256226
Model None Epoch 32 Batch 76: Loss 1.4266031980514526
Model None Epoch 32 Batch 77: Loss 1.4263992309570312
Model None Epoch 32 Batch 78: Loss 1.5231577157974243
Model None Epoch 32 Batch 79: Loss 1.441955804824829
Model None Epoch 32 Batch 80: Loss 1.354748010635376
Model None Epoch 32 Batch 81: Loss 1.3656383752822876
Model None Epoch 32 Batch 82: Loss 1.3890397548675537
Model None Epoch 32 Batch 83: Loss 1.3663817644119263
Model None Epoch 32 Batch 84: Loss 1.428004503250122
Model None Epoch 32 Batch 85: Loss 1.4725353717803955
Model None Epoch 32 Batch 86: Loss 1.2950468063354492
Model None Epoch 32 Batch 87: Loss 1.4371846914291382
Model None Epoch 32 Batch 88: Loss 1.3501613140106201
Model None Epoch 32 Batch 89: Loss 1.3282997608184814
Model None Epoch 32 Batch 90: Loss 1.4519294500350952
Model None Epoch 32 Batch 91: Loss 1.4046998023986816
Model None Epoch 32 Batch 92: Loss 1.3928502798080444
Model None Epoch 32 Batch 93: Loss 1.5732043981552124
Model None Epoch 32 Batch 94: Loss 1.4210649728775024
Model None Epoch 32 Batch 95: Loss 1.502708077430725
Model None Epoch 32 Batch 96: Loss 1.4471088647842407
Model None Epoch 32 Batch 97: Loss 1.4481159448623657
Model None Epoch 32 Batch 98: Loss 1.319546103477478
Model None Epoch 32 Batch 99: Loss 1.4119893312454224
Downstream Train Epoch: 32 [25600/50000 (51%)]	Loss: 1.495498
Model None Epoch 32 Batch 100: Loss 1.4954975843429565
Model None Epoch 32 Batch 101: Loss 1.4309148788452148
Model None Epoch 32 Batch 102: Loss 1.3875404596328735
Model None Epoch 32 Batch 103: Loss 1.3996286392211914
Model None Epoch 32 Batch 104: Loss 1.474068522453308
Model None Epoch 32 Batch 105: Loss 1.4297478199005127
Model None Epoch 32 Batch 106: Loss 1.4576616287231445
Model None Epoch 32 Batch 107: Loss 1.3202954530715942
Model None Epoch 32 Batch 108: Loss 1.5322282314300537
Model None Epoch 32 Batch 109: Loss 1.3990399837493896
Model None Epoch 32 Batch 110: Loss 1.5459165573120117
Model None Epoch 32 Batch 111: Loss 1.453708291053772
Model None Epoch 32 Batch 112: Loss 1.388524055480957
Model None Epoch 32 Batch 113: Loss 1.4833049774169922
Model None Epoch 32 Batch 114: Loss 1.3564339876174927
Model None Epoch 32 Batch 115: Loss 1.4687573909759521
Model None Epoch 32 Batch 116: Loss 1.489537239074707
Model None Epoch 32 Batch 117: Loss 1.339570164680481
Model None Epoch 32 Batch 118: Loss 1.464590311050415
Model None Epoch 32 Batch 119: Loss 1.3354268074035645
Model None Epoch 32 Batch 120: Loss 1.4290844202041626
Model None Epoch 32 Batch 121: Loss 1.4771815538406372
Model None Epoch 32 Batch 122: Loss 1.3915928602218628
Model None Epoch 32 Batch 123: Loss 1.5031864643096924
Model None Epoch 32 Batch 124: Loss 1.5309333801269531
Model None Epoch 32 Batch 125: Loss 1.4363387823104858
Model None Epoch 32 Batch 126: Loss 1.2815927267074585
Model None Epoch 32 Batch 127: Loss 1.3765130043029785
Model None Epoch 32 Batch 128: Loss 1.3873651027679443
Model None Epoch 32 Batch 129: Loss 1.302628517150879
Model None Epoch 32 Batch 130: Loss 1.4269171953201294
Model None Epoch 32 Batch 131: Loss 1.3301243782043457
Model None Epoch 32 Batch 132: Loss 1.4651576280593872
Model None Epoch 32 Batch 133: Loss 1.4606640338897705
Model None Epoch 32 Batch 134: Loss 1.3979482650756836
Model None Epoch 32 Batch 135: Loss 1.3570492267608643
Model None Epoch 32 Batch 136: Loss 1.5353655815124512
Model None Epoch 32 Batch 137: Loss 1.3880584239959717
Model None Epoch 32 Batch 138: Loss 1.5635161399841309
Model None Epoch 32 Batch 139: Loss 1.3575351238250732
Model None Epoch 32 Batch 140: Loss 1.3988423347473145
Model None Epoch 32 Batch 141: Loss 1.5288795232772827
Model None Epoch 32 Batch 142: Loss 1.3280999660491943
Model None Epoch 32 Batch 143: Loss 1.3267995119094849
Model None Epoch 32 Batch 144: Loss 1.3794087171554565
Model None Epoch 32 Batch 145: Loss 1.4508877992630005
Model None Epoch 32 Batch 146: Loss 1.481225609779358
Model None Epoch 32 Batch 147: Loss 1.4682109355926514
Model None Epoch 32 Batch 148: Loss 1.4205151796340942
Model None Epoch 32 Batch 149: Loss 1.5203572511672974
Downstream Train Epoch: 32 [38400/50000 (77%)]	Loss: 1.365541
Model None Epoch 32 Batch 150: Loss 1.3655413389205933
Model None Epoch 32 Batch 151: Loss 1.4313477277755737
Model None Epoch 32 Batch 152: Loss 1.3597058057785034
Model None Epoch 32 Batch 153: Loss 1.4184620380401611
Model None Epoch 32 Batch 154: Loss 1.443419337272644
Model None Epoch 32 Batch 155: Loss 1.3036235570907593
Model None Epoch 32 Batch 156: Loss 1.5873332023620605
Model None Epoch 32 Batch 157: Loss 1.3638620376586914
Model None Epoch 32 Batch 158: Loss 1.473519206047058
Model None Epoch 32 Batch 159: Loss 1.431662917137146
Model None Epoch 32 Batch 160: Loss 1.2813464403152466
Model None Epoch 32 Batch 161: Loss 1.3606656789779663
Model None Epoch 32 Batch 162: Loss 1.4425357580184937
Model None Epoch 32 Batch 163: Loss 1.5011690855026245
Model None Epoch 32 Batch 164: Loss 1.2851958274841309
Model None Epoch 32 Batch 165: Loss 1.4035272598266602
Model None Epoch 32 Batch 166: Loss 1.358445167541504
Model None Epoch 32 Batch 167: Loss 1.4442094564437866
Model None Epoch 32 Batch 168: Loss 1.456613540649414
Model None Epoch 32 Batch 169: Loss 1.4492613077163696
Model None Epoch 32 Batch 170: Loss 1.5215115547180176
Model None Epoch 32 Batch 171: Loss 1.487854242324829
Model None Epoch 32 Batch 172: Loss 1.3721102476119995
Model None Epoch 32 Batch 173: Loss 1.3701753616333008
Model None Epoch 32 Batch 174: Loss 1.3567383289337158
Model None Epoch 32 Batch 175: Loss 1.3307688236236572
Model None Epoch 32 Batch 176: Loss 1.5060876607894897
Model None Epoch 32 Batch 177: Loss 1.352485179901123
Model None Epoch 32 Batch 178: Loss 1.5845673084259033
Model None Epoch 32 Batch 179: Loss 1.4681942462921143
Model None Epoch 32 Batch 180: Loss 1.503252625465393
Model None Epoch 32 Batch 181: Loss 1.507898211479187
Model None Epoch 32 Batch 182: Loss 1.4289577007293701
Model None Epoch 32 Batch 183: Loss 1.2497214078903198
Model None Epoch 32 Batch 184: Loss 1.4635671377182007
Model None Epoch 32 Batch 185: Loss 1.3238005638122559
Model None Epoch 32 Batch 186: Loss 1.509494662284851
Model None Epoch 32 Batch 187: Loss 1.4607317447662354
Model None Epoch 32 Batch 188: Loss 1.375478982925415
Model None Epoch 32 Batch 189: Loss 1.3701539039611816
Model None Epoch 32 Batch 190: Loss 1.42587411403656
Model None Epoch 32 Batch 191: Loss 1.3050143718719482
Model None Epoch 32 Batch 192: Loss 1.4054561853408813
Model None Epoch 32 Batch 193: Loss 1.4486345052719116
Model None Epoch 32 Batch 194: Loss 1.5932763814926147
Model None Epoch 32 Batch 195: Loss 1.3697199821472168

 Downstream Train loss: 1.4156929351845566 Acc: 0.5614
Downstream Train Epoch: 33 [0/50000 (0%)]	Loss: 1.488122
Model None Epoch 33 Batch 0: Loss 1.488121747970581
Model None Epoch 33 Batch 1: Loss 1.4094241857528687
Model None Epoch 33 Batch 2: Loss 1.4458175897598267
Model None Epoch 33 Batch 3: Loss 1.3836710453033447
Model None Epoch 33 Batch 4: Loss 1.4654464721679688
Model None Epoch 33 Batch 5: Loss 1.411089539527893
Model None Epoch 33 Batch 6: Loss 1.415105938911438
Model None Epoch 33 Batch 7: Loss 1.3821016550064087
Model None Epoch 33 Batch 8: Loss 1.3923277854919434
Model None Epoch 33 Batch 9: Loss 1.347977638244629
Model None Epoch 33 Batch 10: Loss 1.4971164464950562
Model None Epoch 33 Batch 11: Loss 1.389219880104065
Model None Epoch 33 Batch 12: Loss 1.4225066900253296
Model None Epoch 33 Batch 13: Loss 1.449592113494873
Model None Epoch 33 Batch 14: Loss 1.4478768110275269
Model None Epoch 33 Batch 15: Loss 1.3920131921768188
Model None Epoch 33 Batch 16: Loss 1.439609408378601
Model None Epoch 33 Batch 17: Loss 1.3495619297027588
Model None Epoch 33 Batch 18: Loss 1.4021532535552979
Model None Epoch 33 Batch 19: Loss 1.3279424905776978
Model None Epoch 33 Batch 20: Loss 1.4130127429962158
Model None Epoch 33 Batch 21: Loss 1.376114845275879
Model None Epoch 33 Batch 22: Loss 1.3850138187408447
Model None Epoch 33 Batch 23: Loss 1.363090991973877
Model None Epoch 33 Batch 24: Loss 1.3505222797393799
Model None Epoch 33 Batch 25: Loss 1.3679404258728027
Model None Epoch 33 Batch 26: Loss 1.4653325080871582
Model None Epoch 33 Batch 27: Loss 1.5077030658721924
Model None Epoch 33 Batch 28: Loss 1.4844602346420288
Model None Epoch 33 Batch 29: Loss 1.382045030593872
Model None Epoch 33 Batch 30: Loss 1.273045301437378
Model None Epoch 33 Batch 31: Loss 1.441799283027649
Model None Epoch 33 Batch 32: Loss 1.4722837209701538
Model None Epoch 33 Batch 33: Loss 1.508882761001587
Model None Epoch 33 Batch 34: Loss 1.4152885675430298
Model None Epoch 33 Batch 35: Loss 1.4819631576538086
Model None Epoch 33 Batch 36: Loss 1.541900634765625
Model None Epoch 33 Batch 37: Loss 1.4077054262161255
Model None Epoch 33 Batch 38: Loss 1.5332719087600708
Model None Epoch 33 Batch 39: Loss 1.4837018251419067
Model None Epoch 33 Batch 40: Loss 1.3791428804397583
Model None Epoch 33 Batch 41: Loss 1.3415753841400146
Model None Epoch 33 Batch 42: Loss 1.3773010969161987
Model None Epoch 33 Batch 43: Loss 1.46621572971344
Model None Epoch 33 Batch 44: Loss 1.3558746576309204
Model None Epoch 33 Batch 45: Loss 1.542338252067566
Model None Epoch 33 Batch 46: Loss 1.3587262630462646
Model None Epoch 33 Batch 47: Loss 1.3900399208068848
Model None Epoch 33 Batch 48: Loss 1.3942146301269531
Model None Epoch 33 Batch 49: Loss 1.3549877405166626
Downstream Train Epoch: 33 [12800/50000 (26%)]	Loss: 1.402005
Model None Epoch 33 Batch 50: Loss 1.402004599571228
Model None Epoch 33 Batch 51: Loss 1.4341846704483032
Model None Epoch 33 Batch 52: Loss 1.5328158140182495
Model None Epoch 33 Batch 53: Loss 1.4243468046188354
Model None Epoch 33 Batch 54: Loss 1.3973578214645386
Model None Epoch 33 Batch 55: Loss 1.4502993822097778
Model None Epoch 33 Batch 56: Loss 1.2848494052886963
Model None Epoch 33 Batch 57: Loss 1.4466503858566284
Model None Epoch 33 Batch 58: Loss 1.4257009029388428
Model None Epoch 33 Batch 59: Loss 1.4899400472640991
Model None Epoch 33 Batch 60: Loss 1.3951401710510254
Model None Epoch 33 Batch 61: Loss 1.4215656518936157
Model None Epoch 33 Batch 62: Loss 1.386834740638733
Model None Epoch 33 Batch 63: Loss 1.361495018005371
Model None Epoch 33 Batch 64: Loss 1.4313596487045288
Model None Epoch 33 Batch 65: Loss 1.4840929508209229
Model None Epoch 33 Batch 66: Loss 1.4160915613174438
Model None Epoch 33 Batch 67: Loss 1.3903237581253052
Model None Epoch 33 Batch 68: Loss 1.4319747686386108
Model None Epoch 33 Batch 69: Loss 1.3559781312942505
Model None Epoch 33 Batch 70: Loss 1.4184691905975342
Model None Epoch 33 Batch 71: Loss 1.5250598192214966
Model None Epoch 33 Batch 72: Loss 1.5033797025680542
Model None Epoch 33 Batch 73: Loss 1.278947114944458
Model None Epoch 33 Batch 74: Loss 1.4159793853759766
Model None Epoch 33 Batch 75: Loss 1.434546947479248
Model None Epoch 33 Batch 76: Loss 1.3176991939544678
Model None Epoch 33 Batch 77: Loss 1.4496793746948242
Model None Epoch 33 Batch 78: Loss 1.549246907234192
Model None Epoch 33 Batch 79: Loss 1.4399296045303345
Model None Epoch 33 Batch 80: Loss 1.5680723190307617
Model None Epoch 33 Batch 81: Loss 1.5665699243545532
Model None Epoch 33 Batch 82: Loss 1.3228880167007446
Model None Epoch 33 Batch 83: Loss 1.4879534244537354
Model None Epoch 33 Batch 84: Loss 1.4196518659591675
Model None Epoch 33 Batch 85: Loss 1.3048359155654907
Model None Epoch 33 Batch 86: Loss 1.4070323705673218
Model None Epoch 33 Batch 87: Loss 1.4543880224227905
Model None Epoch 33 Batch 88: Loss 1.3529691696166992
Model None Epoch 33 Batch 89: Loss 1.4096450805664062
Model None Epoch 33 Batch 90: Loss 1.3915205001831055
Model None Epoch 33 Batch 91: Loss 1.3534057140350342
Model None Epoch 33 Batch 92: Loss 1.3616045713424683
Model None Epoch 33 Batch 93: Loss 1.5050559043884277
Model None Epoch 33 Batch 94: Loss 1.3813996315002441
Model None Epoch 33 Batch 95: Loss 1.3911445140838623
Model None Epoch 33 Batch 96: Loss 1.3403822183609009
Model None Epoch 33 Batch 97: Loss 1.4973714351654053
Model None Epoch 33 Batch 98: Loss 1.4322876930236816
Model None Epoch 33 Batch 99: Loss 1.3490105867385864
Downstream Train Epoch: 33 [25600/50000 (51%)]	Loss: 1.556861
Model None Epoch 33 Batch 100: Loss 1.5568606853485107
Model None Epoch 33 Batch 101: Loss 1.4240435361862183
Model None Epoch 33 Batch 102: Loss 1.45039701461792
Model None Epoch 33 Batch 103: Loss 1.5236150026321411
Model None Epoch 33 Batch 104: Loss 1.3517042398452759
Model None Epoch 33 Batch 105: Loss 1.4352953433990479
Model None Epoch 33 Batch 106: Loss 1.403050184249878
Model None Epoch 33 Batch 107: Loss 1.4014173746109009
Model None Epoch 33 Batch 108: Loss 1.5310184955596924
Model None Epoch 33 Batch 109: Loss 1.449661374092102
Model None Epoch 33 Batch 110: Loss 1.3324503898620605
Model None Epoch 33 Batch 111: Loss 1.320704698562622
Model None Epoch 33 Batch 112: Loss 1.3377692699432373
Model None Epoch 33 Batch 113: Loss 1.3372565507888794
Model None Epoch 33 Batch 114: Loss 1.4633954763412476
Model None Epoch 33 Batch 115: Loss 1.4382699728012085
Model None Epoch 33 Batch 116: Loss 1.3835318088531494
Model None Epoch 33 Batch 117: Loss 1.4791700839996338
Model None Epoch 33 Batch 118: Loss 1.4059240818023682
Model None Epoch 33 Batch 119: Loss 1.4865972995758057
Model None Epoch 33 Batch 120: Loss 1.5081298351287842
Model None Epoch 33 Batch 121: Loss 1.4089713096618652
Model None Epoch 33 Batch 122: Loss 1.2837313413619995
Model None Epoch 33 Batch 123: Loss 1.4591318368911743
Model None Epoch 33 Batch 124: Loss 1.2246578931808472
Model None Epoch 33 Batch 125: Loss 1.5027467012405396
Model None Epoch 33 Batch 126: Loss 1.404176950454712
Model None Epoch 33 Batch 127: Loss 1.4530489444732666
Model None Epoch 33 Batch 128: Loss 1.4404958486557007
Model None Epoch 33 Batch 129: Loss 1.4301952123641968
Model None Epoch 33 Batch 130: Loss 1.4469151496887207
Model None Epoch 33 Batch 131: Loss 1.3808951377868652
Model None Epoch 33 Batch 132: Loss 1.3685994148254395
Model None Epoch 33 Batch 133: Loss 1.3808517456054688
Model None Epoch 33 Batch 134: Loss 1.4140667915344238
Model None Epoch 33 Batch 135: Loss 1.3230525255203247
Model None Epoch 33 Batch 136: Loss 1.4245439767837524
Model None Epoch 33 Batch 137: Loss 1.3994145393371582
Model None Epoch 33 Batch 138: Loss 1.448475956916809
Model None Epoch 33 Batch 139: Loss 1.5202969312667847
Model None Epoch 33 Batch 140: Loss 1.407607913017273
Model None Epoch 33 Batch 141: Loss 1.3917677402496338
Model None Epoch 33 Batch 142: Loss 1.585646629333496
Model None Epoch 33 Batch 143: Loss 1.365634560585022
Model None Epoch 33 Batch 144: Loss 1.4941234588623047
Model None Epoch 33 Batch 145: Loss 1.3653055429458618
Model None Epoch 33 Batch 146: Loss 1.3866536617279053
Model None Epoch 33 Batch 147: Loss 1.4205310344696045
Model None Epoch 33 Batch 148: Loss 1.4416191577911377
Model None Epoch 33 Batch 149: Loss 1.4939041137695312
Downstream Train Epoch: 33 [38400/50000 (77%)]	Loss: 1.432011
Model None Epoch 33 Batch 150: Loss 1.432011365890503
Model None Epoch 33 Batch 151: Loss 1.451100468635559
Model None Epoch 33 Batch 152: Loss 1.554565191268921
Model None Epoch 33 Batch 153: Loss 1.3219047784805298
Model None Epoch 33 Batch 154: Loss 1.3385529518127441
Model None Epoch 33 Batch 155: Loss 1.3039308786392212
Model None Epoch 33 Batch 156: Loss 1.39817214012146
Model None Epoch 33 Batch 157: Loss 1.2999645471572876
Model None Epoch 33 Batch 158: Loss 1.402756929397583
Model None Epoch 33 Batch 159: Loss 1.337676763534546
Model None Epoch 33 Batch 160: Loss 1.348161220550537
Model None Epoch 33 Batch 161: Loss 1.378197431564331
Model None Epoch 33 Batch 162: Loss 1.452307105064392
Model None Epoch 33 Batch 163: Loss 1.4815912246704102
Model None Epoch 33 Batch 164: Loss 1.3397350311279297
Model None Epoch 33 Batch 165: Loss 1.421393871307373
Model None Epoch 33 Batch 166: Loss 1.391048550605774
Model None Epoch 33 Batch 167: Loss 1.4783475399017334
Model None Epoch 33 Batch 168: Loss 1.344972014427185
Model None Epoch 33 Batch 169: Loss 1.428006887435913
Model None Epoch 33 Batch 170: Loss 1.3394598960876465
Model None Epoch 33 Batch 171: Loss 1.3873660564422607
Model None Epoch 33 Batch 172: Loss 1.3085745573043823
Model None Epoch 33 Batch 173: Loss 1.4968488216400146
Model None Epoch 33 Batch 174: Loss 1.440288782119751
Model None Epoch 33 Batch 175: Loss 1.4297757148742676
Model None Epoch 33 Batch 176: Loss 1.412042260169983
Model None Epoch 33 Batch 177: Loss 1.4302356243133545
Model None Epoch 33 Batch 178: Loss 1.4604467153549194
Model None Epoch 33 Batch 179: Loss 1.3223674297332764
Model None Epoch 33 Batch 180: Loss 1.4476449489593506
Model None Epoch 33 Batch 181: Loss 1.4256463050842285
Model None Epoch 33 Batch 182: Loss 1.3869013786315918
Model None Epoch 33 Batch 183: Loss 1.3939037322998047
Model None Epoch 33 Batch 184: Loss 1.399921178817749
Model None Epoch 33 Batch 185: Loss 1.318811297416687
Model None Epoch 33 Batch 186: Loss 1.4354456663131714
Model None Epoch 33 Batch 187: Loss 1.3947356939315796
Model None Epoch 33 Batch 188: Loss 1.4352625608444214
Model None Epoch 33 Batch 189: Loss 1.3703569173812866
Model None Epoch 33 Batch 190: Loss 1.4245104789733887
Model None Epoch 33 Batch 191: Loss 1.3694312572479248
Model None Epoch 33 Batch 192: Loss 1.4532511234283447
Model None Epoch 33 Batch 193: Loss 1.4130834341049194
Model None Epoch 33 Batch 194: Loss 1.3305667638778687
Model None Epoch 33 Batch 195: Loss 1.3336105346679688

 Downstream Train loss: 1.4143852755731465 Acc: 0.5614
Downstream Train Epoch: 34 [0/50000 (0%)]	Loss: 1.291592
Model None Epoch 34 Batch 0: Loss 1.2915918827056885
Model None Epoch 34 Batch 1: Loss 1.3235447406768799
Model None Epoch 34 Batch 2: Loss 1.4039885997772217
Model None Epoch 34 Batch 3: Loss 1.430826187133789
Model None Epoch 34 Batch 4: Loss 1.3375494480133057
Model None Epoch 34 Batch 5: Loss 1.3750296831130981
Model None Epoch 34 Batch 6: Loss 1.441020131111145
Model None Epoch 34 Batch 7: Loss 1.5149751901626587
Model None Epoch 34 Batch 8: Loss 1.3799312114715576
Model None Epoch 34 Batch 9: Loss 1.4008718729019165
Model None Epoch 34 Batch 10: Loss 1.5061219930648804
Model None Epoch 34 Batch 11: Loss 1.4089269638061523
Model None Epoch 34 Batch 12: Loss 1.3710883855819702
Model None Epoch 34 Batch 13: Loss 1.4243065118789673
Model None Epoch 34 Batch 14: Loss 1.3594685792922974
Model None Epoch 34 Batch 15: Loss 1.3580151796340942
Model None Epoch 34 Batch 16: Loss 1.4045857191085815
Model None Epoch 34 Batch 17: Loss 1.4869316816329956
Model None Epoch 34 Batch 18: Loss 1.4995685815811157
Model None Epoch 34 Batch 19: Loss 1.4326786994934082
Model None Epoch 34 Batch 20: Loss 1.442864179611206
Model None Epoch 34 Batch 21: Loss 1.4087170362472534
Model None Epoch 34 Batch 22: Loss 1.42900812625885
Model None Epoch 34 Batch 23: Loss 1.436498999595642
Model None Epoch 34 Batch 24: Loss 1.4247350692749023
Model None Epoch 34 Batch 25: Loss 1.345624327659607
Model None Epoch 34 Batch 26: Loss 1.433222770690918
Model None Epoch 34 Batch 27: Loss 1.3125602006912231
Model None Epoch 34 Batch 28: Loss 1.3534079790115356
Model None Epoch 34 Batch 29: Loss 1.3674026727676392
Model None Epoch 34 Batch 30: Loss 1.3783448934555054
Model None Epoch 34 Batch 31: Loss 1.338233232498169
Model None Epoch 34 Batch 32: Loss 1.4983010292053223
Model None Epoch 34 Batch 33: Loss 1.426671028137207
Model None Epoch 34 Batch 34: Loss 1.3980293273925781
Model None Epoch 34 Batch 35: Loss 1.4795094728469849
Model None Epoch 34 Batch 36: Loss 1.3712167739868164
Model None Epoch 34 Batch 37: Loss 1.3526965379714966
Model None Epoch 34 Batch 38: Loss 1.4368484020233154
Model None Epoch 34 Batch 39: Loss 1.4931583404541016
Model None Epoch 34 Batch 40: Loss 1.444201946258545
Model None Epoch 34 Batch 41: Loss 1.2814350128173828
Model None Epoch 34 Batch 42: Loss 1.386658787727356
Model None Epoch 34 Batch 43: Loss 1.5243233442306519
Model None Epoch 34 Batch 44: Loss 1.3428399562835693
Model None Epoch 34 Batch 45: Loss 1.5103814601898193
Model None Epoch 34 Batch 46: Loss 1.428146243095398
Model None Epoch 34 Batch 47: Loss 1.457590103149414
Model None Epoch 34 Batch 48: Loss 1.4270663261413574
Model None Epoch 34 Batch 49: Loss 1.3392404317855835
Downstream Train Epoch: 34 [12800/50000 (26%)]	Loss: 1.419108
Model None Epoch 34 Batch 50: Loss 1.4191077947616577
Model None Epoch 34 Batch 51: Loss 1.4014947414398193
Model None Epoch 34 Batch 52: Loss 1.4198728799819946
Model None Epoch 34 Batch 53: Loss 1.3465303182601929
Model None Epoch 34 Batch 54: Loss 1.5586695671081543
Model None Epoch 34 Batch 55: Loss 1.3681941032409668
Model None Epoch 34 Batch 56: Loss 1.509737253189087
Model None Epoch 34 Batch 57: Loss 1.407273292541504
Model None Epoch 34 Batch 58: Loss 1.3985201120376587
Model None Epoch 34 Batch 59: Loss 1.4319067001342773
Model None Epoch 34 Batch 60: Loss 1.3565233945846558
Model None Epoch 34 Batch 61: Loss 1.4379571676254272
Model None Epoch 34 Batch 62: Loss 1.4052165746688843
Model None Epoch 34 Batch 63: Loss 1.4057902097702026
Model None Epoch 34 Batch 64: Loss 1.5010249614715576
Model None Epoch 34 Batch 65: Loss 1.409503698348999
Model None Epoch 34 Batch 66: Loss 1.4858726263046265
Model None Epoch 34 Batch 67: Loss 1.5698798894882202
Model None Epoch 34 Batch 68: Loss 1.4898216724395752
Model None Epoch 34 Batch 69: Loss 1.3670876026153564
Model None Epoch 34 Batch 70: Loss 1.4391913414001465
Model None Epoch 34 Batch 71: Loss 1.3654086589813232
Model None Epoch 34 Batch 72: Loss 1.4192758798599243
Model None Epoch 34 Batch 73: Loss 1.3950254917144775
Model None Epoch 34 Batch 74: Loss 1.372030258178711
Model None Epoch 34 Batch 75: Loss 1.4004366397857666
Model None Epoch 34 Batch 76: Loss 1.4027833938598633
Model None Epoch 34 Batch 77: Loss 1.478638768196106
Model None Epoch 34 Batch 78: Loss 1.3388112783432007
Model None Epoch 34 Batch 79: Loss 1.4381158351898193
Model None Epoch 34 Batch 80: Loss 1.4455581903457642
Model None Epoch 34 Batch 81: Loss 1.3074103593826294
Model None Epoch 34 Batch 82: Loss 1.5343704223632812
Model None Epoch 34 Batch 83: Loss 1.4656791687011719
Model None Epoch 34 Batch 84: Loss 1.3556373119354248
Model None Epoch 34 Batch 85: Loss 1.3520442247390747
Model None Epoch 34 Batch 86: Loss 1.425437331199646
Model None Epoch 34 Batch 87: Loss 1.486280918121338
Model None Epoch 34 Batch 88: Loss 1.4125728607177734
Model None Epoch 34 Batch 89: Loss 1.4292017221450806
Model None Epoch 34 Batch 90: Loss 1.4047436714172363
Model None Epoch 34 Batch 91: Loss 1.4255363941192627
Model None Epoch 34 Batch 92: Loss 1.4170507192611694
Model None Epoch 34 Batch 93: Loss 1.441185712814331
Model None Epoch 34 Batch 94: Loss 1.5098899602890015
Model None Epoch 34 Batch 95: Loss 1.392372965812683
Model None Epoch 34 Batch 96: Loss 1.399886131286621
Model None Epoch 34 Batch 97: Loss 1.3350595235824585
Model None Epoch 34 Batch 98: Loss 1.4315626621246338
Model None Epoch 34 Batch 99: Loss 1.4576956033706665
Downstream Train Epoch: 34 [25600/50000 (51%)]	Loss: 1.469081
Model None Epoch 34 Batch 100: Loss 1.4690806865692139
Model None Epoch 34 Batch 101: Loss 1.38005793094635
Model None Epoch 34 Batch 102: Loss 1.5017527341842651
Model None Epoch 34 Batch 103: Loss 1.4066166877746582
Model None Epoch 34 Batch 104: Loss 1.4993408918380737
Model None Epoch 34 Batch 105: Loss 1.4318928718566895
Model None Epoch 34 Batch 106: Loss 1.4068628549575806
Model None Epoch 34 Batch 107: Loss 1.337254285812378
Model None Epoch 34 Batch 108: Loss 1.4573228359222412
Model None Epoch 34 Batch 109: Loss 1.5315073728561401
Model None Epoch 34 Batch 110: Loss 1.5240135192871094
Model None Epoch 34 Batch 111: Loss 1.451542854309082
Model None Epoch 34 Batch 112: Loss 1.4070274829864502
Model None Epoch 34 Batch 113: Loss 1.4040628671646118
Model None Epoch 34 Batch 114: Loss 1.4734623432159424
Model None Epoch 34 Batch 115: Loss 1.5227546691894531
Model None Epoch 34 Batch 116: Loss 1.5352017879486084
Model None Epoch 34 Batch 117: Loss 1.3568147420883179
Model None Epoch 34 Batch 118: Loss 1.3750603199005127
Model None Epoch 34 Batch 119: Loss 1.3405417203903198
Model None Epoch 34 Batch 120: Loss 1.436739206314087
Model None Epoch 34 Batch 121: Loss 1.333583950996399
Model None Epoch 34 Batch 122: Loss 1.5380011796951294
Model None Epoch 34 Batch 123: Loss 1.52177095413208
Model None Epoch 34 Batch 124: Loss 1.5251915454864502
Model None Epoch 34 Batch 125: Loss 1.3307082653045654
Model None Epoch 34 Batch 126: Loss 1.399407148361206
Model None Epoch 34 Batch 127: Loss 1.3685574531555176
Model None Epoch 34 Batch 128: Loss 1.3598214387893677
Model None Epoch 34 Batch 129: Loss 1.3820157051086426
Model None Epoch 34 Batch 130: Loss 1.377884864807129
Model None Epoch 34 Batch 131: Loss 1.3488659858703613
Model None Epoch 34 Batch 132: Loss 1.3940401077270508
Model None Epoch 34 Batch 133: Loss 1.3621138334274292
Model None Epoch 34 Batch 134: Loss 1.403671383857727
Model None Epoch 34 Batch 135: Loss 1.3633394241333008
Model None Epoch 34 Batch 136: Loss 1.301777958869934
Model None Epoch 34 Batch 137: Loss 1.4812018871307373
Model None Epoch 34 Batch 138: Loss 1.4500826597213745
Model None Epoch 34 Batch 139: Loss 1.4344892501831055
Model None Epoch 34 Batch 140: Loss 1.4507702589035034
Model None Epoch 34 Batch 141: Loss 1.4249743223190308
Model None Epoch 34 Batch 142: Loss 1.3655610084533691
Model None Epoch 34 Batch 143: Loss 1.3748185634613037
Model None Epoch 34 Batch 144: Loss 1.5157880783081055
Model None Epoch 34 Batch 145: Loss 1.4460675716400146
Model None Epoch 34 Batch 146: Loss 1.3598337173461914
Model None Epoch 34 Batch 147: Loss 1.3501759767532349
Model None Epoch 34 Batch 148: Loss 1.3417296409606934
Model None Epoch 34 Batch 149: Loss 1.3940340280532837
Downstream Train Epoch: 34 [38400/50000 (77%)]	Loss: 1.340128
Model None Epoch 34 Batch 150: Loss 1.3401275873184204
Model None Epoch 34 Batch 151: Loss 1.3784276247024536
Model None Epoch 34 Batch 152: Loss 1.428285002708435
Model None Epoch 34 Batch 153: Loss 1.449450969696045
Model None Epoch 34 Batch 154: Loss 1.3431223630905151
Model None Epoch 34 Batch 155: Loss 1.4479973316192627
Model None Epoch 34 Batch 156: Loss 1.4091103076934814
Model None Epoch 34 Batch 157: Loss 1.4585472345352173
Model None Epoch 34 Batch 158: Loss 1.522100806236267
Model None Epoch 34 Batch 159: Loss 1.3802454471588135
Model None Epoch 34 Batch 160: Loss 1.3177337646484375
Model None Epoch 34 Batch 161: Loss 1.447349190711975
Model None Epoch 34 Batch 162: Loss 1.3725683689117432
Model None Epoch 34 Batch 163: Loss 1.3592782020568848
Model None Epoch 34 Batch 164: Loss 1.2946264743804932
Model None Epoch 34 Batch 165: Loss 1.4088022708892822
Model None Epoch 34 Batch 166: Loss 1.340789794921875
Model None Epoch 34 Batch 167: Loss 1.5060689449310303
Model None Epoch 34 Batch 168: Loss 1.3936432600021362
Model None Epoch 34 Batch 169: Loss 1.5411323308944702
Model None Epoch 34 Batch 170: Loss 1.315473198890686
Model None Epoch 34 Batch 171: Loss 1.3398991823196411
Model None Epoch 34 Batch 172: Loss 1.4513744115829468
Model None Epoch 34 Batch 173: Loss 1.346741795539856
Model None Epoch 34 Batch 174: Loss 1.3395211696624756
Model None Epoch 34 Batch 175: Loss 1.3483984470367432
Model None Epoch 34 Batch 176: Loss 1.2980948686599731
Model None Epoch 34 Batch 177: Loss 1.4237655401229858
Model None Epoch 34 Batch 178: Loss 1.3544747829437256
Model None Epoch 34 Batch 179: Loss 1.3978949785232544
Model None Epoch 34 Batch 180: Loss 1.3557640314102173
Model None Epoch 34 Batch 181: Loss 1.477033019065857
Model None Epoch 34 Batch 182: Loss 1.3503882884979248
Model None Epoch 34 Batch 183: Loss 1.4539587497711182
Model None Epoch 34 Batch 184: Loss 1.4522687196731567
Model None Epoch 34 Batch 185: Loss 1.4357280731201172
Model None Epoch 34 Batch 186: Loss 1.5095757246017456
Model None Epoch 34 Batch 187: Loss 1.262231707572937
Model None Epoch 34 Batch 188: Loss 1.5201841592788696
Model None Epoch 34 Batch 189: Loss 1.4201792478561401
Model None Epoch 34 Batch 190: Loss 1.4349700212478638
Model None Epoch 34 Batch 191: Loss 1.3539966344833374
Model None Epoch 34 Batch 192: Loss 1.4790527820587158
Model None Epoch 34 Batch 193: Loss 1.398853063583374
Model None Epoch 34 Batch 194: Loss 1.4440687894821167
Model None Epoch 34 Batch 195: Loss 1.4123523235321045

 Downstream Train loss: 1.4135391481068669 Acc: 0.5614
Downstream Train Epoch: 35 [0/50000 (0%)]	Loss: 1.360548
Model None Epoch 35 Batch 0: Loss 1.3605477809906006
Model None Epoch 35 Batch 1: Loss 1.2970846891403198
Model None Epoch 35 Batch 2: Loss 1.4950370788574219
Model None Epoch 35 Batch 3: Loss 1.4384129047393799
Model None Epoch 35 Batch 4: Loss 1.387115716934204
Model None Epoch 35 Batch 5: Loss 1.4490259885787964
Model None Epoch 35 Batch 6: Loss 1.4120221138000488
Model None Epoch 35 Batch 7: Loss 1.4502228498458862
Model None Epoch 35 Batch 8: Loss 1.2591140270233154
Model None Epoch 35 Batch 9: Loss 1.4179961681365967
Model None Epoch 35 Batch 10: Loss 1.3799490928649902
Model None Epoch 35 Batch 11: Loss 1.4755550622940063
Model None Epoch 35 Batch 12: Loss 1.293025255203247
Model None Epoch 35 Batch 13: Loss 1.364608883857727
Model None Epoch 35 Batch 14: Loss 1.3306330442428589
Model None Epoch 35 Batch 15: Loss 1.3742866516113281
Model None Epoch 35 Batch 16: Loss 1.427469253540039
Model None Epoch 35 Batch 17: Loss 1.3438639640808105
Model None Epoch 35 Batch 18: Loss 1.4651727676391602
Model None Epoch 35 Batch 19: Loss 1.353322982788086
Model None Epoch 35 Batch 20: Loss 1.4068375825881958
Model None Epoch 35 Batch 21: Loss 1.3392888307571411
Model None Epoch 35 Batch 22: Loss 1.5633256435394287
Model None Epoch 35 Batch 23: Loss 1.3327457904815674
Model None Epoch 35 Batch 24: Loss 1.5082781314849854
Model None Epoch 35 Batch 25: Loss 1.3538432121276855
Model None Epoch 35 Batch 26: Loss 1.4769823551177979
Model None Epoch 35 Batch 27: Loss 1.4721627235412598
Model None Epoch 35 Batch 28: Loss 1.4609150886535645
Model None Epoch 35 Batch 29: Loss 1.3647489547729492
Model None Epoch 35 Batch 30: Loss 1.373474359512329
Model None Epoch 35 Batch 31: Loss 1.3748760223388672
Model None Epoch 35 Batch 32: Loss 1.5321091413497925
Model None Epoch 35 Batch 33: Loss 1.4793851375579834
Model None Epoch 35 Batch 34: Loss 1.4172735214233398
Model None Epoch 35 Batch 35: Loss 1.405534267425537
Model None Epoch 35 Batch 36: Loss 1.3584493398666382
Model None Epoch 35 Batch 37: Loss 1.4276455640792847
Model None Epoch 35 Batch 38: Loss 1.3585482835769653
Model None Epoch 35 Batch 39: Loss 1.323554277420044
Model None Epoch 35 Batch 40: Loss 1.4069195985794067
Model None Epoch 35 Batch 41: Loss 1.3156543970108032
Model None Epoch 35 Batch 42: Loss 1.4751896858215332
Model None Epoch 35 Batch 43: Loss 1.4549870491027832
Model None Epoch 35 Batch 44: Loss 1.3937174081802368
Model None Epoch 35 Batch 45: Loss 1.4913384914398193
Model None Epoch 35 Batch 46: Loss 1.35383141040802
Model None Epoch 35 Batch 47: Loss 1.545945167541504
Model None Epoch 35 Batch 48: Loss 1.305539608001709
Model None Epoch 35 Batch 49: Loss 1.3947981595993042
Downstream Train Epoch: 35 [12800/50000 (26%)]	Loss: 1.394781
Model None Epoch 35 Batch 50: Loss 1.3947805166244507
Model None Epoch 35 Batch 51: Loss 1.4821994304656982
Model None Epoch 35 Batch 52: Loss 1.4980767965316772
Model None Epoch 35 Batch 53: Loss 1.411150574684143
Model None Epoch 35 Batch 54: Loss 1.3599432706832886
Model None Epoch 35 Batch 55: Loss 1.3463135957717896
Model None Epoch 35 Batch 56: Loss 1.4486362934112549
Model None Epoch 35 Batch 57: Loss 1.4487876892089844
Model None Epoch 35 Batch 58: Loss 1.4125524759292603
Model None Epoch 35 Batch 59: Loss 1.4508568048477173
Model None Epoch 35 Batch 60: Loss 1.6075705289840698
Model None Epoch 35 Batch 61: Loss 1.4454693794250488
Model None Epoch 35 Batch 62: Loss 1.359375
Model None Epoch 35 Batch 63: Loss 1.3072417974472046
Model None Epoch 35 Batch 64: Loss 1.3649622201919556
Model None Epoch 35 Batch 65: Loss 1.3574893474578857
Model None Epoch 35 Batch 66: Loss 1.4228030443191528
Model None Epoch 35 Batch 67: Loss 1.3470731973648071
Model None Epoch 35 Batch 68: Loss 1.3816561698913574
Model None Epoch 35 Batch 69: Loss 1.3843824863433838
Model None Epoch 35 Batch 70: Loss 1.3493610620498657
Model None Epoch 35 Batch 71: Loss 1.4341026544570923
Model None Epoch 35 Batch 72: Loss 1.3903229236602783
Model None Epoch 35 Batch 73: Loss 1.4041416645050049
Model None Epoch 35 Batch 74: Loss 1.4518200159072876
Model None Epoch 35 Batch 75: Loss 1.5043675899505615
Model None Epoch 35 Batch 76: Loss 1.4134784936904907
Model None Epoch 35 Batch 77: Loss 1.4850009679794312
Model None Epoch 35 Batch 78: Loss 1.4032498598098755
Model None Epoch 35 Batch 79: Loss 1.403367519378662
Model None Epoch 35 Batch 80: Loss 1.4596129655838013
Model None Epoch 35 Batch 81: Loss 1.3915836811065674
Model None Epoch 35 Batch 82: Loss 1.411389708518982
Model None Epoch 35 Batch 83: Loss 1.4684007167816162
Model None Epoch 35 Batch 84: Loss 1.4155443906784058
Model None Epoch 35 Batch 85: Loss 1.509573221206665
Model None Epoch 35 Batch 86: Loss 1.5361745357513428
Model None Epoch 35 Batch 87: Loss 1.351678490638733
Model None Epoch 35 Batch 88: Loss 1.3855640888214111
Model None Epoch 35 Batch 89: Loss 1.5365179777145386
Model None Epoch 35 Batch 90: Loss 1.4258902072906494
Model None Epoch 35 Batch 91: Loss 1.4112887382507324
Model None Epoch 35 Batch 92: Loss 1.3593908548355103
Model None Epoch 35 Batch 93: Loss 1.5513591766357422
Model None Epoch 35 Batch 94: Loss 1.427140235900879
Model None Epoch 35 Batch 95: Loss 1.420134425163269
Model None Epoch 35 Batch 96: Loss 1.4983956813812256
Model None Epoch 35 Batch 97: Loss 1.387058138847351
Model None Epoch 35 Batch 98: Loss 1.3528416156768799
Model None Epoch 35 Batch 99: Loss 1.516706109046936
Downstream Train Epoch: 35 [25600/50000 (51%)]	Loss: 1.445085
Model None Epoch 35 Batch 100: Loss 1.4450852870941162
Model None Epoch 35 Batch 101: Loss 1.2679049968719482
Model None Epoch 35 Batch 102: Loss 1.4420900344848633
Model None Epoch 35 Batch 103: Loss 1.3238518238067627
Model None Epoch 35 Batch 104: Loss 1.3569608926773071
Model None Epoch 35 Batch 105: Loss 1.4263008832931519
Model None Epoch 35 Batch 106: Loss 1.4478970766067505
Model None Epoch 35 Batch 107: Loss 1.4171113967895508
Model None Epoch 35 Batch 108: Loss 1.4043153524398804
Model None Epoch 35 Batch 109: Loss 1.3675904273986816
Model None Epoch 35 Batch 110: Loss 1.4210137128829956
Model None Epoch 35 Batch 111: Loss 1.4181461334228516
Model None Epoch 35 Batch 112: Loss 1.5093146562576294
Model None Epoch 35 Batch 113: Loss 1.3898179531097412
Model None Epoch 35 Batch 114: Loss 1.4313379526138306
Model None Epoch 35 Batch 115: Loss 1.361358404159546
Model None Epoch 35 Batch 116: Loss 1.4337399005889893
Model None Epoch 35 Batch 117: Loss 1.4064794778823853
Model None Epoch 35 Batch 118: Loss 1.4407180547714233
Model None Epoch 35 Batch 119: Loss 1.4994412660598755
Model None Epoch 35 Batch 120: Loss 1.492151141166687
Model None Epoch 35 Batch 121: Loss 1.3672345876693726
Model None Epoch 35 Batch 122: Loss 1.4054551124572754
Model None Epoch 35 Batch 123: Loss 1.4718513488769531
Model None Epoch 35 Batch 124: Loss 1.4252558946609497
Model None Epoch 35 Batch 125: Loss 1.4401663541793823
Model None Epoch 35 Batch 126: Loss 1.3053765296936035
Model None Epoch 35 Batch 127: Loss 1.3792804479599
Model None Epoch 35 Batch 128: Loss 1.394533395767212
Model None Epoch 35 Batch 129: Loss 1.5073025226593018
Model None Epoch 35 Batch 130: Loss 1.3427937030792236
Model None Epoch 35 Batch 131: Loss 1.3528172969818115
Model None Epoch 35 Batch 132: Loss 1.4742060899734497
Model None Epoch 35 Batch 133: Loss 1.4242289066314697
Model None Epoch 35 Batch 134: Loss 1.468216896057129
Model None Epoch 35 Batch 135: Loss 1.395735263824463
Model None Epoch 35 Batch 136: Loss 1.468981385231018
Model None Epoch 35 Batch 137: Loss 1.4670664072036743
Model None Epoch 35 Batch 138: Loss 1.3256878852844238
Model None Epoch 35 Batch 139: Loss 1.320691704750061
Model None Epoch 35 Batch 140: Loss 1.5086593627929688
Model None Epoch 35 Batch 141: Loss 1.5088804960250854
Model None Epoch 35 Batch 142: Loss 1.2932910919189453
Model None Epoch 35 Batch 143: Loss 1.389991283416748
Model None Epoch 35 Batch 144: Loss 1.4194005727767944
Model None Epoch 35 Batch 145: Loss 1.3888400793075562
Model None Epoch 35 Batch 146: Loss 1.3224149942398071
Model None Epoch 35 Batch 147: Loss 1.3392126560211182
Model None Epoch 35 Batch 148: Loss 1.408571481704712
Model None Epoch 35 Batch 149: Loss 1.3801110982894897
Downstream Train Epoch: 35 [38400/50000 (77%)]	Loss: 1.289239
Model None Epoch 35 Batch 150: Loss 1.289238691329956
Model None Epoch 35 Batch 151: Loss 1.3675580024719238
Model None Epoch 35 Batch 152: Loss 1.4423913955688477
Model None Epoch 35 Batch 153: Loss 1.5366876125335693
Model None Epoch 35 Batch 154: Loss 1.3697320222854614
Model None Epoch 35 Batch 155: Loss 1.3736481666564941
Model None Epoch 35 Batch 156: Loss 1.469465970993042
Model None Epoch 35 Batch 157: Loss 1.4342913627624512
Model None Epoch 35 Batch 158: Loss 1.3766379356384277
Model None Epoch 35 Batch 159: Loss 1.365512490272522
Model None Epoch 35 Batch 160: Loss 1.4285963773727417
Model None Epoch 35 Batch 161: Loss 1.5090093612670898
Model None Epoch 35 Batch 162: Loss 1.5075210332870483
Model None Epoch 35 Batch 163: Loss 1.2988111972808838
Model None Epoch 35 Batch 164: Loss 1.5031911134719849
Model None Epoch 35 Batch 165: Loss 1.4822884798049927
Model None Epoch 35 Batch 166: Loss 1.3502520322799683
Model None Epoch 35 Batch 167: Loss 1.397742509841919
Model None Epoch 35 Batch 168: Loss 1.441800594329834
Model None Epoch 35 Batch 169: Loss 1.3942891359329224
Model None Epoch 35 Batch 170: Loss 1.3789870738983154
Model None Epoch 35 Batch 171: Loss 1.425100564956665
Model None Epoch 35 Batch 172: Loss 1.4653878211975098
Model None Epoch 35 Batch 173: Loss 1.487098217010498
Model None Epoch 35 Batch 174: Loss 1.3612549304962158
Model None Epoch 35 Batch 175: Loss 1.3970979452133179
Model None Epoch 35 Batch 176: Loss 1.3257020711898804
Model None Epoch 35 Batch 177: Loss 1.4131519794464111
Model None Epoch 35 Batch 178: Loss 1.358144760131836
Model None Epoch 35 Batch 179: Loss 1.4069417715072632
Model None Epoch 35 Batch 180: Loss 1.3852028846740723
Model None Epoch 35 Batch 181: Loss 1.3731906414031982
Model None Epoch 35 Batch 182: Loss 1.6567445993423462
Model None Epoch 35 Batch 183: Loss 1.4976530075073242
Model None Epoch 35 Batch 184: Loss 1.4679161310195923
Model None Epoch 35 Batch 185: Loss 1.4447872638702393
Model None Epoch 35 Batch 186: Loss 1.3757820129394531
Model None Epoch 35 Batch 187: Loss 1.5650814771652222
Model None Epoch 35 Batch 188: Loss 1.462111234664917
Model None Epoch 35 Batch 189: Loss 1.4333794116973877
Model None Epoch 35 Batch 190: Loss 1.357979416847229
Model None Epoch 35 Batch 191: Loss 1.4281368255615234
Model None Epoch 35 Batch 192: Loss 1.2746485471725464
Model None Epoch 35 Batch 193: Loss 1.336809754371643
Model None Epoch 35 Batch 194: Loss 1.422488808631897
Model None Epoch 35 Batch 195: Loss 1.1665245294570923

 Downstream Train loss: 1.4129285441369426 Acc: 0.5614
Downstream Train Epoch: 36 [0/50000 (0%)]	Loss: 1.379607
Model None Epoch 36 Batch 0: Loss 1.3796069622039795
Model None Epoch 36 Batch 1: Loss 1.350956916809082
Model None Epoch 36 Batch 2: Loss 1.3361806869506836
Model None Epoch 36 Batch 3: Loss 1.3203204870224
Model None Epoch 36 Batch 4: Loss 1.353743076324463
Model None Epoch 36 Batch 5: Loss 1.3737167119979858
Model None Epoch 36 Batch 6: Loss 1.3133786916732788
Model None Epoch 36 Batch 7: Loss 1.443604826927185
Model None Epoch 36 Batch 8: Loss 1.4276000261306763
Model None Epoch 36 Batch 9: Loss 1.444496989250183
Model None Epoch 36 Batch 10: Loss 1.439285159111023
Model None Epoch 36 Batch 11: Loss 1.4734723567962646
Model None Epoch 36 Batch 12: Loss 1.5189884901046753
Model None Epoch 36 Batch 13: Loss 1.4002774953842163
Model None Epoch 36 Batch 14: Loss 1.4872859716415405
Model None Epoch 36 Batch 15: Loss 1.5171701908111572
Model None Epoch 36 Batch 16: Loss 1.5461336374282837
Model None Epoch 36 Batch 17: Loss 1.317304253578186
Model None Epoch 36 Batch 18: Loss 1.5041948556900024
Model None Epoch 36 Batch 19: Loss 1.3406740427017212
Model None Epoch 36 Batch 20: Loss 1.3537921905517578
Model None Epoch 36 Batch 21: Loss 1.52335524559021
Model None Epoch 36 Batch 22: Loss 1.4497441053390503
Model None Epoch 36 Batch 23: Loss 1.5510224103927612
Model None Epoch 36 Batch 24: Loss 1.4249688386917114
Model None Epoch 36 Batch 25: Loss 1.3699194192886353
Model None Epoch 36 Batch 26: Loss 1.4091435670852661
Model None Epoch 36 Batch 27: Loss 1.3503596782684326
Model None Epoch 36 Batch 28: Loss 1.4261950254440308
Model None Epoch 36 Batch 29: Loss 1.352609634399414
Model None Epoch 36 Batch 30: Loss 1.3594754934310913
Model None Epoch 36 Batch 31: Loss 1.5809239149093628
Model None Epoch 36 Batch 32: Loss 1.4401288032531738
Model None Epoch 36 Batch 33: Loss 1.3972008228302002
Model None Epoch 36 Batch 34: Loss 1.3712751865386963
Model None Epoch 36 Batch 35: Loss 1.329271674156189
Model None Epoch 36 Batch 36: Loss 1.3960059881210327
Model None Epoch 36 Batch 37: Loss 1.384183645248413
Model None Epoch 36 Batch 38: Loss 1.4505274295806885
Model None Epoch 36 Batch 39: Loss 1.1140905618667603
Model None Epoch 36 Batch 40: Loss 1.4038101434707642
Model None Epoch 36 Batch 41: Loss 1.213517665863037
Model None Epoch 36 Batch 42: Loss 1.463005542755127
Model None Epoch 36 Batch 43: Loss 1.3054938316345215
Model None Epoch 36 Batch 44: Loss 1.4054964780807495
Model None Epoch 36 Batch 45: Loss 1.5110647678375244
Model None Epoch 36 Batch 46: Loss 1.410609483718872
Model None Epoch 36 Batch 47: Loss 1.4228657484054565
Model None Epoch 36 Batch 48: Loss 1.3836684226989746
Model None Epoch 36 Batch 49: Loss 1.4262646436691284
Downstream Train Epoch: 36 [12800/50000 (26%)]	Loss: 1.413163
Model None Epoch 36 Batch 50: Loss 1.4131630659103394
Model None Epoch 36 Batch 51: Loss 1.49335777759552
Model None Epoch 36 Batch 52: Loss 1.359001874923706
Model None Epoch 36 Batch 53: Loss 1.433466911315918
Model None Epoch 36 Batch 54: Loss 1.3933804035186768
Model None Epoch 36 Batch 55: Loss 1.4153757095336914
Model None Epoch 36 Batch 56: Loss 1.4360929727554321
Model None Epoch 36 Batch 57: Loss 1.4744243621826172
Model None Epoch 36 Batch 58: Loss 1.4389104843139648
Model None Epoch 36 Batch 59: Loss 1.3688055276870728
Model None Epoch 36 Batch 60: Loss 1.3403363227844238
Model None Epoch 36 Batch 61: Loss 1.4339332580566406
Model None Epoch 36 Batch 62: Loss 1.4806857109069824
Model None Epoch 36 Batch 63: Loss 1.3868306875228882
Model None Epoch 36 Batch 64: Loss 1.272417664527893
Model None Epoch 36 Batch 65: Loss 1.3313350677490234
Model None Epoch 36 Batch 66: Loss 1.3755148649215698
Model None Epoch 36 Batch 67: Loss 1.4051926136016846
Model None Epoch 36 Batch 68: Loss 1.3379807472229004
Model None Epoch 36 Batch 69: Loss 1.4569097757339478
Model None Epoch 36 Batch 70: Loss 1.4370571374893188
Model None Epoch 36 Batch 71: Loss 1.3713897466659546
Model None Epoch 36 Batch 72: Loss 1.338584303855896
Model None Epoch 36 Batch 73: Loss 1.4184894561767578
Model None Epoch 36 Batch 74: Loss 1.3355885744094849
Model None Epoch 36 Batch 75: Loss 1.2880924940109253
Model None Epoch 36 Batch 76: Loss 1.4017891883850098
Model None Epoch 36 Batch 77: Loss 1.347225546836853
Model None Epoch 36 Batch 78: Loss 1.4116103649139404
Model None Epoch 36 Batch 79: Loss 1.5056499242782593
Model None Epoch 36 Batch 80: Loss 1.448462963104248
Model None Epoch 36 Batch 81: Loss 1.4159995317459106
Model None Epoch 36 Batch 82: Loss 1.515157699584961
Model None Epoch 36 Batch 83: Loss 1.4728494882583618
Model None Epoch 36 Batch 84: Loss 1.405449390411377
Model None Epoch 36 Batch 85: Loss 1.374793291091919
Model None Epoch 36 Batch 86: Loss 1.4883472919464111
Model None Epoch 36 Batch 87: Loss 1.3278045654296875
Model None Epoch 36 Batch 88: Loss 1.4211456775665283
Model None Epoch 36 Batch 89: Loss 1.3673465251922607
Model None Epoch 36 Batch 90: Loss 1.4151959419250488
Model None Epoch 36 Batch 91: Loss 1.3785536289215088
Model None Epoch 36 Batch 92: Loss 1.3816497325897217
Model None Epoch 36 Batch 93: Loss 1.4494951963424683
Model None Epoch 36 Batch 94: Loss 1.3737989664077759
Model None Epoch 36 Batch 95: Loss 1.3964769840240479
Model None Epoch 36 Batch 96: Loss 1.4219883680343628
Model None Epoch 36 Batch 97: Loss 1.5167109966278076
Model None Epoch 36 Batch 98: Loss 1.4066271781921387
Model None Epoch 36 Batch 99: Loss 1.4038338661193848
Downstream Train Epoch: 36 [25600/50000 (51%)]	Loss: 1.364481
Model None Epoch 36 Batch 100: Loss 1.3644810914993286
Model None Epoch 36 Batch 101: Loss 1.4188028573989868
Model None Epoch 36 Batch 102: Loss 1.4726994037628174
Model None Epoch 36 Batch 103: Loss 1.295864462852478
Model None Epoch 36 Batch 104: Loss 1.4223955869674683
Model None Epoch 36 Batch 105: Loss 1.3985965251922607
Model None Epoch 36 Batch 106: Loss 1.3577117919921875
Model None Epoch 36 Batch 107: Loss 1.4474470615386963
Model None Epoch 36 Batch 108: Loss 1.4151394367218018
Model None Epoch 36 Batch 109: Loss 1.2963991165161133
Model None Epoch 36 Batch 110: Loss 1.3565105199813843
Model None Epoch 36 Batch 111: Loss 1.4493179321289062
Model None Epoch 36 Batch 112: Loss 1.391035795211792
Model None Epoch 36 Batch 113: Loss 1.3639118671417236
Model None Epoch 36 Batch 114: Loss 1.4352461099624634
Model None Epoch 36 Batch 115: Loss 1.4023699760437012
Model None Epoch 36 Batch 116: Loss 1.2718878984451294
Model None Epoch 36 Batch 117: Loss 1.4194127321243286
Model None Epoch 36 Batch 118: Loss 1.4010510444641113
Model None Epoch 36 Batch 119: Loss 1.3455792665481567
Model None Epoch 36 Batch 120: Loss 1.3739827871322632
Model None Epoch 36 Batch 121: Loss 1.374422550201416
Model None Epoch 36 Batch 122: Loss 1.3536603450775146
Model None Epoch 36 Batch 123: Loss 1.4238046407699585
Model None Epoch 36 Batch 124: Loss 1.342448353767395
Model None Epoch 36 Batch 125: Loss 1.4060722589492798
Model None Epoch 36 Batch 126: Loss 1.3946138620376587
Model None Epoch 36 Batch 127: Loss 1.455372929573059
Model None Epoch 36 Batch 128: Loss 1.3940918445587158
Model None Epoch 36 Batch 129: Loss 1.4326090812683105
Model None Epoch 36 Batch 130: Loss 1.3662742376327515
Model None Epoch 36 Batch 131: Loss 1.4309731721878052
Model None Epoch 36 Batch 132: Loss 1.437850832939148
Model None Epoch 36 Batch 133: Loss 1.3884916305541992
Model None Epoch 36 Batch 134: Loss 1.3392056226730347
Model None Epoch 36 Batch 135: Loss 1.4274306297302246
Model None Epoch 36 Batch 136: Loss 1.4174166917800903
Model None Epoch 36 Batch 137: Loss 1.3770617246627808
Model None Epoch 36 Batch 138: Loss 1.5030109882354736
Model None Epoch 36 Batch 139: Loss 1.3321492671966553
Model None Epoch 36 Batch 140: Loss 1.3354202508926392
Model None Epoch 36 Batch 141: Loss 1.3889106512069702
Model None Epoch 36 Batch 142: Loss 1.3948439359664917
Model None Epoch 36 Batch 143: Loss 1.4282541275024414
Model None Epoch 36 Batch 144: Loss 1.388132929801941
Model None Epoch 36 Batch 145: Loss 1.4411572217941284
Model None Epoch 36 Batch 146: Loss 1.4468713998794556
Model None Epoch 36 Batch 147: Loss 1.3588054180145264
Model None Epoch 36 Batch 148: Loss 1.5623029470443726
Model None Epoch 36 Batch 149: Loss 1.4188870191574097
Downstream Train Epoch: 36 [38400/50000 (77%)]	Loss: 1.478223
Model None Epoch 36 Batch 150: Loss 1.4782233238220215
Model None Epoch 36 Batch 151: Loss 1.3836071491241455
Model None Epoch 36 Batch 152: Loss 1.3057671785354614
Model None Epoch 36 Batch 153: Loss 1.4079551696777344
Model None Epoch 36 Batch 154: Loss 1.3849468231201172
Model None Epoch 36 Batch 155: Loss 1.421242117881775
Model None Epoch 36 Batch 156: Loss 1.4460262060165405
Model None Epoch 36 Batch 157: Loss 1.4298920631408691
Model None Epoch 36 Batch 158: Loss 1.357845664024353
Model None Epoch 36 Batch 159: Loss 1.4303120374679565
Model None Epoch 36 Batch 160: Loss 1.4405471086502075
Model None Epoch 36 Batch 161: Loss 1.4491429328918457
Model None Epoch 36 Batch 162: Loss 1.3477861881256104
Model None Epoch 36 Batch 163: Loss 1.4534366130828857
Model None Epoch 36 Batch 164: Loss 1.4712976217269897
Model None Epoch 36 Batch 165: Loss 1.480583667755127
Model None Epoch 36 Batch 166: Loss 1.4681168794631958
Model None Epoch 36 Batch 167: Loss 1.4121477603912354
Model None Epoch 36 Batch 168: Loss 1.3712599277496338
Model None Epoch 36 Batch 169: Loss 1.441875696182251
Model None Epoch 36 Batch 170: Loss 1.3781659603118896
Model None Epoch 36 Batch 171: Loss 1.4000574350357056
Model None Epoch 36 Batch 172: Loss 1.4159342050552368
Model None Epoch 36 Batch 173: Loss 1.39699125289917
Model None Epoch 36 Batch 174: Loss 1.496152639389038
Model None Epoch 36 Batch 175: Loss 1.4324862957000732
Model None Epoch 36 Batch 176: Loss 1.3631958961486816
Model None Epoch 36 Batch 177: Loss 1.2854516506195068
Model None Epoch 36 Batch 178: Loss 1.3968197107315063
Model None Epoch 36 Batch 179: Loss 1.4456809759140015
Model None Epoch 36 Batch 180: Loss 1.5020318031311035
Model None Epoch 36 Batch 181: Loss 1.5086593627929688
Model None Epoch 36 Batch 182: Loss 1.449724793434143
Model None Epoch 36 Batch 183: Loss 1.3262001276016235
Model None Epoch 36 Batch 184: Loss 1.4505752325057983
Model None Epoch 36 Batch 185: Loss 1.4722214937210083
Model None Epoch 36 Batch 186: Loss 1.3657132387161255
Model None Epoch 36 Batch 187: Loss 1.3290714025497437
Model None Epoch 36 Batch 188: Loss 1.4413306713104248
Model None Epoch 36 Batch 189: Loss 1.492544174194336
Model None Epoch 36 Batch 190: Loss 1.3867301940917969
Model None Epoch 36 Batch 191: Loss 1.2935771942138672
Model None Epoch 36 Batch 192: Loss 1.423116683959961
Model None Epoch 36 Batch 193: Loss 1.4569754600524902
Model None Epoch 36 Batch 194: Loss 1.489443302154541
Model None Epoch 36 Batch 195: Loss 1.410883903503418

 Downstream Train loss: 1.4061469338378128 Acc: 0.5614
Downstream Train Epoch: 37 [0/50000 (0%)]	Loss: 1.597272
Model None Epoch 37 Batch 0: Loss 1.5972719192504883
Model None Epoch 37 Batch 1: Loss 1.3620665073394775
Model None Epoch 37 Batch 2: Loss 1.3637497425079346
Model None Epoch 37 Batch 3: Loss 1.43807852268219
Model None Epoch 37 Batch 4: Loss 1.5635874271392822
Model None Epoch 37 Batch 5: Loss 1.4676114320755005
Model None Epoch 37 Batch 6: Loss 1.5044358968734741
Model None Epoch 37 Batch 7: Loss 1.409989595413208
Model None Epoch 37 Batch 8: Loss 1.5085350275039673
Model None Epoch 37 Batch 9: Loss 1.2586233615875244
Model None Epoch 37 Batch 10: Loss 1.463742733001709
Model None Epoch 37 Batch 11: Loss 1.3995901346206665
Model None Epoch 37 Batch 12: Loss 1.3023008108139038
Model None Epoch 37 Batch 13: Loss 1.3722007274627686
Model None Epoch 37 Batch 14: Loss 1.4164044857025146
Model None Epoch 37 Batch 15: Loss 1.406441569328308
Model None Epoch 37 Batch 16: Loss 1.330557942390442
Model None Epoch 37 Batch 17: Loss 1.472436785697937
Model None Epoch 37 Batch 18: Loss 1.4389417171478271
Model None Epoch 37 Batch 19: Loss 1.3522366285324097
Model None Epoch 37 Batch 20: Loss 1.4240573644638062
Model None Epoch 37 Batch 21: Loss 1.4540328979492188
Model None Epoch 37 Batch 22: Loss 1.3218449354171753
Model None Epoch 37 Batch 23: Loss 1.3444982767105103
Model None Epoch 37 Batch 24: Loss 1.3314415216445923
Model None Epoch 37 Batch 25: Loss 1.425354242324829
Model None Epoch 37 Batch 26: Loss 1.3212064504623413
Model None Epoch 37 Batch 27: Loss 1.4968421459197998
Model None Epoch 37 Batch 28: Loss 1.467736840248108
Model None Epoch 37 Batch 29: Loss 1.3673638105392456
Model None Epoch 37 Batch 30: Loss 1.3867231607437134
Model None Epoch 37 Batch 31: Loss 1.368790864944458
Model None Epoch 37 Batch 32: Loss 1.325191855430603
Model None Epoch 37 Batch 33: Loss 1.4123282432556152
Model None Epoch 37 Batch 34: Loss 1.3860021829605103
Model None Epoch 37 Batch 35: Loss 1.508378028869629
Model None Epoch 37 Batch 36: Loss 1.450276255607605
Model None Epoch 37 Batch 37: Loss 1.3404998779296875
Model None Epoch 37 Batch 38: Loss 1.3969022035598755
Model None Epoch 37 Batch 39: Loss 1.4161956310272217
Model None Epoch 37 Batch 40: Loss 1.4171408414840698
Model None Epoch 37 Batch 41: Loss 1.413084864616394
Model None Epoch 37 Batch 42: Loss 1.4544706344604492
Model None Epoch 37 Batch 43: Loss 1.439743995666504
Model None Epoch 37 Batch 44: Loss 1.519787073135376
Model None Epoch 37 Batch 45: Loss 1.3322668075561523
Model None Epoch 37 Batch 46: Loss 1.3738964796066284
Model None Epoch 37 Batch 47: Loss 1.3613778352737427
Model None Epoch 37 Batch 48: Loss 1.3819149732589722
Model None Epoch 37 Batch 49: Loss 1.3529236316680908
Downstream Train Epoch: 37 [12800/50000 (26%)]	Loss: 1.427883
Model None Epoch 37 Batch 50: Loss 1.4278826713562012
Model None Epoch 37 Batch 51: Loss 1.3405029773712158
Model None Epoch 37 Batch 52: Loss 1.3378304243087769
Model None Epoch 37 Batch 53: Loss 1.4011826515197754
Model None Epoch 37 Batch 54: Loss 1.2982914447784424
Model None Epoch 37 Batch 55: Loss 1.3171533346176147
Model None Epoch 37 Batch 56: Loss 1.4825881719589233
Model None Epoch 37 Batch 57: Loss 1.3567028045654297
Model None Epoch 37 Batch 58: Loss 1.3392021656036377
Model None Epoch 37 Batch 59: Loss 1.4121826887130737
Model None Epoch 37 Batch 60: Loss 1.3893502950668335
Model None Epoch 37 Batch 61: Loss 1.480634331703186
Model None Epoch 37 Batch 62: Loss 1.5186995267868042
Model None Epoch 37 Batch 63: Loss 1.5850244760513306
Model None Epoch 37 Batch 64: Loss 1.368181586265564
Model None Epoch 37 Batch 65: Loss 1.3310703039169312
Model None Epoch 37 Batch 66: Loss 1.3569453954696655
Model None Epoch 37 Batch 67: Loss 1.3638666868209839
Model None Epoch 37 Batch 68: Loss 1.443986177444458
Model None Epoch 37 Batch 69: Loss 1.4676549434661865
Model None Epoch 37 Batch 70: Loss 1.3216935396194458
Model None Epoch 37 Batch 71: Loss 1.4194071292877197
Model None Epoch 37 Batch 72: Loss 1.4350647926330566
Model None Epoch 37 Batch 73: Loss 1.3668372631072998
Model None Epoch 37 Batch 74: Loss 1.4769705533981323
Model None Epoch 37 Batch 75: Loss 1.4746335744857788
Model None Epoch 37 Batch 76: Loss 1.4933083057403564
Model None Epoch 37 Batch 77: Loss 1.4593323469161987
Model None Epoch 37 Batch 78: Loss 1.4614661931991577
Model None Epoch 37 Batch 79: Loss 1.5121477842330933
Model None Epoch 37 Batch 80: Loss 1.361340880393982
Model None Epoch 37 Batch 81: Loss 1.4415864944458008
Model None Epoch 37 Batch 82: Loss 1.4457029104232788
Model None Epoch 37 Batch 83: Loss 1.3476980924606323
Model None Epoch 37 Batch 84: Loss 1.3765766620635986
Model None Epoch 37 Batch 85: Loss 1.3933113813400269
Model None Epoch 37 Batch 86: Loss 1.486575722694397
Model None Epoch 37 Batch 87: Loss 1.373995304107666
Model None Epoch 37 Batch 88: Loss 1.5001697540283203
Model None Epoch 37 Batch 89: Loss 1.345886468887329
Model None Epoch 37 Batch 90: Loss 1.4505188465118408
Model None Epoch 37 Batch 91: Loss 1.2803945541381836
Model None Epoch 37 Batch 92: Loss 1.4555736780166626
Model None Epoch 37 Batch 93: Loss 1.4047044515609741
Model None Epoch 37 Batch 94: Loss 1.3704766035079956
Model None Epoch 37 Batch 95: Loss 1.435147762298584
Model None Epoch 37 Batch 96: Loss 1.4247039556503296
Model None Epoch 37 Batch 97: Loss 1.414033055305481
Model None Epoch 37 Batch 98: Loss 1.4906755685806274
Model None Epoch 37 Batch 99: Loss 1.4140386581420898
Downstream Train Epoch: 37 [25600/50000 (51%)]	Loss: 1.356862
Model None Epoch 37 Batch 100: Loss 1.3568620681762695
Model None Epoch 37 Batch 101: Loss 1.40817129611969
Model None Epoch 37 Batch 102: Loss 1.3490045070648193
Model None Epoch 37 Batch 103: Loss 1.5878818035125732
Model None Epoch 37 Batch 104: Loss 1.429386854171753
Model None Epoch 37 Batch 105: Loss 1.3814562559127808
Model None Epoch 37 Batch 106: Loss 1.563977599143982
Model None Epoch 37 Batch 107: Loss 1.4150331020355225
Model None Epoch 37 Batch 108: Loss 1.3391273021697998
Model None Epoch 37 Batch 109: Loss 1.4211758375167847
Model None Epoch 37 Batch 110: Loss 1.3540594577789307
Model None Epoch 37 Batch 111: Loss 1.3953561782836914
Model None Epoch 37 Batch 112: Loss 1.4787856340408325
Model None Epoch 37 Batch 113: Loss 1.4816834926605225
Model None Epoch 37 Batch 114: Loss 1.4004881381988525
Model None Epoch 37 Batch 115: Loss 1.3501750230789185
Model None Epoch 37 Batch 116: Loss 1.4193366765975952
Model None Epoch 37 Batch 117: Loss 1.4962204694747925
Model None Epoch 37 Batch 118: Loss 1.3853627443313599
Model None Epoch 37 Batch 119: Loss 1.3857295513153076
Model None Epoch 37 Batch 120: Loss 1.4274334907531738
Model None Epoch 37 Batch 121: Loss 1.3503857851028442
Model None Epoch 37 Batch 122: Loss 1.4944021701812744
Model None Epoch 37 Batch 123: Loss 1.5007551908493042
Model None Epoch 37 Batch 124: Loss 1.3791126012802124
Model None Epoch 37 Batch 125: Loss 1.4375196695327759
Model None Epoch 37 Batch 126: Loss 1.3712798357009888
Model None Epoch 37 Batch 127: Loss 1.3935555219650269
Model None Epoch 37 Batch 128: Loss 1.4437335729599
Model None Epoch 37 Batch 129: Loss 1.3561780452728271
Model None Epoch 37 Batch 130: Loss 1.4154367446899414
Model None Epoch 37 Batch 131: Loss 1.4042199850082397
Model None Epoch 37 Batch 132: Loss 1.4018601179122925
Model None Epoch 37 Batch 133: Loss 1.4236100912094116
Model None Epoch 37 Batch 134: Loss 1.5020208358764648
Model None Epoch 37 Batch 135: Loss 1.4958707094192505
Model None Epoch 37 Batch 136: Loss 1.488433837890625
Model None Epoch 37 Batch 137: Loss 1.4040296077728271
Model None Epoch 37 Batch 138: Loss 1.204789400100708
Model None Epoch 37 Batch 139: Loss 1.356170654296875
Model None Epoch 37 Batch 140: Loss 1.504136085510254
Model None Epoch 37 Batch 141: Loss 1.38248610496521
Model None Epoch 37 Batch 142: Loss 1.4828225374221802
Model None Epoch 37 Batch 143: Loss 1.3112382888793945
Model None Epoch 37 Batch 144: Loss 1.3917405605316162
Model None Epoch 37 Batch 145: Loss 1.476768136024475
Model None Epoch 37 Batch 146: Loss 1.4917994737625122
Model None Epoch 37 Batch 147: Loss 1.4500081539154053
Model None Epoch 37 Batch 148: Loss 1.399671196937561
Model None Epoch 37 Batch 149: Loss 1.4419409036636353
Downstream Train Epoch: 37 [38400/50000 (77%)]	Loss: 1.424828
Model None Epoch 37 Batch 150: Loss 1.424828290939331
Model None Epoch 37 Batch 151: Loss 1.3261648416519165
Model None Epoch 37 Batch 152: Loss 1.4285968542099
Model None Epoch 37 Batch 153: Loss 1.3299797773361206
Model None Epoch 37 Batch 154: Loss 1.4226070642471313
Model None Epoch 37 Batch 155: Loss 1.323786735534668
Model None Epoch 37 Batch 156: Loss 1.430525779724121
Model None Epoch 37 Batch 157: Loss 1.5181719064712524
Model None Epoch 37 Batch 158: Loss 1.3799556493759155
Model None Epoch 37 Batch 159: Loss 1.5272029638290405
Model None Epoch 37 Batch 160: Loss 1.3230688571929932
Model None Epoch 37 Batch 161: Loss 1.4468069076538086
Model None Epoch 37 Batch 162: Loss 1.3624039888381958
Model None Epoch 37 Batch 163: Loss 1.399032473564148
Model None Epoch 37 Batch 164: Loss 1.3309850692749023
Model None Epoch 37 Batch 165: Loss 1.4357187747955322
Model None Epoch 37 Batch 166: Loss 1.4805364608764648
Model None Epoch 37 Batch 167: Loss 1.395920753479004
Model None Epoch 37 Batch 168: Loss 1.4366672039031982
Model None Epoch 37 Batch 169: Loss 1.3219772577285767
Model None Epoch 37 Batch 170: Loss 1.4116278886795044
Model None Epoch 37 Batch 171: Loss 1.5337053537368774
Model None Epoch 37 Batch 172: Loss 1.41890287399292
Model None Epoch 37 Batch 173: Loss 1.4094276428222656
Model None Epoch 37 Batch 174: Loss 1.4234517812728882
Model None Epoch 37 Batch 175: Loss 1.3839812278747559
Model None Epoch 37 Batch 176: Loss 1.3452603816986084
Model None Epoch 37 Batch 177: Loss 1.54082190990448
Model None Epoch 37 Batch 178: Loss 1.4678274393081665
Model None Epoch 37 Batch 179: Loss 1.310502529144287
Model None Epoch 37 Batch 180: Loss 1.4232964515686035
Model None Epoch 37 Batch 181: Loss 1.3084222078323364
Model None Epoch 37 Batch 182: Loss 1.3780914545059204
Model None Epoch 37 Batch 183: Loss 1.4895654916763306
Model None Epoch 37 Batch 184: Loss 1.4244797229766846
Model None Epoch 37 Batch 185: Loss 1.4325836896896362
Model None Epoch 37 Batch 186: Loss 1.3720980882644653
Model None Epoch 37 Batch 187: Loss 1.4636949300765991
Model None Epoch 37 Batch 188: Loss 1.426176905632019
Model None Epoch 37 Batch 189: Loss 1.4262335300445557
Model None Epoch 37 Batch 190: Loss 1.3289674520492554
Model None Epoch 37 Batch 191: Loss 1.3742989301681519
Model None Epoch 37 Batch 192: Loss 1.5395994186401367
Model None Epoch 37 Batch 193: Loss 1.403907060623169
Model None Epoch 37 Batch 194: Loss 1.5528607368469238
Model None Epoch 37 Batch 195: Loss 1.4917173385620117

 Downstream Train loss: 1.4141995183059148 Acc: 0.5614
Downstream Train Epoch: 38 [0/50000 (0%)]	Loss: 1.378597
Model None Epoch 38 Batch 0: Loss 1.3785966634750366
Model None Epoch 38 Batch 1: Loss 1.425337791442871
Model None Epoch 38 Batch 2: Loss 1.4899303913116455
Model None Epoch 38 Batch 3: Loss 1.4498099088668823
Model None Epoch 38 Batch 4: Loss 1.366416335105896
Model None Epoch 38 Batch 5: Loss 1.3527761697769165
Model None Epoch 38 Batch 6: Loss 1.511508822441101
Model None Epoch 38 Batch 7: Loss 1.457190990447998
Model None Epoch 38 Batch 8: Loss 1.3731086254119873
Model None Epoch 38 Batch 9: Loss 1.4293808937072754
Model None Epoch 38 Batch 10: Loss 1.579534649848938
Model None Epoch 38 Batch 11: Loss 1.4967007637023926
Model None Epoch 38 Batch 12: Loss 1.4080138206481934
Model None Epoch 38 Batch 13: Loss 1.338426947593689
Model None Epoch 38 Batch 14: Loss 1.2790329456329346
Model None Epoch 38 Batch 15: Loss 1.3189034461975098
Model None Epoch 38 Batch 16: Loss 1.4642993211746216
Model None Epoch 38 Batch 17: Loss 1.4067332744598389
Model None Epoch 38 Batch 18: Loss 1.454188346862793
Model None Epoch 38 Batch 19: Loss 1.4190759658813477
Model None Epoch 38 Batch 20: Loss 1.385485053062439
Model None Epoch 38 Batch 21: Loss 1.3548322916030884
Model None Epoch 38 Batch 22: Loss 1.5249873399734497
Model None Epoch 38 Batch 23: Loss 1.3603582382202148
Model None Epoch 38 Batch 24: Loss 1.4624122381210327
Model None Epoch 38 Batch 25: Loss 1.3655184507369995
Model None Epoch 38 Batch 26: Loss 1.2614154815673828
Model None Epoch 38 Batch 27: Loss 1.3849605321884155
Model None Epoch 38 Batch 28: Loss 1.4625998735427856
Model None Epoch 38 Batch 29: Loss 1.3535537719726562
Model None Epoch 38 Batch 30: Loss 1.4113975763320923
Model None Epoch 38 Batch 31: Loss 1.4475005865097046
Model None Epoch 38 Batch 32: Loss 1.520045518875122
Model None Epoch 38 Batch 33: Loss 1.4334092140197754
Model None Epoch 38 Batch 34: Loss 1.4680427312850952
Model None Epoch 38 Batch 35: Loss 1.3408275842666626
Model None Epoch 38 Batch 36: Loss 1.3345171213150024
Model None Epoch 38 Batch 37: Loss 1.2986314296722412
Model None Epoch 38 Batch 38: Loss 1.3099274635314941
Model None Epoch 38 Batch 39: Loss 1.3907439708709717
Model None Epoch 38 Batch 40: Loss 1.4868049621582031
Model None Epoch 38 Batch 41: Loss 1.4058518409729004
Model None Epoch 38 Batch 42: Loss 1.4546746015548706
Model None Epoch 38 Batch 43: Loss 1.2808161973953247
Model None Epoch 38 Batch 44: Loss 1.3321908712387085
Model None Epoch 38 Batch 45: Loss 1.5423634052276611
Model None Epoch 38 Batch 46: Loss 1.4182026386260986
Model None Epoch 38 Batch 47: Loss 1.4693338871002197
Model None Epoch 38 Batch 48: Loss 1.5619006156921387
Model None Epoch 38 Batch 49: Loss 1.5078009366989136
Downstream Train Epoch: 38 [12800/50000 (26%)]	Loss: 1.438291
Model None Epoch 38 Batch 50: Loss 1.4382908344268799
Model None Epoch 38 Batch 51: Loss 1.3917449712753296
Model None Epoch 38 Batch 52: Loss 1.264975666999817
Model None Epoch 38 Batch 53: Loss 1.4565486907958984
Model None Epoch 38 Batch 54: Loss 1.4676562547683716
Model None Epoch 38 Batch 55: Loss 1.417409896850586
Model None Epoch 38 Batch 56: Loss 1.481312870979309
Model None Epoch 38 Batch 57: Loss 1.39243483543396
Model None Epoch 38 Batch 58: Loss 1.413129448890686
Model None Epoch 38 Batch 59: Loss 1.3266615867614746
Model None Epoch 38 Batch 60: Loss 1.5188522338867188
Model None Epoch 38 Batch 61: Loss 1.4918266534805298
Model None Epoch 38 Batch 62: Loss 1.3893126249313354
Model None Epoch 38 Batch 63: Loss 1.3466691970825195
Model None Epoch 38 Batch 64: Loss 1.3849855661392212
Model None Epoch 38 Batch 65: Loss 1.4508110284805298
Model None Epoch 38 Batch 66: Loss 1.428991436958313
Model None Epoch 38 Batch 67: Loss 1.459105372428894
Model None Epoch 38 Batch 68: Loss 1.4353668689727783
Model None Epoch 38 Batch 69: Loss 1.4309871196746826
Model None Epoch 38 Batch 70: Loss 1.4158344268798828
Model None Epoch 38 Batch 71: Loss 1.4892479181289673
Model None Epoch 38 Batch 72: Loss 1.3310242891311646
Model None Epoch 38 Batch 73: Loss 1.3939021825790405
Model None Epoch 38 Batch 74: Loss 1.4390705823898315
Model None Epoch 38 Batch 75: Loss 1.361689567565918
Model None Epoch 38 Batch 76: Loss 1.428735375404358
Model None Epoch 38 Batch 77: Loss 1.50093674659729
Model None Epoch 38 Batch 78: Loss 1.428592562675476
Model None Epoch 38 Batch 79: Loss 1.4677988290786743
Model None Epoch 38 Batch 80: Loss 1.372794508934021
Model None Epoch 38 Batch 81: Loss 1.4123564958572388
Model None Epoch 38 Batch 82: Loss 1.4029555320739746
Model None Epoch 38 Batch 83: Loss 1.376932144165039
Model None Epoch 38 Batch 84: Loss 1.4390616416931152
Model None Epoch 38 Batch 85: Loss 1.3685067892074585
Model None Epoch 38 Batch 86: Loss 1.3893457651138306
Model None Epoch 38 Batch 87: Loss 1.448204755783081
Model None Epoch 38 Batch 88: Loss 1.4115577936172485
Model None Epoch 38 Batch 89: Loss 1.412964940071106
Model None Epoch 38 Batch 90: Loss 1.3891575336456299
Model None Epoch 38 Batch 91: Loss 1.3564105033874512
Model None Epoch 38 Batch 92: Loss 1.335394263267517
Model None Epoch 38 Batch 93: Loss 1.4951871633529663
Model None Epoch 38 Batch 94: Loss 1.4587844610214233
Model None Epoch 38 Batch 95: Loss 1.348191261291504
Model None Epoch 38 Batch 96: Loss 1.4430850744247437
Model None Epoch 38 Batch 97: Loss 1.440531849861145
Model None Epoch 38 Batch 98: Loss 1.5080606937408447
Model None Epoch 38 Batch 99: Loss 1.464563250541687
Downstream Train Epoch: 38 [25600/50000 (51%)]	Loss: 1.453430
Model None Epoch 38 Batch 100: Loss 1.453430414199829
Model None Epoch 38 Batch 101: Loss 1.4711008071899414
Model None Epoch 38 Batch 102: Loss 1.3915764093399048
Model None Epoch 38 Batch 103: Loss 1.3921878337860107
Model None Epoch 38 Batch 104: Loss 1.4171814918518066
Model None Epoch 38 Batch 105: Loss 1.362342119216919
Model None Epoch 38 Batch 106: Loss 1.4662253856658936
Model None Epoch 38 Batch 107: Loss 1.368475079536438
Model None Epoch 38 Batch 108: Loss 1.5644389390945435
Model None Epoch 38 Batch 109: Loss 1.4247925281524658
Model None Epoch 38 Batch 110: Loss 1.4362157583236694
Model None Epoch 38 Batch 111: Loss 1.3940778970718384
Model None Epoch 38 Batch 112: Loss 1.4734727144241333
Model None Epoch 38 Batch 113: Loss 1.4714082479476929
Model None Epoch 38 Batch 114: Loss 1.4128962755203247
Model None Epoch 38 Batch 115: Loss 1.3244682550430298
Model None Epoch 38 Batch 116: Loss 1.3026081323623657
Model None Epoch 38 Batch 117: Loss 1.4181755781173706
Model None Epoch 38 Batch 118: Loss 1.4134701490402222
Model None Epoch 38 Batch 119: Loss 1.4546082019805908
Model None Epoch 38 Batch 120: Loss 1.4012454748153687
Model None Epoch 38 Batch 121: Loss 1.2864919900894165
Model None Epoch 38 Batch 122: Loss 1.3793056011199951
Model None Epoch 38 Batch 123: Loss 1.3970280885696411
Model None Epoch 38 Batch 124: Loss 1.3692615032196045
Model None Epoch 38 Batch 125: Loss 1.4143116474151611
Model None Epoch 38 Batch 126: Loss 1.4923464059829712
Model None Epoch 38 Batch 127: Loss 1.5277009010314941
Model None Epoch 38 Batch 128: Loss 1.5936131477355957
Model None Epoch 38 Batch 129: Loss 1.3434478044509888
Model None Epoch 38 Batch 130: Loss 1.3736323118209839
Model None Epoch 38 Batch 131: Loss 1.35499107837677
Model None Epoch 38 Batch 132: Loss 1.3474302291870117
Model None Epoch 38 Batch 133: Loss 1.5595641136169434
Model None Epoch 38 Batch 134: Loss 1.4986944198608398
Model None Epoch 38 Batch 135: Loss 1.471744418144226
Model None Epoch 38 Batch 136: Loss 1.384576439857483
Model None Epoch 38 Batch 137: Loss 1.3816519975662231
Model None Epoch 38 Batch 138: Loss 1.4928468465805054
Model None Epoch 38 Batch 139: Loss 1.344973087310791
Model None Epoch 38 Batch 140: Loss 1.3427958488464355
Model None Epoch 38 Batch 141: Loss 1.4903433322906494
Model None Epoch 38 Batch 142: Loss 1.4324084520339966
Model None Epoch 38 Batch 143: Loss 1.465569019317627
Model None Epoch 38 Batch 144: Loss 1.3981410264968872
Model None Epoch 38 Batch 145: Loss 1.5189021825790405
Model None Epoch 38 Batch 146: Loss 1.4323720932006836
Model None Epoch 38 Batch 147: Loss 1.5174745321273804
Model None Epoch 38 Batch 148: Loss 1.3777152299880981
Model None Epoch 38 Batch 149: Loss 1.2905137538909912
Downstream Train Epoch: 38 [38400/50000 (77%)]	Loss: 1.459171
Model None Epoch 38 Batch 150: Loss 1.459171175956726
Model None Epoch 38 Batch 151: Loss 1.4429028034210205
Model None Epoch 38 Batch 152: Loss 1.4149311780929565
Model None Epoch 38 Batch 153: Loss 1.4061399698257446
Model None Epoch 38 Batch 154: Loss 1.318212628364563
Model None Epoch 38 Batch 155: Loss 1.3902771472930908
Model None Epoch 38 Batch 156: Loss 1.2046422958374023
Model None Epoch 38 Batch 157: Loss 1.4213465452194214
Model None Epoch 38 Batch 158: Loss 1.4159916639328003
Model None Epoch 38 Batch 159: Loss 1.359169363975525
Model None Epoch 38 Batch 160: Loss 1.4171425104141235
Model None Epoch 38 Batch 161: Loss 1.3729974031448364
Model None Epoch 38 Batch 162: Loss 1.5736855268478394
Model None Epoch 38 Batch 163: Loss 1.4642059803009033
Model None Epoch 38 Batch 164: Loss 1.4001425504684448
Model None Epoch 38 Batch 165: Loss 1.4231789112091064
Model None Epoch 38 Batch 166: Loss 1.3746731281280518
Model None Epoch 38 Batch 167: Loss 1.4346660375595093
Model None Epoch 38 Batch 168: Loss 1.2414764165878296
Model None Epoch 38 Batch 169: Loss 1.4737869501113892
Model None Epoch 38 Batch 170: Loss 1.3106534481048584
Model None Epoch 38 Batch 171: Loss 1.4609228372573853
Model None Epoch 38 Batch 172: Loss 1.4666582345962524
Model None Epoch 38 Batch 173: Loss 1.3452869653701782
Model None Epoch 38 Batch 174: Loss 1.3394768238067627
Model None Epoch 38 Batch 175: Loss 1.452610731124878
Model None Epoch 38 Batch 176: Loss 1.404512882232666
Model None Epoch 38 Batch 177: Loss 1.5202726125717163
Model None Epoch 38 Batch 178: Loss 1.486634373664856
Model None Epoch 38 Batch 179: Loss 1.4659382104873657
Model None Epoch 38 Batch 180: Loss 1.4514375925064087
Model None Epoch 38 Batch 181: Loss 1.5071302652359009
Model None Epoch 38 Batch 182: Loss 1.5410306453704834
Model None Epoch 38 Batch 183: Loss 1.375435471534729
Model None Epoch 38 Batch 184: Loss 1.437120795249939
Model None Epoch 38 Batch 185: Loss 1.412213683128357
Model None Epoch 38 Batch 186: Loss 1.3841063976287842
Model None Epoch 38 Batch 187: Loss 1.4623627662658691
Model None Epoch 38 Batch 188: Loss 1.4480102062225342
Model None Epoch 38 Batch 189: Loss 1.359519362449646
Model None Epoch 38 Batch 190: Loss 1.516758918762207
Model None Epoch 38 Batch 191: Loss 1.4531124830245972
Model None Epoch 38 Batch 192: Loss 1.3996782302856445
Model None Epoch 38 Batch 193: Loss 1.3312329053878784
Model None Epoch 38 Batch 194: Loss 1.4694017171859741
Model None Epoch 38 Batch 195: Loss 1.502018690109253

 Downstream Train loss: 1.4186456489319703 Acc: 0.5614
Downstream Train Epoch: 39 [0/50000 (0%)]	Loss: 1.427283
Model None Epoch 39 Batch 0: Loss 1.4272831678390503
Model None Epoch 39 Batch 1: Loss 1.3888068199157715
Model None Epoch 39 Batch 2: Loss 1.2872880697250366
Model None Epoch 39 Batch 3: Loss 1.4243683815002441
Model None Epoch 39 Batch 4: Loss 1.4660922288894653
Model None Epoch 39 Batch 5: Loss 1.2977979183197021
Model None Epoch 39 Batch 6: Loss 1.513602614402771
Model None Epoch 39 Batch 7: Loss 1.4895697832107544
Model None Epoch 39 Batch 8: Loss 1.3470427989959717
Model None Epoch 39 Batch 9: Loss 1.3724125623703003
Model None Epoch 39 Batch 10: Loss 1.5417561531066895
Model None Epoch 39 Batch 11: Loss 1.3416084051132202
Model None Epoch 39 Batch 12: Loss 1.421337366104126
Model None Epoch 39 Batch 13: Loss 1.3001384735107422
Model None Epoch 39 Batch 14: Loss 1.4455208778381348
Model None Epoch 39 Batch 15: Loss 1.3817763328552246
Model None Epoch 39 Batch 16: Loss 1.5602316856384277
Model None Epoch 39 Batch 17: Loss 1.3595706224441528
Model None Epoch 39 Batch 18: Loss 1.4293272495269775
Model None Epoch 39 Batch 19: Loss 1.404846429824829
Model None Epoch 39 Batch 20: Loss 1.4718421697616577
Model None Epoch 39 Batch 21: Loss 1.2961816787719727
Model None Epoch 39 Batch 22: Loss 1.417669653892517
Model None Epoch 39 Batch 23: Loss 1.4731600284576416
Model None Epoch 39 Batch 24: Loss 1.3842074871063232
Model None Epoch 39 Batch 25: Loss 1.5385135412216187
Model None Epoch 39 Batch 26: Loss 1.478572964668274
Model None Epoch 39 Batch 27: Loss 1.430867314338684
Model None Epoch 39 Batch 28: Loss 1.3861286640167236
Model None Epoch 39 Batch 29: Loss 1.4550116062164307
Model None Epoch 39 Batch 30: Loss 1.4416714906692505
Model None Epoch 39 Batch 31: Loss 1.409826636314392
Model None Epoch 39 Batch 32: Loss 1.406143307685852
Model None Epoch 39 Batch 33: Loss 1.296724796295166
Model None Epoch 39 Batch 34: Loss 1.47581946849823
Model None Epoch 39 Batch 35: Loss 1.4235748052597046
Model None Epoch 39 Batch 36: Loss 1.4006110429763794
Model None Epoch 39 Batch 37: Loss 1.4314762353897095
Model None Epoch 39 Batch 38: Loss 1.4384616613388062
Model None Epoch 39 Batch 39: Loss 1.38528311252594
Model None Epoch 39 Batch 40: Loss 1.4168075323104858
Model None Epoch 39 Batch 41: Loss 1.5502814054489136
Model None Epoch 39 Batch 42: Loss 1.5704783201217651
Model None Epoch 39 Batch 43: Loss 1.401983618736267
Model None Epoch 39 Batch 44: Loss 1.2946730852127075
Model None Epoch 39 Batch 45: Loss 1.4594801664352417
Model None Epoch 39 Batch 46: Loss 1.3466551303863525
Model None Epoch 39 Batch 47: Loss 1.338834285736084
Model None Epoch 39 Batch 48: Loss 1.3552794456481934
Model None Epoch 39 Batch 49: Loss 1.3718132972717285
Downstream Train Epoch: 39 [12800/50000 (26%)]	Loss: 1.481455
Model None Epoch 39 Batch 50: Loss 1.4814547300338745
Model None Epoch 39 Batch 51: Loss 1.41525399684906
Model None Epoch 39 Batch 52: Loss 1.448571801185608
Model None Epoch 39 Batch 53: Loss 1.2586511373519897
Model None Epoch 39 Batch 54: Loss 1.3937650918960571
Model None Epoch 39 Batch 55: Loss 1.4410704374313354
Model None Epoch 39 Batch 56: Loss 1.4331134557724
Model None Epoch 39 Batch 57: Loss 1.4772261381149292
Model None Epoch 39 Batch 58: Loss 1.3535884618759155
Model None Epoch 39 Batch 59: Loss 1.6000957489013672
Model None Epoch 39 Batch 60: Loss 1.3373759984970093
Model None Epoch 39 Batch 61: Loss 1.4119502305984497
Model None Epoch 39 Batch 62: Loss 1.4314624071121216
Model None Epoch 39 Batch 63: Loss 1.4997915029525757
Model None Epoch 39 Batch 64: Loss 1.3614383935928345
Model None Epoch 39 Batch 65: Loss 1.385902762413025
Model None Epoch 39 Batch 66: Loss 1.4667456150054932
Model None Epoch 39 Batch 67: Loss 1.4295481443405151
Model None Epoch 39 Batch 68: Loss 1.4260348081588745
Model None Epoch 39 Batch 69: Loss 1.4057387113571167
Model None Epoch 39 Batch 70: Loss 1.3757729530334473
Model None Epoch 39 Batch 71: Loss 1.4135315418243408
Model None Epoch 39 Batch 72: Loss 1.343288540840149
Model None Epoch 39 Batch 73: Loss 1.4533827304840088
Model None Epoch 39 Batch 74: Loss 1.5092512369155884
Model None Epoch 39 Batch 75: Loss 1.5090434551239014
Model None Epoch 39 Batch 76: Loss 1.3350454568862915
Model None Epoch 39 Batch 77: Loss 1.2794406414031982
Model None Epoch 39 Batch 78: Loss 1.4182661771774292
Model None Epoch 39 Batch 79: Loss 1.4489924907684326
Model None Epoch 39 Batch 80: Loss 1.3440954685211182
Model None Epoch 39 Batch 81: Loss 1.5081154108047485
Model None Epoch 39 Batch 82: Loss 1.4521137475967407
Model None Epoch 39 Batch 83: Loss 1.3678385019302368
Model None Epoch 39 Batch 84: Loss 1.3647617101669312
Model None Epoch 39 Batch 85: Loss 1.4249922037124634
Model None Epoch 39 Batch 86: Loss 1.425039529800415
Model None Epoch 39 Batch 87: Loss 1.3858405351638794
Model None Epoch 39 Batch 88: Loss 1.420076847076416
Model None Epoch 39 Batch 89: Loss 1.337238073348999
Model None Epoch 39 Batch 90: Loss 1.5456117391586304
Model None Epoch 39 Batch 91: Loss 1.4477906227111816
Model None Epoch 39 Batch 92: Loss 1.3888912200927734
Model None Epoch 39 Batch 93: Loss 1.5956103801727295
Model None Epoch 39 Batch 94: Loss 1.3794435262680054
Model None Epoch 39 Batch 95: Loss 1.4343246221542358
Model None Epoch 39 Batch 96: Loss 1.497469186782837
Model None Epoch 39 Batch 97: Loss 1.3751994371414185
Model None Epoch 39 Batch 98: Loss 1.4108418226242065
Model None Epoch 39 Batch 99: Loss 1.34456205368042
Downstream Train Epoch: 39 [25600/50000 (51%)]	Loss: 1.468045
Model None Epoch 39 Batch 100: Loss 1.4680447578430176
Model None Epoch 39 Batch 101: Loss 1.3999736309051514
Model None Epoch 39 Batch 102: Loss 1.4383881092071533
Model None Epoch 39 Batch 103: Loss 1.4114251136779785
Model None Epoch 39 Batch 104: Loss 1.3626680374145508
Model None Epoch 39 Batch 105: Loss 1.4225304126739502
Model None Epoch 39 Batch 106: Loss 1.3859946727752686
Model None Epoch 39 Batch 107: Loss 1.4383875131607056
Model None Epoch 39 Batch 108: Loss 1.3383680582046509
Model None Epoch 39 Batch 109: Loss 1.4488306045532227
Model None Epoch 39 Batch 110: Loss 1.4524176120758057
Model None Epoch 39 Batch 111: Loss 1.4137626886367798
Model None Epoch 39 Batch 112: Loss 1.4066742658615112
Model None Epoch 39 Batch 113: Loss 1.423604965209961
Model None Epoch 39 Batch 114: Loss 1.422202229499817
Model None Epoch 39 Batch 115: Loss 1.3706345558166504
Model None Epoch 39 Batch 116: Loss 1.3218549489974976
Model None Epoch 39 Batch 117: Loss 1.3462597131729126
Model None Epoch 39 Batch 118: Loss 1.484006643295288
Model None Epoch 39 Batch 119: Loss 1.4254862070083618
Model None Epoch 39 Batch 120: Loss 1.3376070261001587
Model None Epoch 39 Batch 121: Loss 1.4111407995224
Model None Epoch 39 Batch 122: Loss 1.3380898237228394
Model None Epoch 39 Batch 123: Loss 1.4593957662582397
Model None Epoch 39 Batch 124: Loss 1.5382598638534546
Model None Epoch 39 Batch 125: Loss 1.4307957887649536
Model None Epoch 39 Batch 126: Loss 1.2693015336990356
Model None Epoch 39 Batch 127: Loss 1.3848841190338135
Model None Epoch 39 Batch 128: Loss 1.39468514919281
Model None Epoch 39 Batch 129: Loss 1.4107739925384521
Model None Epoch 39 Batch 130: Loss 1.3696680068969727
Model None Epoch 39 Batch 131: Loss 1.3737019300460815
Model None Epoch 39 Batch 132: Loss 1.424148678779602
Model None Epoch 39 Batch 133: Loss 1.3388996124267578
Model None Epoch 39 Batch 134: Loss 1.3750053644180298
Model None Epoch 39 Batch 135: Loss 1.43429696559906
Model None Epoch 39 Batch 136: Loss 1.3616371154785156
Model None Epoch 39 Batch 137: Loss 1.277405023574829
Model None Epoch 39 Batch 138: Loss 1.4884601831436157
Model None Epoch 39 Batch 139: Loss 1.4385141134262085
Model None Epoch 39 Batch 140: Loss 1.242016077041626
Model None Epoch 39 Batch 141: Loss 1.5137401819229126
Model None Epoch 39 Batch 142: Loss 1.3866868019104004
Model None Epoch 39 Batch 143: Loss 1.2854164838790894
Model None Epoch 39 Batch 144: Loss 1.4958062171936035
Model None Epoch 39 Batch 145: Loss 1.5143005847930908
Model None Epoch 39 Batch 146: Loss 1.4511055946350098
Model None Epoch 39 Batch 147: Loss 1.378216028213501
Model None Epoch 39 Batch 148: Loss 1.358735203742981
Model None Epoch 39 Batch 149: Loss 1.3524904251098633
Downstream Train Epoch: 39 [38400/50000 (77%)]	Loss: 1.477019
Model None Epoch 39 Batch 150: Loss 1.4770185947418213
Model None Epoch 39 Batch 151: Loss 1.4477055072784424
Model None Epoch 39 Batch 152: Loss 1.3945449590682983
Model None Epoch 39 Batch 153: Loss 1.3436452150344849
Model None Epoch 39 Batch 154: Loss 1.3838634490966797
Model None Epoch 39 Batch 155: Loss 1.3483166694641113
Model None Epoch 39 Batch 156: Loss 1.3867133855819702
Model None Epoch 39 Batch 157: Loss 1.4345884323120117
Model None Epoch 39 Batch 158: Loss 1.3976857662200928
Model None Epoch 39 Batch 159: Loss 1.4962855577468872
Model None Epoch 39 Batch 160: Loss 1.4126777648925781
Model None Epoch 39 Batch 161: Loss 1.395423412322998
Model None Epoch 39 Batch 162: Loss 1.4100371599197388
Model None Epoch 39 Batch 163: Loss 1.300191879272461
Model None Epoch 39 Batch 164: Loss 1.463409662246704
Model None Epoch 39 Batch 165: Loss 1.400042176246643
Model None Epoch 39 Batch 166: Loss 1.4126741886138916
Model None Epoch 39 Batch 167: Loss 1.4117213487625122
Model None Epoch 39 Batch 168: Loss 1.3733683824539185
Model None Epoch 39 Batch 169: Loss 1.4921950101852417
Model None Epoch 39 Batch 170: Loss 1.3523634672164917
Model None Epoch 39 Batch 171: Loss 1.5667537450790405
Model None Epoch 39 Batch 172: Loss 1.4349020719528198
Model None Epoch 39 Batch 173: Loss 1.5124119520187378
Model None Epoch 39 Batch 174: Loss 1.4051883220672607
Model None Epoch 39 Batch 175: Loss 1.358792781829834
Model None Epoch 39 Batch 176: Loss 1.3016300201416016
Model None Epoch 39 Batch 177: Loss 1.3477321863174438
Model None Epoch 39 Batch 178: Loss 1.4599039554595947
Model None Epoch 39 Batch 179: Loss 1.395122766494751
Model None Epoch 39 Batch 180: Loss 1.4496874809265137
Model None Epoch 39 Batch 181: Loss 1.4107427597045898
Model None Epoch 39 Batch 182: Loss 1.3631658554077148
Model None Epoch 39 Batch 183: Loss 1.4916093349456787
Model None Epoch 39 Batch 184: Loss 1.4242652654647827
Model None Epoch 39 Batch 185: Loss 1.4280213117599487
Model None Epoch 39 Batch 186: Loss 1.4541411399841309
Model None Epoch 39 Batch 187: Loss 1.4271553754806519
Model None Epoch 39 Batch 188: Loss 1.504341721534729
Model None Epoch 39 Batch 189: Loss 1.4271156787872314
Model None Epoch 39 Batch 190: Loss 1.4265308380126953
Model None Epoch 39 Batch 191: Loss 1.3869194984436035
Model None Epoch 39 Batch 192: Loss 1.2978582382202148
Model None Epoch 39 Batch 193: Loss 1.454786777496338
Model None Epoch 39 Batch 194: Loss 1.3495256900787354
Model None Epoch 39 Batch 195: Loss 1.3983383178710938

 Downstream Train loss: 1.4120963142842662 Acc: 0.5614
Downstream Train Epoch: 40 [0/50000 (0%)]	Loss: 1.279989
Model None Epoch 40 Batch 0: Loss 1.2799887657165527
Model None Epoch 40 Batch 1: Loss 1.4896923303604126
Model None Epoch 40 Batch 2: Loss 1.4864109754562378
Model None Epoch 40 Batch 3: Loss 1.425075888633728
Model None Epoch 40 Batch 4: Loss 1.4455913305282593
Model None Epoch 40 Batch 5: Loss 1.2577157020568848
Model None Epoch 40 Batch 6: Loss 1.3185973167419434
Model None Epoch 40 Batch 7: Loss 1.4003654718399048
Model None Epoch 40 Batch 8: Loss 1.584559679031372
Model None Epoch 40 Batch 9: Loss 1.258070945739746
Model None Epoch 40 Batch 10: Loss 1.355945110321045
Model None Epoch 40 Batch 11: Loss 1.5277858972549438
Model None Epoch 40 Batch 12: Loss 1.390110731124878
Model None Epoch 40 Batch 13: Loss 1.4066323041915894
Model None Epoch 40 Batch 14: Loss 1.4390536546707153
Model None Epoch 40 Batch 15: Loss 1.377105712890625
Model None Epoch 40 Batch 16: Loss 1.3901616334915161
Model None Epoch 40 Batch 17: Loss 1.4062483310699463
Model None Epoch 40 Batch 18: Loss 1.4641484022140503
Model None Epoch 40 Batch 19: Loss 1.3916945457458496
Model None Epoch 40 Batch 20: Loss 1.418808937072754
Model None Epoch 40 Batch 21: Loss 1.2639155387878418
Model None Epoch 40 Batch 22: Loss 1.3048261404037476
Model None Epoch 40 Batch 23: Loss 1.3035590648651123
Model None Epoch 40 Batch 24: Loss 1.3797450065612793
Model None Epoch 40 Batch 25: Loss 1.480290412902832
Model None Epoch 40 Batch 26: Loss 1.2967084646224976
Model None Epoch 40 Batch 27: Loss 1.374274730682373
Model None Epoch 40 Batch 28: Loss 1.4745200872421265
Model None Epoch 40 Batch 29: Loss 1.4433472156524658
Model None Epoch 40 Batch 30: Loss 1.2989788055419922
Model None Epoch 40 Batch 31: Loss 1.2615370750427246
Model None Epoch 40 Batch 32: Loss 1.3837621212005615
Model None Epoch 40 Batch 33: Loss 1.311289668083191
Model None Epoch 40 Batch 34: Loss 1.5349934101104736
Model None Epoch 40 Batch 35: Loss 1.499215006828308
Model None Epoch 40 Batch 36: Loss 1.478662133216858
Model None Epoch 40 Batch 37: Loss 1.448379397392273
Model None Epoch 40 Batch 38: Loss 1.4869494438171387
Model None Epoch 40 Batch 39: Loss 1.4441438913345337
Model None Epoch 40 Batch 40: Loss 1.4126477241516113
Model None Epoch 40 Batch 41: Loss 1.4872380495071411
Model None Epoch 40 Batch 42: Loss 1.4703588485717773
Model None Epoch 40 Batch 43: Loss 1.3674345016479492
Model None Epoch 40 Batch 44: Loss 1.4657920598983765
Model None Epoch 40 Batch 45: Loss 1.4048066139221191
Model None Epoch 40 Batch 46: Loss 1.447161078453064
Model None Epoch 40 Batch 47: Loss 1.4463757276535034
Model None Epoch 40 Batch 48: Loss 1.43593168258667
Model None Epoch 40 Batch 49: Loss 1.4195096492767334
Downstream Train Epoch: 40 [12800/50000 (26%)]	Loss: 1.302561
Model None Epoch 40 Batch 50: Loss 1.3025606870651245
Model None Epoch 40 Batch 51: Loss 1.3773912191390991
Model None Epoch 40 Batch 52: Loss 1.4120696783065796
Model None Epoch 40 Batch 53: Loss 1.3472696542739868
Model None Epoch 40 Batch 54: Loss 1.4298317432403564
Model None Epoch 40 Batch 55: Loss 1.541125774383545
Model None Epoch 40 Batch 56: Loss 1.3759636878967285
Model None Epoch 40 Batch 57: Loss 1.4272023439407349
Model None Epoch 40 Batch 58: Loss 1.3850980997085571
Model None Epoch 40 Batch 59: Loss 1.477789282798767
Model None Epoch 40 Batch 60: Loss 1.4240790605545044
Model None Epoch 40 Batch 61: Loss 1.3666095733642578
Model None Epoch 40 Batch 62: Loss 1.4846035242080688
Model None Epoch 40 Batch 63: Loss 1.5118266344070435
Model None Epoch 40 Batch 64: Loss 1.4322760105133057
Model None Epoch 40 Batch 65: Loss 1.4173171520233154
Model None Epoch 40 Batch 66: Loss 1.263692855834961
Model None Epoch 40 Batch 67: Loss 1.357623815536499
Model None Epoch 40 Batch 68: Loss 1.4872010946273804
Model None Epoch 40 Batch 69: Loss 1.4479469060897827
Model None Epoch 40 Batch 70: Loss 1.4746593236923218
Model None Epoch 40 Batch 71: Loss 1.4414396286010742
Model None Epoch 40 Batch 72: Loss 1.3824753761291504
Model None Epoch 40 Batch 73: Loss 1.3808553218841553
Model None Epoch 40 Batch 74: Loss 1.4902236461639404
Model None Epoch 40 Batch 75: Loss 1.4500654935836792
Model None Epoch 40 Batch 76: Loss 1.3824352025985718
Model None Epoch 40 Batch 77: Loss 1.4034395217895508
Model None Epoch 40 Batch 78: Loss 1.4683434963226318
Model None Epoch 40 Batch 79: Loss 1.4363291263580322
Model None Epoch 40 Batch 80: Loss 1.4891526699066162
Model None Epoch 40 Batch 81: Loss 1.2780272960662842
Model None Epoch 40 Batch 82: Loss 1.3783067464828491
Model None Epoch 40 Batch 83: Loss 1.3195929527282715
Model None Epoch 40 Batch 84: Loss 1.244708776473999
Model None Epoch 40 Batch 85: Loss 1.346582055091858
Model None Epoch 40 Batch 86: Loss 1.4301354885101318
Model None Epoch 40 Batch 87: Loss 1.4569950103759766
Model None Epoch 40 Batch 88: Loss 1.5243874788284302
Model None Epoch 40 Batch 89: Loss 1.4647972583770752
Model None Epoch 40 Batch 90: Loss 1.3349562883377075
Model None Epoch 40 Batch 91: Loss 1.4726386070251465
Model None Epoch 40 Batch 92: Loss 1.3857742547988892
Model None Epoch 40 Batch 93: Loss 1.4359931945800781
Model None Epoch 40 Batch 94: Loss 1.4301064014434814
Model None Epoch 40 Batch 95: Loss 1.4906319379806519
Model None Epoch 40 Batch 96: Loss 1.356839895248413
Model None Epoch 40 Batch 97: Loss 1.429824948310852
Model None Epoch 40 Batch 98: Loss 1.4424301385879517
Model None Epoch 40 Batch 99: Loss 1.4031411409378052
Downstream Train Epoch: 40 [25600/50000 (51%)]	Loss: 1.452292
Model None Epoch 40 Batch 100: Loss 1.45229172706604
Model None Epoch 40 Batch 101: Loss 1.3778178691864014
Model None Epoch 40 Batch 102: Loss 1.4090148210525513
Model None Epoch 40 Batch 103: Loss 1.434604525566101
Model None Epoch 40 Batch 104: Loss 1.4664949178695679
Model None Epoch 40 Batch 105: Loss 1.4029595851898193
Model None Epoch 40 Batch 106: Loss 1.4347816705703735
Model None Epoch 40 Batch 107: Loss 1.3855657577514648
Model None Epoch 40 Batch 108: Loss 1.2689955234527588
Model None Epoch 40 Batch 109: Loss 1.491110920906067
Model None Epoch 40 Batch 110: Loss 1.3625551462173462
Model None Epoch 40 Batch 111: Loss 1.4352482557296753
Model None Epoch 40 Batch 112: Loss 1.300849437713623
Model None Epoch 40 Batch 113: Loss 1.4345823526382446
Model None Epoch 40 Batch 114: Loss 1.429677128791809
Model None Epoch 40 Batch 115: Loss 1.376261591911316
Model None Epoch 40 Batch 116: Loss 1.5235280990600586
Model None Epoch 40 Batch 117: Loss 1.3793368339538574
Model None Epoch 40 Batch 118: Loss 1.4908653497695923
Model None Epoch 40 Batch 119: Loss 1.425673484802246
Model None Epoch 40 Batch 120: Loss 1.3777140378952026
Model None Epoch 40 Batch 121: Loss 1.4839762449264526
Model None Epoch 40 Batch 122: Loss 1.391986608505249
Model None Epoch 40 Batch 123: Loss 1.4148365259170532
Model None Epoch 40 Batch 124: Loss 1.3650652170181274
Model None Epoch 40 Batch 125: Loss 1.4848968982696533
Model None Epoch 40 Batch 126: Loss 1.3880878686904907
Model None Epoch 40 Batch 127: Loss 1.3571017980575562
Model None Epoch 40 Batch 128: Loss 1.3515936136245728
Model None Epoch 40 Batch 129: Loss 1.4302929639816284
Model None Epoch 40 Batch 130: Loss 1.3567191362380981
Model None Epoch 40 Batch 131: Loss 1.4104169607162476
Model None Epoch 40 Batch 132: Loss 1.5046916007995605
Model None Epoch 40 Batch 133: Loss 1.3846287727355957
Model None Epoch 40 Batch 134: Loss 1.4192657470703125
Model None Epoch 40 Batch 135: Loss 1.4407076835632324
Model None Epoch 40 Batch 136: Loss 1.4010305404663086
Model None Epoch 40 Batch 137: Loss 1.4461772441864014
Model None Epoch 40 Batch 138: Loss 1.3632692098617554
Model None Epoch 40 Batch 139: Loss 1.4598519802093506
Model None Epoch 40 Batch 140: Loss 1.3351171016693115
Model None Epoch 40 Batch 141: Loss 1.3282456398010254
Model None Epoch 40 Batch 142: Loss 1.3510468006134033
Model None Epoch 40 Batch 143: Loss 1.4270581007003784
Model None Epoch 40 Batch 144: Loss 1.4223421812057495
Model None Epoch 40 Batch 145: Loss 1.482024908065796
Model None Epoch 40 Batch 146: Loss 1.3364112377166748
Model None Epoch 40 Batch 147: Loss 1.4468804597854614
Model None Epoch 40 Batch 148: Loss 1.380079746246338
Model None Epoch 40 Batch 149: Loss 1.428202509880066
Downstream Train Epoch: 40 [38400/50000 (77%)]	Loss: 1.420659
Model None Epoch 40 Batch 150: Loss 1.4206589460372925
Model None Epoch 40 Batch 151: Loss 1.3821581602096558
Model None Epoch 40 Batch 152: Loss 1.3859143257141113
Model None Epoch 40 Batch 153: Loss 1.410223364830017
Model None Epoch 40 Batch 154: Loss 1.317775011062622
Model None Epoch 40 Batch 155: Loss 1.321899652481079
Model None Epoch 40 Batch 156: Loss 1.3262468576431274
Model None Epoch 40 Batch 157: Loss 1.3730425834655762
Model None Epoch 40 Batch 158: Loss 1.434120774269104
Model None Epoch 40 Batch 159: Loss 1.410568118095398
Model None Epoch 40 Batch 160: Loss 1.261186957359314
Model None Epoch 40 Batch 161: Loss 1.4563945531845093
Model None Epoch 40 Batch 162: Loss 1.4468010663986206
Model None Epoch 40 Batch 163: Loss 1.3308526277542114
Model None Epoch 40 Batch 164: Loss 1.4819207191467285
Model None Epoch 40 Batch 165: Loss 1.4606491327285767
Model None Epoch 40 Batch 166: Loss 1.4779046773910522
Model None Epoch 40 Batch 167: Loss 1.3698809146881104
Model None Epoch 40 Batch 168: Loss 1.511603593826294
Model None Epoch 40 Batch 169: Loss 1.519296646118164
Model None Epoch 40 Batch 170: Loss 1.4170247316360474
Model None Epoch 40 Batch 171: Loss 1.4281659126281738
Model None Epoch 40 Batch 172: Loss 1.3147809505462646
Model None Epoch 40 Batch 173: Loss 1.4360746145248413
Model None Epoch 40 Batch 174: Loss 1.3682447671890259
Model None Epoch 40 Batch 175: Loss 1.3865152597427368
Model None Epoch 40 Batch 176: Loss 1.3826082944869995
Model None Epoch 40 Batch 177: Loss 1.5241599082946777
Model None Epoch 40 Batch 178: Loss 1.441169023513794
Model None Epoch 40 Batch 179: Loss 1.3539128303527832
Model None Epoch 40 Batch 180: Loss 1.2730752229690552
Model None Epoch 40 Batch 181: Loss 1.4015008211135864
Model None Epoch 40 Batch 182: Loss 1.4710350036621094
Model None Epoch 40 Batch 183: Loss 1.3561198711395264
Model None Epoch 40 Batch 184: Loss 1.3664414882659912
Model None Epoch 40 Batch 185: Loss 1.5032187700271606
Model None Epoch 40 Batch 186: Loss 1.3503360748291016
Model None Epoch 40 Batch 187: Loss 1.417419195175171
Model None Epoch 40 Batch 188: Loss 1.3330906629562378
Model None Epoch 40 Batch 189: Loss 1.2921346426010132
Model None Epoch 40 Batch 190: Loss 1.4719208478927612
Model None Epoch 40 Batch 191: Loss 1.378626823425293
Model None Epoch 40 Batch 192: Loss 1.3613063097000122
Model None Epoch 40 Batch 193: Loss 1.4117532968521118
Model None Epoch 40 Batch 194: Loss 1.4081720113754272
Model None Epoch 40 Batch 195: Loss 1.3085745573043823

 Downstream Train loss: 1.4068535694054194 Acc: 0.5655
Downstream Train Epoch: 41 [0/50000 (0%)]	Loss: 1.383329
Model None Epoch 41 Batch 0: Loss 1.3833293914794922
Model None Epoch 41 Batch 1: Loss 1.3088772296905518
Model None Epoch 41 Batch 2: Loss 1.4945707321166992
Model None Epoch 41 Batch 3: Loss 1.460862636566162
Model None Epoch 41 Batch 4: Loss 1.4700658321380615
Model None Epoch 41 Batch 5: Loss 1.3516204357147217
Model None Epoch 41 Batch 6: Loss 1.4466934204101562
Model None Epoch 41 Batch 7: Loss 1.4097933769226074
Model None Epoch 41 Batch 8: Loss 1.349889874458313
Model None Epoch 41 Batch 9: Loss 1.3938599824905396
Model None Epoch 41 Batch 10: Loss 1.4408018589019775
Model None Epoch 41 Batch 11: Loss 1.3953146934509277
Model None Epoch 41 Batch 12: Loss 1.433591604232788
Model None Epoch 41 Batch 13: Loss 1.3359895944595337
Model None Epoch 41 Batch 14: Loss 1.421075701713562
Model None Epoch 41 Batch 15: Loss 1.3915010690689087
Model None Epoch 41 Batch 16: Loss 1.3972554206848145
Model None Epoch 41 Batch 17: Loss 1.4240620136260986
Model None Epoch 41 Batch 18: Loss 1.5197474956512451
Model None Epoch 41 Batch 19: Loss 1.4619784355163574
Model None Epoch 41 Batch 20: Loss 1.4115593433380127
Model None Epoch 41 Batch 21: Loss 1.4466543197631836
Model None Epoch 41 Batch 22: Loss 1.505906105041504
Model None Epoch 41 Batch 23: Loss 1.5324325561523438
Model None Epoch 41 Batch 24: Loss 1.386642575263977
Model None Epoch 41 Batch 25: Loss 1.544600009918213
Model None Epoch 41 Batch 26: Loss 1.3635978698730469
Model None Epoch 41 Batch 27: Loss 1.3346644639968872
Model None Epoch 41 Batch 28: Loss 1.3626347780227661
Model None Epoch 41 Batch 29: Loss 1.476863145828247
Model None Epoch 41 Batch 30: Loss 1.4614298343658447
Model None Epoch 41 Batch 31: Loss 1.3948148488998413
Model None Epoch 41 Batch 32: Loss 1.41221022605896
Model None Epoch 41 Batch 33: Loss 1.40131413936615
Model None Epoch 41 Batch 34: Loss 1.435336947441101
Model None Epoch 41 Batch 35: Loss 1.394353985786438
Model None Epoch 41 Batch 36: Loss 1.4007269144058228
Model None Epoch 41 Batch 37: Loss 1.423523187637329
Model None Epoch 41 Batch 38: Loss 1.3590887784957886
Model None Epoch 41 Batch 39: Loss 1.4912127256393433
Model None Epoch 41 Batch 40: Loss 1.4665266275405884
Model None Epoch 41 Batch 41: Loss 1.4241831302642822
Model None Epoch 41 Batch 42: Loss 1.4190223217010498
Model None Epoch 41 Batch 43: Loss 1.407322645187378
Model None Epoch 41 Batch 44: Loss 1.3372381925582886
Model None Epoch 41 Batch 45: Loss 1.5070263147354126
Model None Epoch 41 Batch 46: Loss 1.355185866355896
Model None Epoch 41 Batch 47: Loss 1.4738295078277588
Model None Epoch 41 Batch 48: Loss 1.4451408386230469
Model None Epoch 41 Batch 49: Loss 1.404226541519165
Downstream Train Epoch: 41 [12800/50000 (26%)]	Loss: 1.237405
Model None Epoch 41 Batch 50: Loss 1.2374054193496704
Model None Epoch 41 Batch 51: Loss 1.3864542245864868
Model None Epoch 41 Batch 52: Loss 1.435429334640503
Model None Epoch 41 Batch 53: Loss 1.4017876386642456
Model None Epoch 41 Batch 54: Loss 1.4000734090805054
Model None Epoch 41 Batch 55: Loss 1.5430749654769897
Model None Epoch 41 Batch 56: Loss 1.3783268928527832
Model None Epoch 41 Batch 57: Loss 1.3768867254257202
Model None Epoch 41 Batch 58: Loss 1.3606091737747192
Model None Epoch 41 Batch 59: Loss 1.4642339944839478
Model None Epoch 41 Batch 60: Loss 1.3381202220916748
Model None Epoch 41 Batch 61: Loss 1.3388328552246094
Model None Epoch 41 Batch 62: Loss 1.3613247871398926
Model None Epoch 41 Batch 63: Loss 1.443022608757019
Model None Epoch 41 Batch 64: Loss 1.344925880432129
Model None Epoch 41 Batch 65: Loss 1.4417297840118408
Model None Epoch 41 Batch 66: Loss 1.4801610708236694
Model None Epoch 41 Batch 67: Loss 1.410881519317627
Model None Epoch 41 Batch 68: Loss 1.451112151145935
Model None Epoch 41 Batch 69: Loss 1.3437917232513428
Model None Epoch 41 Batch 70: Loss 1.3858751058578491
Model None Epoch 41 Batch 71: Loss 1.355967402458191
Model None Epoch 41 Batch 72: Loss 1.4293854236602783
Model None Epoch 41 Batch 73: Loss 1.5330313444137573
Model None Epoch 41 Batch 74: Loss 1.427372932434082
Model None Epoch 41 Batch 75: Loss 1.3732069730758667
Model None Epoch 41 Batch 76: Loss 1.3956228494644165
Model None Epoch 41 Batch 77: Loss 1.5043038129806519
Model None Epoch 41 Batch 78: Loss 1.3796250820159912
Model None Epoch 41 Batch 79: Loss 1.3305094242095947
Model None Epoch 41 Batch 80: Loss 1.3966020345687866
Model None Epoch 41 Batch 81: Loss 1.3984386920928955
Model None Epoch 41 Batch 82: Loss 1.3932946920394897
Model None Epoch 41 Batch 83: Loss 1.528648853302002
Model None Epoch 41 Batch 84: Loss 1.4601103067398071
Model None Epoch 41 Batch 85: Loss 1.392477035522461
Model None Epoch 41 Batch 86: Loss 1.3538533449172974
Model None Epoch 41 Batch 87: Loss 1.302710771560669
Model None Epoch 41 Batch 88: Loss 1.4241129159927368
Model None Epoch 41 Batch 89: Loss 1.3215147256851196
Model None Epoch 41 Batch 90: Loss 1.3297314643859863
Model None Epoch 41 Batch 91: Loss 1.3474969863891602
Model None Epoch 41 Batch 92: Loss 1.4054875373840332
Model None Epoch 41 Batch 93: Loss 1.4199895858764648
Model None Epoch 41 Batch 94: Loss 1.375180721282959
Model None Epoch 41 Batch 95: Loss 1.3437445163726807
Model None Epoch 41 Batch 96: Loss 1.4219218492507935
Model None Epoch 41 Batch 97: Loss 1.4447273015975952
Model None Epoch 41 Batch 98: Loss 1.5210105180740356
Model None Epoch 41 Batch 99: Loss 1.3542224168777466
Downstream Train Epoch: 41 [25600/50000 (51%)]	Loss: 1.415690
Model None Epoch 41 Batch 100: Loss 1.4156898260116577
Model None Epoch 41 Batch 101: Loss 1.388026237487793
Model None Epoch 41 Batch 102: Loss 1.2382667064666748
Model None Epoch 41 Batch 103: Loss 1.4573205709457397
Model None Epoch 41 Batch 104: Loss 1.3968497514724731
Model None Epoch 41 Batch 105: Loss 1.3740246295928955
Model None Epoch 41 Batch 106: Loss 1.404483675956726
Model None Epoch 41 Batch 107: Loss 1.383143424987793
Model None Epoch 41 Batch 108: Loss 1.4149093627929688
Model None Epoch 41 Batch 109: Loss 1.3742518424987793
Model None Epoch 41 Batch 110: Loss 1.4844934940338135
Model None Epoch 41 Batch 111: Loss 1.4292289018630981
Model None Epoch 41 Batch 112: Loss 1.3497321605682373
Model None Epoch 41 Batch 113: Loss 1.3615564107894897
Model None Epoch 41 Batch 114: Loss 1.316103458404541
Model None Epoch 41 Batch 115: Loss 1.2637968063354492
Model None Epoch 41 Batch 116: Loss 1.4781250953674316
Model None Epoch 41 Batch 117: Loss 1.4661012887954712
Model None Epoch 41 Batch 118: Loss 1.377007007598877
Model None Epoch 41 Batch 119: Loss 1.270998239517212
Model None Epoch 41 Batch 120: Loss 1.4539449214935303
Model None Epoch 41 Batch 121: Loss 1.2446904182434082
Model None Epoch 41 Batch 122: Loss 1.3321974277496338
Model None Epoch 41 Batch 123: Loss 1.2902733087539673
Model None Epoch 41 Batch 124: Loss 1.3845921754837036
Model None Epoch 41 Batch 125: Loss 1.3970065116882324
Model None Epoch 41 Batch 126: Loss 1.4565296173095703
Model None Epoch 41 Batch 127: Loss 1.4879951477050781
Model None Epoch 41 Batch 128: Loss 1.3995450735092163
Model None Epoch 41 Batch 129: Loss 1.3611640930175781
Model None Epoch 41 Batch 130: Loss 1.4422515630722046
Model None Epoch 41 Batch 131: Loss 1.4074398279190063
Model None Epoch 41 Batch 132: Loss 1.3509044647216797
Model None Epoch 41 Batch 133: Loss 1.3665409088134766
Model None Epoch 41 Batch 134: Loss 1.4279091358184814
Model None Epoch 41 Batch 135: Loss 1.3752578496932983
Model None Epoch 41 Batch 136: Loss 1.3492265939712524
Model None Epoch 41 Batch 137: Loss 1.5234417915344238
Model None Epoch 41 Batch 138: Loss 1.4179415702819824
Model None Epoch 41 Batch 139: Loss 1.435679316520691
Model None Epoch 41 Batch 140: Loss 1.3812236785888672
Model None Epoch 41 Batch 141: Loss 1.436693787574768
Model None Epoch 41 Batch 142: Loss 1.4563416242599487
Model None Epoch 41 Batch 143: Loss 1.4745814800262451
Model None Epoch 41 Batch 144: Loss 1.3776695728302002
Model None Epoch 41 Batch 145: Loss 1.399335503578186
Model None Epoch 41 Batch 146: Loss 1.3494007587432861
Model None Epoch 41 Batch 147: Loss 1.405077338218689
Model None Epoch 41 Batch 148: Loss 1.29233980178833
Model None Epoch 41 Batch 149: Loss 1.282519817352295
Downstream Train Epoch: 41 [38400/50000 (77%)]	Loss: 1.407086
Model None Epoch 41 Batch 150: Loss 1.4070861339569092
Model None Epoch 41 Batch 151: Loss 1.3549599647521973
Model None Epoch 41 Batch 152: Loss 1.3196707963943481
Model None Epoch 41 Batch 153: Loss 1.505277156829834
Model None Epoch 41 Batch 154: Loss 1.367855429649353
Model None Epoch 41 Batch 155: Loss 1.3000988960266113
Model None Epoch 41 Batch 156: Loss 1.3942707777023315
Model None Epoch 41 Batch 157: Loss 1.4617127180099487
Model None Epoch 41 Batch 158: Loss 1.335824966430664
Model None Epoch 41 Batch 159: Loss 1.451253890991211
Model None Epoch 41 Batch 160: Loss 1.3621814250946045
Model None Epoch 41 Batch 161: Loss 1.4662103652954102
Model None Epoch 41 Batch 162: Loss 1.3657652139663696
Model None Epoch 41 Batch 163: Loss 1.2902305126190186
Model None Epoch 41 Batch 164: Loss 1.4084343910217285
Model None Epoch 41 Batch 165: Loss 1.4780856370925903
Model None Epoch 41 Batch 166: Loss 1.4844317436218262
Model None Epoch 41 Batch 167: Loss 1.2884798049926758
Model None Epoch 41 Batch 168: Loss 1.1971471309661865
Model None Epoch 41 Batch 169: Loss 1.4082205295562744
Model None Epoch 41 Batch 170: Loss 1.4270215034484863
Model None Epoch 41 Batch 171: Loss 1.4138906002044678
Model None Epoch 41 Batch 172: Loss 1.3456438779830933
Model None Epoch 41 Batch 173: Loss 1.4006145000457764
Model None Epoch 41 Batch 174: Loss 1.4848921298980713
Model None Epoch 41 Batch 175: Loss 1.3618470430374146
Model None Epoch 41 Batch 176: Loss 1.329420566558838
Model None Epoch 41 Batch 177: Loss 1.258722186088562
Model None Epoch 41 Batch 178: Loss 1.4971755743026733
Model None Epoch 41 Batch 179: Loss 1.5040690898895264
Model None Epoch 41 Batch 180: Loss 1.3700852394104004
Model None Epoch 41 Batch 181: Loss 1.3022983074188232
Model None Epoch 41 Batch 182: Loss 1.512182354927063
Model None Epoch 41 Batch 183: Loss 1.3561183214187622
Model None Epoch 41 Batch 184: Loss 1.4337737560272217
Model None Epoch 41 Batch 185: Loss 1.3209254741668701
Model None Epoch 41 Batch 186: Loss 1.3842158317565918
Model None Epoch 41 Batch 187: Loss 1.4716976881027222
Model None Epoch 41 Batch 188: Loss 1.476264476776123
Model None Epoch 41 Batch 189: Loss 1.4319626092910767
Model None Epoch 41 Batch 190: Loss 1.3854997158050537
Model None Epoch 41 Batch 191: Loss 1.4609907865524292
Model None Epoch 41 Batch 192: Loss 1.3960131406784058
Model None Epoch 41 Batch 193: Loss 1.2864620685577393
Model None Epoch 41 Batch 194: Loss 1.4236589670181274
Model None Epoch 41 Batch 195: Loss 1.5026811361312866

 Downstream Train loss: 1.4012635660414794 Acc: 0.5655
Downstream Train Epoch: 42 [0/50000 (0%)]	Loss: 1.420279
Model None Epoch 42 Batch 0: Loss 1.4202791452407837
Model None Epoch 42 Batch 1: Loss 1.5350908041000366
Model None Epoch 42 Batch 2: Loss 1.3676148653030396
Model None Epoch 42 Batch 3: Loss 1.421828269958496
Model None Epoch 42 Batch 4: Loss 1.3053133487701416
Model None Epoch 42 Batch 5: Loss 1.3665962219238281
Model None Epoch 42 Batch 6: Loss 1.4520548582077026
Model None Epoch 42 Batch 7: Loss 1.3787213563919067
Model None Epoch 42 Batch 8: Loss 1.442683458328247
Model None Epoch 42 Batch 9: Loss 1.378324270248413
Model None Epoch 42 Batch 10: Loss 1.2766704559326172
Model None Epoch 42 Batch 11: Loss 1.3555551767349243
Model None Epoch 42 Batch 12: Loss 1.4086005687713623
Model None Epoch 42 Batch 13: Loss 1.526501178741455
Model None Epoch 42 Batch 14: Loss 1.4769681692123413
Model None Epoch 42 Batch 15: Loss 1.4581679105758667
Model None Epoch 42 Batch 16: Loss 1.4541832208633423
Model None Epoch 42 Batch 17: Loss 1.4372529983520508
Model None Epoch 42 Batch 18: Loss 1.3386011123657227
Model None Epoch 42 Batch 19: Loss 1.391897439956665
Model None Epoch 42 Batch 20: Loss 1.3911259174346924
Model None Epoch 42 Batch 21: Loss 1.4878075122833252
Model None Epoch 42 Batch 22: Loss 1.4709906578063965
Model None Epoch 42 Batch 23: Loss 1.4660896062850952
Model None Epoch 42 Batch 24: Loss 1.3580131530761719
Model None Epoch 42 Batch 25: Loss 1.3353558778762817
Model None Epoch 42 Batch 26: Loss 1.4692038297653198
Model None Epoch 42 Batch 27: Loss 1.4851222038269043
Model None Epoch 42 Batch 28: Loss 1.4731732606887817
Model None Epoch 42 Batch 29: Loss 1.3998039960861206
Model None Epoch 42 Batch 30: Loss 1.453872799873352
Model None Epoch 42 Batch 31: Loss 1.3721097707748413
Model None Epoch 42 Batch 32: Loss 1.4072833061218262
Model None Epoch 42 Batch 33: Loss 1.4413822889328003
Model None Epoch 42 Batch 34: Loss 1.457189679145813
Model None Epoch 42 Batch 35: Loss 1.3361282348632812
Model None Epoch 42 Batch 36: Loss 1.4114916324615479
Model None Epoch 42 Batch 37: Loss 1.3869857788085938
Model None Epoch 42 Batch 38: Loss 1.474905252456665
Model None Epoch 42 Batch 39: Loss 1.278001070022583
Model None Epoch 42 Batch 40: Loss 1.4681061506271362
Model None Epoch 42 Batch 41: Loss 1.4106923341751099
Model None Epoch 42 Batch 42: Loss 1.483390212059021
Model None Epoch 42 Batch 43: Loss 1.379376769065857
Model None Epoch 42 Batch 44: Loss 1.5268617868423462
Model None Epoch 42 Batch 45: Loss 1.447779655456543
Model None Epoch 42 Batch 46: Loss 1.452174186706543
Model None Epoch 42 Batch 47: Loss 1.3965744972229004
Model None Epoch 42 Batch 48: Loss 1.3483173847198486
Model None Epoch 42 Batch 49: Loss 1.4115402698516846
Downstream Train Epoch: 42 [12800/50000 (26%)]	Loss: 1.438314
Model None Epoch 42 Batch 50: Loss 1.4383143186569214
Model None Epoch 42 Batch 51: Loss 1.4752097129821777
Model None Epoch 42 Batch 52: Loss 1.4150166511535645
Model None Epoch 42 Batch 53: Loss 1.2657959461212158
Model None Epoch 42 Batch 54: Loss 1.3570120334625244
Model None Epoch 42 Batch 55: Loss 1.359885573387146
Model None Epoch 42 Batch 56: Loss 1.4111841917037964
Model None Epoch 42 Batch 57: Loss 1.4094994068145752
Model None Epoch 42 Batch 58: Loss 1.3799934387207031
Model None Epoch 42 Batch 59: Loss 1.5694198608398438
Model None Epoch 42 Batch 60: Loss 1.3625962734222412
Model None Epoch 42 Batch 61: Loss 1.4462413787841797
Model None Epoch 42 Batch 62: Loss 1.5206711292266846
Model None Epoch 42 Batch 63: Loss 1.3924102783203125
Model None Epoch 42 Batch 64: Loss 1.4552968740463257
Model None Epoch 42 Batch 65: Loss 1.50369393825531
Model None Epoch 42 Batch 66: Loss 1.4095538854599
Model None Epoch 42 Batch 67: Loss 1.4243392944335938
Model None Epoch 42 Batch 68: Loss 1.3926167488098145
Model None Epoch 42 Batch 69: Loss 1.3157671689987183
Model None Epoch 42 Batch 70: Loss 1.411817193031311
Model None Epoch 42 Batch 71: Loss 1.4565222263336182
Model None Epoch 42 Batch 72: Loss 1.4667458534240723
Model None Epoch 42 Batch 73: Loss 1.372118353843689
Model None Epoch 42 Batch 74: Loss 1.5405162572860718
Model None Epoch 42 Batch 75: Loss 1.4397766590118408
Model None Epoch 42 Batch 76: Loss 1.4750959873199463
Model None Epoch 42 Batch 77: Loss 1.461703896522522
Model None Epoch 42 Batch 78: Loss 1.3808554410934448
Model None Epoch 42 Batch 79: Loss 1.5078011751174927
Model None Epoch 42 Batch 80: Loss 1.369468331336975
Model None Epoch 42 Batch 81: Loss 1.5237843990325928
Model None Epoch 42 Batch 82: Loss 1.461297631263733
Model None Epoch 42 Batch 83: Loss 1.5433622598648071
Model None Epoch 42 Batch 84: Loss 1.4212764501571655
Model None Epoch 42 Batch 85: Loss 1.426000952720642
Model None Epoch 42 Batch 86: Loss 1.403710961341858
Model None Epoch 42 Batch 87: Loss 1.2740750312805176
Model None Epoch 42 Batch 88: Loss 1.5300402641296387
Model None Epoch 42 Batch 89: Loss 1.4898295402526855
Model None Epoch 42 Batch 90: Loss 1.4214445352554321
Model None Epoch 42 Batch 91: Loss 1.3507847785949707
Model None Epoch 42 Batch 92: Loss 1.4548996686935425
Model None Epoch 42 Batch 93: Loss 1.446452021598816
Model None Epoch 42 Batch 94: Loss 1.4594897031784058
Model None Epoch 42 Batch 95: Loss 1.3962855339050293
Model None Epoch 42 Batch 96: Loss 1.492625117301941
Model None Epoch 42 Batch 97: Loss 1.4189690351486206
Model None Epoch 42 Batch 98: Loss 1.388862133026123
Model None Epoch 42 Batch 99: Loss 1.4738023281097412
Downstream Train Epoch: 42 [25600/50000 (51%)]	Loss: 1.492307
Model None Epoch 42 Batch 100: Loss 1.4923070669174194
Model None Epoch 42 Batch 101: Loss 1.495665431022644
Model None Epoch 42 Batch 102: Loss 1.292722225189209
Model None Epoch 42 Batch 103: Loss 1.3530069589614868
Model None Epoch 42 Batch 104: Loss 1.4541621208190918
Model None Epoch 42 Batch 105: Loss 1.4311480522155762
Model None Epoch 42 Batch 106: Loss 1.4460654258728027
Model None Epoch 42 Batch 107: Loss 1.3088390827178955
Model None Epoch 42 Batch 108: Loss 1.4868905544281006
Model None Epoch 42 Batch 109: Loss 1.3609603643417358
Model None Epoch 42 Batch 110: Loss 1.4513386487960815
Model None Epoch 42 Batch 111: Loss 1.3753433227539062
Model None Epoch 42 Batch 112: Loss 1.3571549654006958
Model None Epoch 42 Batch 113: Loss 1.3237683773040771
Model None Epoch 42 Batch 114: Loss 1.555651068687439
Model None Epoch 42 Batch 115: Loss 1.5098745822906494
Model None Epoch 42 Batch 116: Loss 1.4016172885894775
Model None Epoch 42 Batch 117: Loss 1.5258691310882568
Model None Epoch 42 Batch 118: Loss 1.3368743658065796
Model None Epoch 42 Batch 119: Loss 1.448641061782837
Model None Epoch 42 Batch 120: Loss 1.4443224668502808
Model None Epoch 42 Batch 121: Loss 1.520887017250061
Model None Epoch 42 Batch 122: Loss 1.452097773551941
Model None Epoch 42 Batch 123: Loss 1.3976542949676514
Model None Epoch 42 Batch 124: Loss 1.4686261415481567
Model None Epoch 42 Batch 125: Loss 1.4091756343841553
Model None Epoch 42 Batch 126: Loss 1.479158639907837
Model None Epoch 42 Batch 127: Loss 1.3577290773391724
Model None Epoch 42 Batch 128: Loss 1.4028860330581665
Model None Epoch 42 Batch 129: Loss 1.372254729270935
Model None Epoch 42 Batch 130: Loss 1.428739309310913
Model None Epoch 42 Batch 131: Loss 1.3072900772094727
Model None Epoch 42 Batch 132: Loss 1.349090576171875
Model None Epoch 42 Batch 133: Loss 1.4205297231674194
Model None Epoch 42 Batch 134: Loss 1.3307805061340332
Model None Epoch 42 Batch 135: Loss 1.4476478099822998
Model None Epoch 42 Batch 136: Loss 1.444908857345581
Model None Epoch 42 Batch 137: Loss 1.227009892463684
Model None Epoch 42 Batch 138: Loss 1.3121211528778076
Model None Epoch 42 Batch 139: Loss 1.3753684759140015
Model None Epoch 42 Batch 140: Loss 1.4192619323730469
Model None Epoch 42 Batch 141: Loss 1.4531824588775635
Model None Epoch 42 Batch 142: Loss 1.385867953300476
Model None Epoch 42 Batch 143: Loss 1.3127049207687378
Model None Epoch 42 Batch 144: Loss 1.238068699836731
Model None Epoch 42 Batch 145: Loss 1.4116241931915283
Model None Epoch 42 Batch 146: Loss 1.3687424659729004
Model None Epoch 42 Batch 147: Loss 1.503119707107544
Model None Epoch 42 Batch 148: Loss 1.4043885469436646
Model None Epoch 42 Batch 149: Loss 1.3853669166564941
Downstream Train Epoch: 42 [38400/50000 (77%)]	Loss: 1.428384
Model None Epoch 42 Batch 150: Loss 1.4283835887908936
Model None Epoch 42 Batch 151: Loss 1.4179308414459229
Model None Epoch 42 Batch 152: Loss 1.4211047887802124
Model None Epoch 42 Batch 153: Loss 1.3942158222198486
Model None Epoch 42 Batch 154: Loss 1.3746591806411743
Model None Epoch 42 Batch 155: Loss 1.4304189682006836
Model None Epoch 42 Batch 156: Loss 1.373950481414795
Model None Epoch 42 Batch 157: Loss 1.2579346895217896
Model None Epoch 42 Batch 158: Loss 1.3772461414337158
Model None Epoch 42 Batch 159: Loss 1.4536395072937012
Model None Epoch 42 Batch 160: Loss 1.37240469455719
Model None Epoch 42 Batch 161: Loss 1.3413289785385132
Model None Epoch 42 Batch 162: Loss 1.3977843523025513
Model None Epoch 42 Batch 163: Loss 1.355595588684082
Model None Epoch 42 Batch 164: Loss 1.3130078315734863
Model None Epoch 42 Batch 165: Loss 1.4773542881011963
Model None Epoch 42 Batch 166: Loss 1.3730297088623047
Model None Epoch 42 Batch 167: Loss 1.3538018465042114
Model None Epoch 42 Batch 168: Loss 1.3666479587554932
Model None Epoch 42 Batch 169: Loss 1.3880730867385864
Model None Epoch 42 Batch 170: Loss 1.4363499879837036
Model None Epoch 42 Batch 171: Loss 1.4764525890350342
Model None Epoch 42 Batch 172: Loss 1.410563349723816
Model None Epoch 42 Batch 173: Loss 1.3691344261169434
Model None Epoch 42 Batch 174: Loss 1.360952615737915
Model None Epoch 42 Batch 175: Loss 1.5256701707839966
Model None Epoch 42 Batch 176: Loss 1.3995943069458008
Model None Epoch 42 Batch 177: Loss 1.2972614765167236
Model None Epoch 42 Batch 178: Loss 1.309682846069336
Model None Epoch 42 Batch 179: Loss 1.3999183177947998
Model None Epoch 42 Batch 180: Loss 1.4006147384643555
Model None Epoch 42 Batch 181: Loss 1.341948390007019
Model None Epoch 42 Batch 182: Loss 1.4008606672286987
Model None Epoch 42 Batch 183: Loss 1.3842742443084717
Model None Epoch 42 Batch 184: Loss 1.390440821647644
Model None Epoch 42 Batch 185: Loss 1.3152782917022705
Model None Epoch 42 Batch 186: Loss 1.54837965965271
Model None Epoch 42 Batch 187: Loss 1.448947787284851
Model None Epoch 42 Batch 188: Loss 1.3811153173446655
Model None Epoch 42 Batch 189: Loss 1.474578619003296
Model None Epoch 42 Batch 190: Loss 1.4584656953811646
Model None Epoch 42 Batch 191: Loss 1.4579863548278809
Model None Epoch 42 Batch 192: Loss 1.3617929220199585
Model None Epoch 42 Batch 193: Loss 1.3952851295471191
Model None Epoch 42 Batch 194: Loss 1.4148995876312256
Model None Epoch 42 Batch 195: Loss 1.2948286533355713

 Downstream Train loss: 1.4122448016186149 Acc: 0.5655
Downstream Train Epoch: 43 [0/50000 (0%)]	Loss: 1.342306
Model None Epoch 43 Batch 0: Loss 1.3423064947128296
Model None Epoch 43 Batch 1: Loss 1.4244937896728516
Model None Epoch 43 Batch 2: Loss 1.4052764177322388
Model None Epoch 43 Batch 3: Loss 1.4740735292434692
Model None Epoch 43 Batch 4: Loss 1.3124444484710693
Model None Epoch 43 Batch 5: Loss 1.4515354633331299
Model None Epoch 43 Batch 6: Loss 1.3647165298461914
Model None Epoch 43 Batch 7: Loss 1.437409520149231
Model None Epoch 43 Batch 8: Loss 1.4003359079360962
Model None Epoch 43 Batch 9: Loss 1.342288851737976
Model None Epoch 43 Batch 10: Loss 1.373748540878296
Model None Epoch 43 Batch 11: Loss 1.3770205974578857
Model None Epoch 43 Batch 12: Loss 1.3083891868591309
Model None Epoch 43 Batch 13: Loss 1.4972035884857178
Model None Epoch 43 Batch 14: Loss 1.3962231874465942
Model None Epoch 43 Batch 15: Loss 1.4352269172668457
Model None Epoch 43 Batch 16: Loss 1.5728431940078735
Model None Epoch 43 Batch 17: Loss 1.3770917654037476
Model None Epoch 43 Batch 18: Loss 1.409900188446045
Model None Epoch 43 Batch 19: Loss 1.366753339767456
Model None Epoch 43 Batch 20: Loss 1.4957209825515747
Model None Epoch 43 Batch 21: Loss 1.5195691585540771
Model None Epoch 43 Batch 22: Loss 1.4200807809829712
Model None Epoch 43 Batch 23: Loss 1.3155021667480469
Model None Epoch 43 Batch 24: Loss 1.2881464958190918
Model None Epoch 43 Batch 25: Loss 1.3693512678146362
Model None Epoch 43 Batch 26: Loss 1.3834110498428345
Model None Epoch 43 Batch 27: Loss 1.3993663787841797
Model None Epoch 43 Batch 28: Loss 1.4203382730484009
Model None Epoch 43 Batch 29: Loss 1.4343773126602173
Model None Epoch 43 Batch 30: Loss 1.396168828010559
Model None Epoch 43 Batch 31: Loss 1.360320806503296
Model None Epoch 43 Batch 32: Loss 1.4693647623062134
Model None Epoch 43 Batch 33: Loss 1.3976376056671143
Model None Epoch 43 Batch 34: Loss 1.5016125440597534
Model None Epoch 43 Batch 35: Loss 1.3834806680679321
Model None Epoch 43 Batch 36: Loss 1.3683538436889648
Model None Epoch 43 Batch 37: Loss 1.3633849620819092
Model None Epoch 43 Batch 38: Loss 1.4790159463882446
Model None Epoch 43 Batch 39: Loss 1.3446946144104004
Model None Epoch 43 Batch 40: Loss 1.3779537677764893
Model None Epoch 43 Batch 41: Loss 1.5017138719558716
Model None Epoch 43 Batch 42: Loss 1.4992451667785645
Model None Epoch 43 Batch 43: Loss 1.3134384155273438
Model None Epoch 43 Batch 44: Loss 1.4689030647277832
Model None Epoch 43 Batch 45: Loss 1.4112344980239868
Model None Epoch 43 Batch 46: Loss 1.4287917613983154
Model None Epoch 43 Batch 47: Loss 1.4856699705123901
Model None Epoch 43 Batch 48: Loss 1.3970664739608765
Model None Epoch 43 Batch 49: Loss 1.4708871841430664
Downstream Train Epoch: 43 [12800/50000 (26%)]	Loss: 1.446171
Model None Epoch 43 Batch 50: Loss 1.4461705684661865
Model None Epoch 43 Batch 51: Loss 1.3780298233032227
Model None Epoch 43 Batch 52: Loss 1.39589524269104
Model None Epoch 43 Batch 53: Loss 1.4690096378326416
Model None Epoch 43 Batch 54: Loss 1.3044981956481934
Model None Epoch 43 Batch 55: Loss 1.3936998844146729
Model None Epoch 43 Batch 56: Loss 1.3624416589736938
Model None Epoch 43 Batch 57: Loss 1.447737216949463
Model None Epoch 43 Batch 58: Loss 1.344214916229248
Model None Epoch 43 Batch 59: Loss 1.3746678829193115
Model None Epoch 43 Batch 60: Loss 1.4071669578552246
Model None Epoch 43 Batch 61: Loss 1.3010822534561157
Model None Epoch 43 Batch 62: Loss 1.4750068187713623
Model None Epoch 43 Batch 63: Loss 1.4574791193008423
Model None Epoch 43 Batch 64: Loss 1.4584428071975708
Model None Epoch 43 Batch 65: Loss 1.3336728811264038
Model None Epoch 43 Batch 66: Loss 1.3268994092941284
Model None Epoch 43 Batch 67: Loss 1.4408674240112305
Model None Epoch 43 Batch 68: Loss 1.4685560464859009
Model None Epoch 43 Batch 69: Loss 1.3717440366744995
Model None Epoch 43 Batch 70: Loss 1.390695333480835
Model None Epoch 43 Batch 71: Loss 1.4107784032821655
Model None Epoch 43 Batch 72: Loss 1.2704647779464722
Model None Epoch 43 Batch 73: Loss 1.4236959218978882
Model None Epoch 43 Batch 74: Loss 1.4199252128601074
Model None Epoch 43 Batch 75: Loss 1.3736801147460938
Model None Epoch 43 Batch 76: Loss 1.416656732559204
Model None Epoch 43 Batch 77: Loss 1.4859815835952759
Model None Epoch 43 Batch 78: Loss 1.3792718648910522
Model None Epoch 43 Batch 79: Loss 1.417625904083252
Model None Epoch 43 Batch 80: Loss 1.466691493988037
Model None Epoch 43 Batch 81: Loss 1.5691094398498535
Model None Epoch 43 Batch 82: Loss 1.5394086837768555
Model None Epoch 43 Batch 83: Loss 1.4235953092575073
Model None Epoch 43 Batch 84: Loss 1.3895010948181152
Model None Epoch 43 Batch 85: Loss 1.5058269500732422
Model None Epoch 43 Batch 86: Loss 1.358757734298706
Model None Epoch 43 Batch 87: Loss 1.4677006006240845
Model None Epoch 43 Batch 88: Loss 1.295832633972168
Model None Epoch 43 Batch 89: Loss 1.4514062404632568
Model None Epoch 43 Batch 90: Loss 1.408034086227417
Model None Epoch 43 Batch 91: Loss 1.3923882246017456
Model None Epoch 43 Batch 92: Loss 1.490802526473999
Model None Epoch 43 Batch 93: Loss 1.412508487701416
Model None Epoch 43 Batch 94: Loss 1.4168858528137207
Model None Epoch 43 Batch 95: Loss 1.4157214164733887
Model None Epoch 43 Batch 96: Loss 1.3377602100372314
Model None Epoch 43 Batch 97: Loss 1.4251158237457275
Model None Epoch 43 Batch 98: Loss 1.3398828506469727
Model None Epoch 43 Batch 99: Loss 1.3483572006225586
Downstream Train Epoch: 43 [25600/50000 (51%)]	Loss: 1.465785
Model None Epoch 43 Batch 100: Loss 1.4657851457595825
Model None Epoch 43 Batch 101: Loss 1.3764020204544067
Model None Epoch 43 Batch 102: Loss 1.4175313711166382
Model None Epoch 43 Batch 103: Loss 1.3474282026290894
Model None Epoch 43 Batch 104: Loss 1.3807718753814697
Model None Epoch 43 Batch 105: Loss 1.4076780080795288
Model None Epoch 43 Batch 106: Loss 1.3830598592758179
Model None Epoch 43 Batch 107: Loss 1.424597144126892
Model None Epoch 43 Batch 108: Loss 1.5699266195297241
Model None Epoch 43 Batch 109: Loss 1.4129365682601929
Model None Epoch 43 Batch 110: Loss 1.4352703094482422
Model None Epoch 43 Batch 111: Loss 1.3547435998916626
Model None Epoch 43 Batch 112: Loss 1.5481984615325928
Model None Epoch 43 Batch 113: Loss 1.406908392906189
Model None Epoch 43 Batch 114: Loss 1.595067024230957
Model None Epoch 43 Batch 115: Loss 1.4181668758392334
Model None Epoch 43 Batch 116: Loss 1.3802241086959839
Model None Epoch 43 Batch 117: Loss 1.4288614988327026
Model None Epoch 43 Batch 118: Loss 1.3290154933929443
Model None Epoch 43 Batch 119: Loss 1.4272093772888184
Model None Epoch 43 Batch 120: Loss 1.4279510974884033
Model None Epoch 43 Batch 121: Loss 1.4916117191314697
Model None Epoch 43 Batch 122: Loss 1.4599677324295044
Model None Epoch 43 Batch 123: Loss 1.4563500881195068
Model None Epoch 43 Batch 124: Loss 1.3951009511947632
Model None Epoch 43 Batch 125: Loss 1.4451208114624023
Model None Epoch 43 Batch 126: Loss 1.3193217515945435
Model None Epoch 43 Batch 127: Loss 1.3494906425476074
Model None Epoch 43 Batch 128: Loss 1.4646282196044922
Model None Epoch 43 Batch 129: Loss 1.5007946491241455
Model None Epoch 43 Batch 130: Loss 1.4020912647247314
Model None Epoch 43 Batch 131: Loss 1.4007363319396973
Model None Epoch 43 Batch 132: Loss 1.425066590309143
Model None Epoch 43 Batch 133: Loss 1.3753893375396729
Model None Epoch 43 Batch 134: Loss 1.4795093536376953
Model None Epoch 43 Batch 135: Loss 1.4233827590942383
Model None Epoch 43 Batch 136: Loss 1.3579715490341187
Model None Epoch 43 Batch 137: Loss 1.3445037603378296
Model None Epoch 43 Batch 138: Loss 1.4665323495864868
Model None Epoch 43 Batch 139: Loss 1.3745450973510742
Model None Epoch 43 Batch 140: Loss 1.3774216175079346
Model None Epoch 43 Batch 141: Loss 1.519628643989563
Model None Epoch 43 Batch 142: Loss 1.3521785736083984
Model None Epoch 43 Batch 143: Loss 1.42258620262146
Model None Epoch 43 Batch 144: Loss 1.4313349723815918
Model None Epoch 43 Batch 145: Loss 1.3326202630996704
Model None Epoch 43 Batch 146: Loss 1.460985779762268
Model None Epoch 43 Batch 147: Loss 1.4434329271316528
Model None Epoch 43 Batch 148: Loss 1.292288064956665
Model None Epoch 43 Batch 149: Loss 1.3603706359863281
Downstream Train Epoch: 43 [38400/50000 (77%)]	Loss: 1.397383
Model None Epoch 43 Batch 150: Loss 1.3973828554153442
Model None Epoch 43 Batch 151: Loss 1.324937343597412
Model None Epoch 43 Batch 152: Loss 1.3624919652938843
Model None Epoch 43 Batch 153: Loss 1.3465945720672607
Model None Epoch 43 Batch 154: Loss 1.46858549118042
Model None Epoch 43 Batch 155: Loss 1.4029468297958374
Model None Epoch 43 Batch 156: Loss 1.2385255098342896
Model None Epoch 43 Batch 157: Loss 1.4081166982650757
Model None Epoch 43 Batch 158: Loss 1.3740307092666626
Model None Epoch 43 Batch 159: Loss 1.451669692993164
Model None Epoch 43 Batch 160: Loss 1.3977264165878296
Model None Epoch 43 Batch 161: Loss 1.4304050207138062
Model None Epoch 43 Batch 162: Loss 1.4332597255706787
Model None Epoch 43 Batch 163: Loss 1.2918199300765991
Model None Epoch 43 Batch 164: Loss 1.4132659435272217
Model None Epoch 43 Batch 165: Loss 1.4770275354385376
Model None Epoch 43 Batch 166: Loss 1.534132719039917
Model None Epoch 43 Batch 167: Loss 1.3303171396255493
Model None Epoch 43 Batch 168: Loss 1.4290130138397217
Model None Epoch 43 Batch 169: Loss 1.2867933511734009
Model None Epoch 43 Batch 170: Loss 1.4074444770812988
Model None Epoch 43 Batch 171: Loss 1.3443012237548828
Model None Epoch 43 Batch 172: Loss 1.4182432889938354
Model None Epoch 43 Batch 173: Loss 1.357155442237854
Model None Epoch 43 Batch 174: Loss 1.4644464254379272
Model None Epoch 43 Batch 175: Loss 1.332040548324585
Model None Epoch 43 Batch 176: Loss 1.401245355606079
Model None Epoch 43 Batch 177: Loss 1.382237434387207
Model None Epoch 43 Batch 178: Loss 1.4300658702850342
Model None Epoch 43 Batch 179: Loss 1.4082764387130737
Model None Epoch 43 Batch 180: Loss 1.418276071548462
Model None Epoch 43 Batch 181: Loss 1.4533264636993408
Model None Epoch 43 Batch 182: Loss 1.3556221723556519
Model None Epoch 43 Batch 183: Loss 1.4869556427001953
Model None Epoch 43 Batch 184: Loss 1.3876646757125854
Model None Epoch 43 Batch 185: Loss 1.3303636312484741
Model None Epoch 43 Batch 186: Loss 1.3799443244934082
Model None Epoch 43 Batch 187: Loss 1.4036781787872314
Model None Epoch 43 Batch 188: Loss 1.5212346315383911
Model None Epoch 43 Batch 189: Loss 1.3992834091186523
Model None Epoch 43 Batch 190: Loss 1.4054511785507202
Model None Epoch 43 Batch 191: Loss 1.568745493888855
Model None Epoch 43 Batch 192: Loss 1.5067176818847656
Model None Epoch 43 Batch 193: Loss 1.2983593940734863
Model None Epoch 43 Batch 194: Loss 1.3308336734771729
Model None Epoch 43 Batch 195: Loss 1.36961030960083

 Downstream Train loss: 1.408972914121589 Acc: 0.5655
Downstream Train Epoch: 44 [0/50000 (0%)]	Loss: 1.474869
Model None Epoch 44 Batch 0: Loss 1.4748685359954834
Model None Epoch 44 Batch 1: Loss 1.3754847049713135
Model None Epoch 44 Batch 2: Loss 1.3452688455581665
Model None Epoch 44 Batch 3: Loss 1.402200698852539
Model None Epoch 44 Batch 4: Loss 1.327225685119629
Model None Epoch 44 Batch 5: Loss 1.4189369678497314
Model None Epoch 44 Batch 6: Loss 1.4412810802459717
Model None Epoch 44 Batch 7: Loss 1.3570539951324463
Model None Epoch 44 Batch 8: Loss 1.3530197143554688
Model None Epoch 44 Batch 9: Loss 1.3701725006103516
Model None Epoch 44 Batch 10: Loss 1.4538419246673584
Model None Epoch 44 Batch 11: Loss 1.3524856567382812
Model None Epoch 44 Batch 12: Loss 1.3530977964401245
Model None Epoch 44 Batch 13: Loss 1.4168317317962646
Model None Epoch 44 Batch 14: Loss 1.3243136405944824
Model None Epoch 44 Batch 15: Loss 1.365208625793457
Model None Epoch 44 Batch 16: Loss 1.38154137134552
Model None Epoch 44 Batch 17: Loss 1.4164650440216064
Model None Epoch 44 Batch 18: Loss 1.3883777856826782
Model None Epoch 44 Batch 19: Loss 1.3551758527755737
Model None Epoch 44 Batch 20: Loss 1.4694666862487793
Model None Epoch 44 Batch 21: Loss 1.5178685188293457
Model None Epoch 44 Batch 22: Loss 1.5024316310882568
Model None Epoch 44 Batch 23: Loss 1.5776450634002686
Model None Epoch 44 Batch 24: Loss 1.4590109586715698
Model None Epoch 44 Batch 25: Loss 1.3113276958465576
Model None Epoch 44 Batch 26: Loss 1.479636311531067
Model None Epoch 44 Batch 27: Loss 1.588340163230896
Model None Epoch 44 Batch 28: Loss 1.3596910238265991
Model None Epoch 44 Batch 29: Loss 1.3235875368118286
Model None Epoch 44 Batch 30: Loss 1.4219021797180176
Model None Epoch 44 Batch 31: Loss 1.37394118309021
Model None Epoch 44 Batch 32: Loss 1.3557164669036865
Model None Epoch 44 Batch 33: Loss 1.4475739002227783
Model None Epoch 44 Batch 34: Loss 1.4012433290481567
Model None Epoch 44 Batch 35: Loss 1.4747929573059082
Model None Epoch 44 Batch 36: Loss 1.4616869688034058
Model None Epoch 44 Batch 37: Loss 1.3301537036895752
Model None Epoch 44 Batch 38: Loss 1.3855371475219727
Model None Epoch 44 Batch 39: Loss 1.2896397113800049
Model None Epoch 44 Batch 40: Loss 1.4104894399642944
Model None Epoch 44 Batch 41: Loss 1.54933762550354
Model None Epoch 44 Batch 42: Loss 1.2780307531356812
Model None Epoch 44 Batch 43: Loss 1.4474806785583496
Model None Epoch 44 Batch 44: Loss 1.315782070159912
Model None Epoch 44 Batch 45: Loss 1.4923261404037476
Model None Epoch 44 Batch 46: Loss 1.314110517501831
Model None Epoch 44 Batch 47: Loss 1.376768946647644
Model None Epoch 44 Batch 48: Loss 1.4895743131637573
Model None Epoch 44 Batch 49: Loss 1.285264492034912
Downstream Train Epoch: 44 [12800/50000 (26%)]	Loss: 1.284147
Model None Epoch 44 Batch 50: Loss 1.2841473817825317
Model None Epoch 44 Batch 51: Loss 1.4048243761062622
Model None Epoch 44 Batch 52: Loss 1.4738818407058716
Model None Epoch 44 Batch 53: Loss 1.3577698469161987
Model None Epoch 44 Batch 54: Loss 1.388497233390808
Model None Epoch 44 Batch 55: Loss 1.3871705532073975
Model None Epoch 44 Batch 56: Loss 1.372827410697937
Model None Epoch 44 Batch 57: Loss 1.3784692287445068
Model None Epoch 44 Batch 58: Loss 1.3870570659637451
Model None Epoch 44 Batch 59: Loss 1.4229661226272583
Model None Epoch 44 Batch 60: Loss 1.3960959911346436
Model None Epoch 44 Batch 61: Loss 1.4212610721588135
Model None Epoch 44 Batch 62: Loss 1.4391889572143555
Model None Epoch 44 Batch 63: Loss 1.4163899421691895
Model None Epoch 44 Batch 64: Loss 1.2795964479446411
Model None Epoch 44 Batch 65: Loss 1.275739312171936
Model None Epoch 44 Batch 66: Loss 1.3920413255691528
Model None Epoch 44 Batch 67: Loss 1.3960285186767578
Model None Epoch 44 Batch 68: Loss 1.329817771911621
Model None Epoch 44 Batch 69: Loss 1.3995805978775024
Model None Epoch 44 Batch 70: Loss 1.4336273670196533
Model None Epoch 44 Batch 71: Loss 1.499951958656311
Model None Epoch 44 Batch 72: Loss 1.4365322589874268
Model None Epoch 44 Batch 73: Loss 1.4066550731658936
Model None Epoch 44 Batch 74: Loss 1.4419751167297363
Model None Epoch 44 Batch 75: Loss 1.410346269607544
Model None Epoch 44 Batch 76: Loss 1.439584732055664
Model None Epoch 44 Batch 77: Loss 1.3292458057403564
Model None Epoch 44 Batch 78: Loss 1.4080060720443726
Model None Epoch 44 Batch 79: Loss 1.3249413967132568
Model None Epoch 44 Batch 80: Loss 1.3990405797958374
Model None Epoch 44 Batch 81: Loss 1.4582096338272095
Model None Epoch 44 Batch 82: Loss 1.3343795537948608
Model None Epoch 44 Batch 83: Loss 1.3238990306854248
Model None Epoch 44 Batch 84: Loss 1.434143304824829
Model None Epoch 44 Batch 85: Loss 1.4371567964553833
Model None Epoch 44 Batch 86: Loss 1.4006181955337524
Model None Epoch 44 Batch 87: Loss 1.4330838918685913
Model None Epoch 44 Batch 88: Loss 1.4716907739639282
Model None Epoch 44 Batch 89: Loss 1.3394569158554077
Model None Epoch 44 Batch 90: Loss 1.3690874576568604
Model None Epoch 44 Batch 91: Loss 1.4736404418945312
Model None Epoch 44 Batch 92: Loss 1.3725496530532837
Model None Epoch 44 Batch 93: Loss 1.3276927471160889
Model None Epoch 44 Batch 94: Loss 1.458579659461975
Model None Epoch 44 Batch 95: Loss 1.511450171470642
Model None Epoch 44 Batch 96: Loss 1.4641892910003662
Model None Epoch 44 Batch 97: Loss 1.548117995262146
Model None Epoch 44 Batch 98: Loss 1.4206136465072632
Model None Epoch 44 Batch 99: Loss 1.3613406419754028
Downstream Train Epoch: 44 [25600/50000 (51%)]	Loss: 1.370160
Model None Epoch 44 Batch 100: Loss 1.37015962600708
Model None Epoch 44 Batch 101: Loss 1.4075533151626587
Model None Epoch 44 Batch 102: Loss 1.3861722946166992
Model None Epoch 44 Batch 103: Loss 1.5143015384674072
Model None Epoch 44 Batch 104: Loss 1.3501032590866089
Model None Epoch 44 Batch 105: Loss 1.4173781871795654
Model None Epoch 44 Batch 106: Loss 1.4143518209457397
Model None Epoch 44 Batch 107: Loss 1.441021203994751
Model None Epoch 44 Batch 108: Loss 1.4566298723220825
Model None Epoch 44 Batch 109: Loss 1.2968418598175049
Model None Epoch 44 Batch 110: Loss 1.388056993484497
Model None Epoch 44 Batch 111: Loss 1.4134001731872559
Model None Epoch 44 Batch 112: Loss 1.3818246126174927
Model None Epoch 44 Batch 113: Loss 1.462213158607483
Model None Epoch 44 Batch 114: Loss 1.5441440343856812
Model None Epoch 44 Batch 115: Loss 1.3679989576339722
Model None Epoch 44 Batch 116: Loss 1.4659740924835205
Model None Epoch 44 Batch 117: Loss 1.4434624910354614
Model None Epoch 44 Batch 118: Loss 1.4860635995864868
Model None Epoch 44 Batch 119: Loss 1.4820969104766846
Model None Epoch 44 Batch 120: Loss 1.4086681604385376
Model None Epoch 44 Batch 121: Loss 1.3963699340820312
Model None Epoch 44 Batch 122: Loss 1.2990819215774536
Model None Epoch 44 Batch 123: Loss 1.44134521484375
Model None Epoch 44 Batch 124: Loss 1.4805816411972046
Model None Epoch 44 Batch 125: Loss 1.4052435159683228
Model None Epoch 44 Batch 126: Loss 1.4152119159698486
Model None Epoch 44 Batch 127: Loss 1.2990467548370361
Model None Epoch 44 Batch 128: Loss 1.368036150932312
Model None Epoch 44 Batch 129: Loss 1.3535083532333374
Model None Epoch 44 Batch 130: Loss 1.414981722831726
Model None Epoch 44 Batch 131: Loss 1.3472768068313599
Model None Epoch 44 Batch 132: Loss 1.3711789846420288
Model None Epoch 44 Batch 133: Loss 1.453944206237793
Model None Epoch 44 Batch 134: Loss 1.5151222944259644
Model None Epoch 44 Batch 135: Loss 1.4110705852508545
Model None Epoch 44 Batch 136: Loss 1.4283744096755981
Model None Epoch 44 Batch 137: Loss 1.4277515411376953
Model None Epoch 44 Batch 138: Loss 1.3170863389968872
Model None Epoch 44 Batch 139: Loss 1.4477884769439697
Model None Epoch 44 Batch 140: Loss 1.3933165073394775
Model None Epoch 44 Batch 141: Loss 1.4846255779266357
Model None Epoch 44 Batch 142: Loss 1.3830366134643555
Model None Epoch 44 Batch 143: Loss 1.4561233520507812
Model None Epoch 44 Batch 144: Loss 1.481702446937561
Model None Epoch 44 Batch 145: Loss 1.4021966457366943
Model None Epoch 44 Batch 146: Loss 1.3472827672958374
Model None Epoch 44 Batch 147: Loss 1.5636012554168701
Model None Epoch 44 Batch 148: Loss 1.3555835485458374
Model None Epoch 44 Batch 149: Loss 1.334652066230774
Downstream Train Epoch: 44 [38400/50000 (77%)]	Loss: 1.538914
Model None Epoch 44 Batch 150: Loss 1.5389139652252197
Model None Epoch 44 Batch 151: Loss 1.4371284246444702
Model None Epoch 44 Batch 152: Loss 1.2854394912719727
Model None Epoch 44 Batch 153: Loss 1.2992539405822754
Model None Epoch 44 Batch 154: Loss 1.4321693181991577
Model None Epoch 44 Batch 155: Loss 1.4416260719299316
Model None Epoch 44 Batch 156: Loss 1.3803685903549194
Model None Epoch 44 Batch 157: Loss 1.4793047904968262
Model None Epoch 44 Batch 158: Loss 1.4486042261123657
Model None Epoch 44 Batch 159: Loss 1.409378170967102
Model None Epoch 44 Batch 160: Loss 1.3345177173614502
Model None Epoch 44 Batch 161: Loss 1.4733151197433472
Model None Epoch 44 Batch 162: Loss 1.5094704627990723
Model None Epoch 44 Batch 163: Loss 1.4312585592269897
Model None Epoch 44 Batch 164: Loss 1.397447943687439
Model None Epoch 44 Batch 165: Loss 1.5197300910949707
Model None Epoch 44 Batch 166: Loss 1.3975881338119507
Model None Epoch 44 Batch 167: Loss 1.4519407749176025
Model None Epoch 44 Batch 168: Loss 1.4157923460006714
Model None Epoch 44 Batch 169: Loss 1.3434691429138184
Model None Epoch 44 Batch 170: Loss 1.400177240371704
Model None Epoch 44 Batch 171: Loss 1.358782410621643
Model None Epoch 44 Batch 172: Loss 1.3684598207473755
Model None Epoch 44 Batch 173: Loss 1.379249930381775
Model None Epoch 44 Batch 174: Loss 1.4392797946929932
Model None Epoch 44 Batch 175: Loss 1.3922544717788696
Model None Epoch 44 Batch 176: Loss 1.3164284229278564
Model None Epoch 44 Batch 177: Loss 1.3854628801345825
Model None Epoch 44 Batch 178: Loss 1.3842618465423584
Model None Epoch 44 Batch 179: Loss 1.4040372371673584
Model None Epoch 44 Batch 180: Loss 1.4396605491638184
Model None Epoch 44 Batch 181: Loss 1.3442648649215698
Model None Epoch 44 Batch 182: Loss 1.3798093795776367
Model None Epoch 44 Batch 183: Loss 1.3996756076812744
Model None Epoch 44 Batch 184: Loss 1.4862511157989502
Model None Epoch 44 Batch 185: Loss 1.45896315574646
Model None Epoch 44 Batch 186: Loss 1.4717159271240234
Model None Epoch 44 Batch 187: Loss 1.4305320978164673
Model None Epoch 44 Batch 188: Loss 1.272359013557434
Model None Epoch 44 Batch 189: Loss 1.3095885515213013
Model None Epoch 44 Batch 190: Loss 1.49387788772583
Model None Epoch 44 Batch 191: Loss 1.5219299793243408
Model None Epoch 44 Batch 192: Loss 1.4062230587005615
Model None Epoch 44 Batch 193: Loss 1.4345418214797974
Model None Epoch 44 Batch 194: Loss 1.404103398323059
Model None Epoch 44 Batch 195: Loss 1.4309394359588623

 Downstream Train loss: 1.4074972071209733 Acc: 0.5655
Downstream Train Epoch: 45 [0/50000 (0%)]	Loss: 1.353737
Model None Epoch 45 Batch 0: Loss 1.3537373542785645
Model None Epoch 45 Batch 1: Loss 1.442361831665039
Model None Epoch 45 Batch 2: Loss 1.3373786211013794
Model None Epoch 45 Batch 3: Loss 1.3113094568252563
Model None Epoch 45 Batch 4: Loss 1.423224687576294
Model None Epoch 45 Batch 5: Loss 1.415826439857483
Model None Epoch 45 Batch 6: Loss 1.3717018365859985
Model None Epoch 45 Batch 7: Loss 1.3092730045318604
Model None Epoch 45 Batch 8: Loss 1.5176465511322021
Model None Epoch 45 Batch 9: Loss 1.3430382013320923
Model None Epoch 45 Batch 10: Loss 1.4183577299118042
Model None Epoch 45 Batch 11: Loss 1.2973815202713013
Model None Epoch 45 Batch 12: Loss 1.420046329498291
Model None Epoch 45 Batch 13: Loss 1.4000575542449951
Model None Epoch 45 Batch 14: Loss 1.4998104572296143
Model None Epoch 45 Batch 15: Loss 1.485424280166626
Model None Epoch 45 Batch 16: Loss 1.4477941989898682
Model None Epoch 45 Batch 17: Loss 1.5038899183273315
Model None Epoch 45 Batch 18: Loss 1.3716572523117065
Model None Epoch 45 Batch 19: Loss 1.5378227233886719
Model None Epoch 45 Batch 20: Loss 1.4722926616668701
Model None Epoch 45 Batch 21: Loss 1.3469338417053223
Model None Epoch 45 Batch 22: Loss 1.3941110372543335
Model None Epoch 45 Batch 23: Loss 1.4202510118484497
Model None Epoch 45 Batch 24: Loss 1.240727424621582
Model None Epoch 45 Batch 25: Loss 1.4120066165924072
Model None Epoch 45 Batch 26: Loss 1.433401107788086
Model None Epoch 45 Batch 27: Loss 1.2509956359863281
Model None Epoch 45 Batch 28: Loss 1.568245768547058
Model None Epoch 45 Batch 29: Loss 1.4796890020370483
Model None Epoch 45 Batch 30: Loss 1.3569790124893188
Model None Epoch 45 Batch 31: Loss 1.5287857055664062
Model None Epoch 45 Batch 32: Loss 1.395757794380188
Model None Epoch 45 Batch 33: Loss 1.4504622220993042
Model None Epoch 45 Batch 34: Loss 1.3526045083999634
Model None Epoch 45 Batch 35: Loss 1.3915990591049194
Model None Epoch 45 Batch 36: Loss 1.3702075481414795
Model None Epoch 45 Batch 37: Loss 1.385465383529663
Model None Epoch 45 Batch 38: Loss 1.3007338047027588
Model None Epoch 45 Batch 39: Loss 1.475956916809082
Model None Epoch 45 Batch 40: Loss 1.4638265371322632
Model None Epoch 45 Batch 41: Loss 1.3693547248840332
Model None Epoch 45 Batch 42: Loss 1.4779001474380493
Model None Epoch 45 Batch 43: Loss 1.4243037700653076
Model None Epoch 45 Batch 44: Loss 1.4375416040420532
Model None Epoch 45 Batch 45: Loss 1.5138047933578491
Model None Epoch 45 Batch 46: Loss 1.357394814491272
Model None Epoch 45 Batch 47: Loss 1.5131698846817017
Model None Epoch 45 Batch 48: Loss 1.4110627174377441
Model None Epoch 45 Batch 49: Loss 1.434751033782959
Downstream Train Epoch: 45 [12800/50000 (26%)]	Loss: 1.313720
Model None Epoch 45 Batch 50: Loss 1.3137197494506836
Model None Epoch 45 Batch 51: Loss 1.3461692333221436
Model None Epoch 45 Batch 52: Loss 1.377419352531433
Model None Epoch 45 Batch 53: Loss 1.255637764930725
Model None Epoch 45 Batch 54: Loss 1.3875797986984253
Model None Epoch 45 Batch 55: Loss 1.3513859510421753
Model None Epoch 45 Batch 56: Loss 1.400137186050415
Model None Epoch 45 Batch 57: Loss 1.3523074388504028
Model None Epoch 45 Batch 58: Loss 1.4391865730285645
Model None Epoch 45 Batch 59: Loss 1.3367257118225098
Model None Epoch 45 Batch 60: Loss 1.3852602243423462
Model None Epoch 45 Batch 61: Loss 1.3537991046905518
Model None Epoch 45 Batch 62: Loss 1.4614672660827637
Model None Epoch 45 Batch 63: Loss 1.3763164281845093
Model None Epoch 45 Batch 64: Loss 1.352624773979187
Model None Epoch 45 Batch 65: Loss 1.3807289600372314
Model None Epoch 45 Batch 66: Loss 1.3203402757644653
Model None Epoch 45 Batch 67: Loss 1.5903301239013672
Model None Epoch 45 Batch 68: Loss 1.3864145278930664
Model None Epoch 45 Batch 69: Loss 1.4172817468643188
Model None Epoch 45 Batch 70: Loss 1.3246170282363892
Model None Epoch 45 Batch 71: Loss 1.497817873954773
Model None Epoch 45 Batch 72: Loss 1.4320428371429443
Model None Epoch 45 Batch 73: Loss 1.4232279062271118
Model None Epoch 45 Batch 74: Loss 1.3767210245132446
Model None Epoch 45 Batch 75: Loss 1.463544487953186
Model None Epoch 45 Batch 76: Loss 1.38070809841156
Model None Epoch 45 Batch 77: Loss 1.4232994318008423
Model None Epoch 45 Batch 78: Loss 1.4636861085891724
Model None Epoch 45 Batch 79: Loss 1.3618099689483643
Model None Epoch 45 Batch 80: Loss 1.3708909749984741
Model None Epoch 45 Batch 81: Loss 1.4141716957092285
Model None Epoch 45 Batch 82: Loss 1.354749321937561
Model None Epoch 45 Batch 83: Loss 1.4328266382217407
Model None Epoch 45 Batch 84: Loss 1.4048383235931396
Model None Epoch 45 Batch 85: Loss 1.303687334060669
Model None Epoch 45 Batch 86: Loss 1.3276503086090088
Model None Epoch 45 Batch 87: Loss 1.446535348892212
Model None Epoch 45 Batch 88: Loss 1.3628793954849243
Model None Epoch 45 Batch 89: Loss 1.3903568983078003
Model None Epoch 45 Batch 90: Loss 1.3503341674804688
Model None Epoch 45 Batch 91: Loss 1.3403319120407104
Model None Epoch 45 Batch 92: Loss 1.389901876449585
Model None Epoch 45 Batch 93: Loss 1.4035766124725342
Model None Epoch 45 Batch 94: Loss 1.3329638242721558
Model None Epoch 45 Batch 95: Loss 1.3990743160247803
Model None Epoch 45 Batch 96: Loss 1.3247647285461426
Model None Epoch 45 Batch 97: Loss 1.437758207321167
Model None Epoch 45 Batch 98: Loss 1.4306516647338867
Model None Epoch 45 Batch 99: Loss 1.5719314813613892
Downstream Train Epoch: 45 [25600/50000 (51%)]	Loss: 1.440474
Model None Epoch 45 Batch 100: Loss 1.4404739141464233
Model None Epoch 45 Batch 101: Loss 1.4195904731750488
Model None Epoch 45 Batch 102: Loss 1.3898066282272339
Model None Epoch 45 Batch 103: Loss 1.473970651626587
Model None Epoch 45 Batch 104: Loss 1.398202896118164
Model None Epoch 45 Batch 105: Loss 1.4571665525436401
Model None Epoch 45 Batch 106: Loss 1.37812340259552
Model None Epoch 45 Batch 107: Loss 1.4681856632232666
Model None Epoch 45 Batch 108: Loss 1.4938671588897705
Model None Epoch 45 Batch 109: Loss 1.4239449501037598
Model None Epoch 45 Batch 110: Loss 1.4909374713897705
Model None Epoch 45 Batch 111: Loss 1.3694350719451904
Model None Epoch 45 Batch 112: Loss 1.4302899837493896
Model None Epoch 45 Batch 113: Loss 1.2647775411605835
Model None Epoch 45 Batch 114: Loss 1.4011038541793823
Model None Epoch 45 Batch 115: Loss 1.4430325031280518
Model None Epoch 45 Batch 116: Loss 1.3535728454589844
Model None Epoch 45 Batch 117: Loss 1.4722316265106201
Model None Epoch 45 Batch 118: Loss 1.430846929550171
Model None Epoch 45 Batch 119: Loss 1.3947960138320923
Model None Epoch 45 Batch 120: Loss 1.3763865232467651
Model None Epoch 45 Batch 121: Loss 1.408671259880066
Model None Epoch 45 Batch 122: Loss 1.4236305952072144
Model None Epoch 45 Batch 123: Loss 1.4764443635940552
Model None Epoch 45 Batch 124: Loss 1.307337999343872
Model None Epoch 45 Batch 125: Loss 1.4227969646453857
Model None Epoch 45 Batch 126: Loss 1.3402674198150635
Model None Epoch 45 Batch 127: Loss 1.4233468770980835
Model None Epoch 45 Batch 128: Loss 1.2985728979110718
Model None Epoch 45 Batch 129: Loss 1.46807861328125
Model None Epoch 45 Batch 130: Loss 1.536147952079773
Model None Epoch 45 Batch 131: Loss 1.5179109573364258
Model None Epoch 45 Batch 132: Loss 1.4218790531158447
Model None Epoch 45 Batch 133: Loss 1.2548267841339111
Model None Epoch 45 Batch 134: Loss 1.4831541776657104
Model None Epoch 45 Batch 135: Loss 1.4129114151000977
Model None Epoch 45 Batch 136: Loss 1.3966963291168213
Model None Epoch 45 Batch 137: Loss 1.4896106719970703
Model None Epoch 45 Batch 138: Loss 1.3597322702407837
Model None Epoch 45 Batch 139: Loss 1.4161503314971924
Model None Epoch 45 Batch 140: Loss 1.295992136001587
Model None Epoch 45 Batch 141: Loss 1.49135160446167
Model None Epoch 45 Batch 142: Loss 1.3225438594818115
Model None Epoch 45 Batch 143: Loss 1.3789565563201904
Model None Epoch 45 Batch 144: Loss 1.3471213579177856
Model None Epoch 45 Batch 145: Loss 1.4324452877044678
Model None Epoch 45 Batch 146: Loss 1.519919991493225
Model None Epoch 45 Batch 147: Loss 1.339475154876709
Model None Epoch 45 Batch 148: Loss 1.3800508975982666
Model None Epoch 45 Batch 149: Loss 1.4864176511764526
Downstream Train Epoch: 45 [38400/50000 (77%)]	Loss: 1.400337
Model None Epoch 45 Batch 150: Loss 1.400336742401123
Model None Epoch 45 Batch 151: Loss 1.4619771242141724
Model None Epoch 45 Batch 152: Loss 1.3585549592971802
Model None Epoch 45 Batch 153: Loss 1.4773820638656616
Model None Epoch 45 Batch 154: Loss 1.4232842922210693
Model None Epoch 45 Batch 155: Loss 1.3380635976791382
Model None Epoch 45 Batch 156: Loss 1.3795289993286133
Model None Epoch 45 Batch 157: Loss 1.2693241834640503
Model None Epoch 45 Batch 158: Loss 1.4562233686447144
Model None Epoch 45 Batch 159: Loss 1.3882956504821777
Model None Epoch 45 Batch 160: Loss 1.4525787830352783
Model None Epoch 45 Batch 161: Loss 1.4921382665634155
Model None Epoch 45 Batch 162: Loss 1.3652749061584473
Model None Epoch 45 Batch 163: Loss 1.4275113344192505
Model None Epoch 45 Batch 164: Loss 1.4635251760482788
Model None Epoch 45 Batch 165: Loss 1.2581820487976074
Model None Epoch 45 Batch 166: Loss 1.4626396894454956
Model None Epoch 45 Batch 167: Loss 1.2702034711837769
Model None Epoch 45 Batch 168: Loss 1.4346877336502075
Model None Epoch 45 Batch 169: Loss 1.427740216255188
Model None Epoch 45 Batch 170: Loss 1.427290439605713
Model None Epoch 45 Batch 171: Loss 1.4076403379440308
Model None Epoch 45 Batch 172: Loss 1.3496077060699463
Model None Epoch 45 Batch 173: Loss 1.3260295391082764
Model None Epoch 45 Batch 174: Loss 1.4348554611206055
Model None Epoch 45 Batch 175: Loss 1.431907057762146
Model None Epoch 45 Batch 176: Loss 1.3925858736038208
Model None Epoch 45 Batch 177: Loss 1.414581537246704
Model None Epoch 45 Batch 178: Loss 1.387434959411621
Model None Epoch 45 Batch 179: Loss 1.5033862590789795
Model None Epoch 45 Batch 180: Loss 1.3445496559143066
Model None Epoch 45 Batch 181: Loss 1.4622924327850342
Model None Epoch 45 Batch 182: Loss 1.3901082277297974
Model None Epoch 45 Batch 183: Loss 1.3103606700897217
Model None Epoch 45 Batch 184: Loss 1.3670531511306763
Model None Epoch 45 Batch 185: Loss 1.3222599029541016
Model None Epoch 45 Batch 186: Loss 1.4243983030319214
Model None Epoch 45 Batch 187: Loss 1.4290797710418701
Model None Epoch 45 Batch 188: Loss 1.4504151344299316
Model None Epoch 45 Batch 189: Loss 1.3583402633666992
Model None Epoch 45 Batch 190: Loss 1.4192917346954346
Model None Epoch 45 Batch 191: Loss 1.3935763835906982
Model None Epoch 45 Batch 192: Loss 1.3856568336486816
Model None Epoch 45 Batch 193: Loss 1.3905391693115234
Model None Epoch 45 Batch 194: Loss 1.2679471969604492
Model None Epoch 45 Batch 195: Loss 1.3768794536590576

 Downstream Train loss: 1.4028517457903649 Acc: 0.5655
Downstream Train Epoch: 46 [0/50000 (0%)]	Loss: 1.381863
Model None Epoch 46 Batch 0: Loss 1.3818625211715698
Model None Epoch 46 Batch 1: Loss 1.4020951986312866
Model None Epoch 46 Batch 2: Loss 1.3166759014129639
Model None Epoch 46 Batch 3: Loss 1.4193320274353027
Model None Epoch 46 Batch 4: Loss 1.3022189140319824
Model None Epoch 46 Batch 5: Loss 1.442368984222412
Model None Epoch 46 Batch 6: Loss 1.4540694952011108
Model None Epoch 46 Batch 7: Loss 1.460032343864441
Model None Epoch 46 Batch 8: Loss 1.3405629396438599
Model None Epoch 46 Batch 9: Loss 1.505283236503601
Model None Epoch 46 Batch 10: Loss 1.5177890062332153
Model None Epoch 46 Batch 11: Loss 1.1988856792449951
Model None Epoch 46 Batch 12: Loss 1.4558666944503784
Model None Epoch 46 Batch 13: Loss 1.4311200380325317
Model None Epoch 46 Batch 14: Loss 1.4280779361724854
Model None Epoch 46 Batch 15: Loss 1.2662057876586914
Model None Epoch 46 Batch 16: Loss 1.4808224439620972
Model None Epoch 46 Batch 17: Loss 1.429252028465271
Model None Epoch 46 Batch 18: Loss 1.4388816356658936
Model None Epoch 46 Batch 19: Loss 1.4250880479812622
Model None Epoch 46 Batch 20: Loss 1.3711336851119995
Model None Epoch 46 Batch 21: Loss 1.2683011293411255
Model None Epoch 46 Batch 22: Loss 1.3000127077102661
Model None Epoch 46 Batch 23: Loss 1.396731972694397
Model None Epoch 46 Batch 24: Loss 1.3691256046295166
Model None Epoch 46 Batch 25: Loss 1.2238811254501343
Model None Epoch 46 Batch 26: Loss 1.378188133239746
Model None Epoch 46 Batch 27: Loss 1.4181205034255981
Model None Epoch 46 Batch 28: Loss 1.4091168642044067
Model None Epoch 46 Batch 29: Loss 1.4208340644836426
Model None Epoch 46 Batch 30: Loss 1.4677541255950928
Model None Epoch 46 Batch 31: Loss 1.477996587753296
Model None Epoch 46 Batch 32: Loss 1.4519712924957275
Model None Epoch 46 Batch 33: Loss 1.4228615760803223
Model None Epoch 46 Batch 34: Loss 1.408483624458313
Model None Epoch 46 Batch 35: Loss 1.3390926122665405
Model None Epoch 46 Batch 36: Loss 1.2620435953140259
Model None Epoch 46 Batch 37: Loss 1.3760381937026978
Model None Epoch 46 Batch 38: Loss 1.3307361602783203
Model None Epoch 46 Batch 39: Loss 1.4277690649032593
Model None Epoch 46 Batch 40: Loss 1.4752098321914673
Model None Epoch 46 Batch 41: Loss 1.5370454788208008
Model None Epoch 46 Batch 42: Loss 1.465325117111206
Model None Epoch 46 Batch 43: Loss 1.4054644107818604
Model None Epoch 46 Batch 44: Loss 1.515800952911377
Model None Epoch 46 Batch 45: Loss 1.52688467502594
Model None Epoch 46 Batch 46: Loss 1.352294683456421
Model None Epoch 46 Batch 47: Loss 1.4288753271102905
Model None Epoch 46 Batch 48: Loss 1.3889892101287842
Model None Epoch 46 Batch 49: Loss 1.3613754510879517
Downstream Train Epoch: 46 [12800/50000 (26%)]	Loss: 1.413303
Model None Epoch 46 Batch 50: Loss 1.413303017616272
Model None Epoch 46 Batch 51: Loss 1.441906213760376
Model None Epoch 46 Batch 52: Loss 1.356688141822815
Model None Epoch 46 Batch 53: Loss 1.400528907775879
Model None Epoch 46 Batch 54: Loss 1.408378005027771
Model None Epoch 46 Batch 55: Loss 1.34718656539917
Model None Epoch 46 Batch 56: Loss 1.3474863767623901
Model None Epoch 46 Batch 57: Loss 1.4323333501815796
Model None Epoch 46 Batch 58: Loss 1.3221551179885864
Model None Epoch 46 Batch 59: Loss 1.4228236675262451
Model None Epoch 46 Batch 60: Loss 1.3124867677688599
Model None Epoch 46 Batch 61: Loss 1.402891993522644
Model None Epoch 46 Batch 62: Loss 1.374782919883728
Model None Epoch 46 Batch 63: Loss 1.2853848934173584
Model None Epoch 46 Batch 64: Loss 1.4227159023284912
Model None Epoch 46 Batch 65: Loss 1.37899649143219
Model None Epoch 46 Batch 66: Loss 1.3059498071670532
Model None Epoch 46 Batch 67: Loss 1.4724303483963013
Model None Epoch 46 Batch 68: Loss 1.3102308511734009
Model None Epoch 46 Batch 69: Loss 1.3926368951797485
Model None Epoch 46 Batch 70: Loss 1.5011907815933228
Model None Epoch 46 Batch 71: Loss 1.4574435949325562
Model None Epoch 46 Batch 72: Loss 1.4298325777053833
Model None Epoch 46 Batch 73: Loss 1.3818411827087402
Model None Epoch 46 Batch 74: Loss 1.2254033088684082
Model None Epoch 46 Batch 75: Loss 1.4546955823898315
Model None Epoch 46 Batch 76: Loss 1.3556187152862549
Model None Epoch 46 Batch 77: Loss 1.417922854423523
Model None Epoch 46 Batch 78: Loss 1.4248048067092896
Model None Epoch 46 Batch 79: Loss 1.262599229812622
Model None Epoch 46 Batch 80: Loss 1.4404551982879639
Model None Epoch 46 Batch 81: Loss 1.400381088256836
Model None Epoch 46 Batch 82: Loss 1.3356084823608398
Model None Epoch 46 Batch 83: Loss 1.501685619354248
Model None Epoch 46 Batch 84: Loss 1.4140828847885132
Model None Epoch 46 Batch 85: Loss 1.3463325500488281
Model None Epoch 46 Batch 86: Loss 1.3348833322525024
Model None Epoch 46 Batch 87: Loss 1.3769619464874268
Model None Epoch 46 Batch 88: Loss 1.308998942375183
Model None Epoch 46 Batch 89: Loss 1.3613699674606323
Model None Epoch 46 Batch 90: Loss 1.4355591535568237
Model None Epoch 46 Batch 91: Loss 1.354193091392517
Model None Epoch 46 Batch 92: Loss 1.5105218887329102
Model None Epoch 46 Batch 93: Loss 1.3720910549163818
Model None Epoch 46 Batch 94: Loss 1.4461665153503418
Model None Epoch 46 Batch 95: Loss 1.4279221296310425
Model None Epoch 46 Batch 96: Loss 1.3590168952941895
Model None Epoch 46 Batch 97: Loss 1.4268964529037476
Model None Epoch 46 Batch 98: Loss 1.326086163520813
Model None Epoch 46 Batch 99: Loss 1.4174869060516357
Downstream Train Epoch: 46 [25600/50000 (51%)]	Loss: 1.588708
Model None Epoch 46 Batch 100: Loss 1.5887080430984497
Model None Epoch 46 Batch 101: Loss 1.4977465867996216
Model None Epoch 46 Batch 102: Loss 1.2405312061309814
Model None Epoch 46 Batch 103: Loss 1.3885493278503418
Model None Epoch 46 Batch 104: Loss 1.4557451009750366
Model None Epoch 46 Batch 105: Loss 1.3747165203094482
Model None Epoch 46 Batch 106: Loss 1.4608068466186523
Model None Epoch 46 Batch 107: Loss 1.4065592288970947
Model None Epoch 46 Batch 108: Loss 1.5068531036376953
Model None Epoch 46 Batch 109: Loss 1.4286060333251953
Model None Epoch 46 Batch 110: Loss 1.4748347997665405
Model None Epoch 46 Batch 111: Loss 1.4615051746368408
Model None Epoch 46 Batch 112: Loss 1.3391079902648926
Model None Epoch 46 Batch 113: Loss 1.3891147375106812
Model None Epoch 46 Batch 114: Loss 1.4571208953857422
Model None Epoch 46 Batch 115: Loss 1.4137670993804932
Model None Epoch 46 Batch 116: Loss 1.508543610572815
Model None Epoch 46 Batch 117: Loss 1.2755792140960693
Model None Epoch 46 Batch 118: Loss 1.3556103706359863
Model None Epoch 46 Batch 119: Loss 1.4792003631591797
Model None Epoch 46 Batch 120: Loss 1.3727577924728394
Model None Epoch 46 Batch 121: Loss 1.3384504318237305
Model None Epoch 46 Batch 122: Loss 1.455646276473999
Model None Epoch 46 Batch 123: Loss 1.3818453550338745
Model None Epoch 46 Batch 124: Loss 1.537116527557373
Model None Epoch 46 Batch 125: Loss 1.4785921573638916
Model None Epoch 46 Batch 126: Loss 1.3984405994415283
Model None Epoch 46 Batch 127: Loss 1.4072527885437012
Model None Epoch 46 Batch 128: Loss 1.4078978300094604
Model None Epoch 46 Batch 129: Loss 1.4352688789367676
Model None Epoch 46 Batch 130: Loss 1.3875336647033691
Model None Epoch 46 Batch 131: Loss 1.3779727220535278
Model None Epoch 46 Batch 132: Loss 1.3750617504119873
Model None Epoch 46 Batch 133: Loss 1.3577929735183716
Model None Epoch 46 Batch 134: Loss 1.3398849964141846
Model None Epoch 46 Batch 135: Loss 1.1993733644485474
Model None Epoch 46 Batch 136: Loss 1.5719877481460571
Model None Epoch 46 Batch 137: Loss 1.5173978805541992
Model None Epoch 46 Batch 138: Loss 1.485352635383606
Model None Epoch 46 Batch 139: Loss 1.417482852935791
Model None Epoch 46 Batch 140: Loss 1.4673657417297363
Model None Epoch 46 Batch 141: Loss 1.4277609586715698
Model None Epoch 46 Batch 142: Loss 1.3086737394332886
Model None Epoch 46 Batch 143: Loss 1.4753731489181519
Model None Epoch 46 Batch 144: Loss 1.38934326171875
Model None Epoch 46 Batch 145: Loss 1.4915629625320435
Model None Epoch 46 Batch 146: Loss 1.3986601829528809
Model None Epoch 46 Batch 147: Loss 1.5080957412719727
Model None Epoch 46 Batch 148: Loss 1.339587926864624
Model None Epoch 46 Batch 149: Loss 1.4283266067504883
Downstream Train Epoch: 46 [38400/50000 (77%)]	Loss: 1.543921
Model None Epoch 46 Batch 150: Loss 1.5439213514328003
Model None Epoch 46 Batch 151: Loss 1.3634843826293945
Model None Epoch 46 Batch 152: Loss 1.3453152179718018
Model None Epoch 46 Batch 153: Loss 1.3897496461868286
Model None Epoch 46 Batch 154: Loss 1.353026270866394
Model None Epoch 46 Batch 155: Loss 1.3125770092010498
Model None Epoch 46 Batch 156: Loss 1.4119775295257568
Model None Epoch 46 Batch 157: Loss 1.3432490825653076
Model None Epoch 46 Batch 158: Loss 1.4335612058639526
Model None Epoch 46 Batch 159: Loss 1.3792458772659302
Model None Epoch 46 Batch 160: Loss 1.353180170059204
Model None Epoch 46 Batch 161: Loss 1.4084315299987793
Model None Epoch 46 Batch 162: Loss 1.4562824964523315
Model None Epoch 46 Batch 163: Loss 1.372119426727295
Model None Epoch 46 Batch 164: Loss 1.4012914896011353
Model None Epoch 46 Batch 165: Loss 1.415982723236084
Model None Epoch 46 Batch 166: Loss 1.4855785369873047
Model None Epoch 46 Batch 167: Loss 1.421667456626892
Model None Epoch 46 Batch 168: Loss 1.4973042011260986
Model None Epoch 46 Batch 169: Loss 1.3838649988174438
Model None Epoch 46 Batch 170: Loss 1.3238517045974731
Model None Epoch 46 Batch 171: Loss 1.4031212329864502
Model None Epoch 46 Batch 172: Loss 1.464461326599121
Model None Epoch 46 Batch 173: Loss 1.3976422548294067
Model None Epoch 46 Batch 174: Loss 1.4376592636108398
Model None Epoch 46 Batch 175: Loss 1.3897861242294312
Model None Epoch 46 Batch 176: Loss 1.387805461883545
Model None Epoch 46 Batch 177: Loss 1.3677301406860352
Model None Epoch 46 Batch 178: Loss 1.305737853050232
Model None Epoch 46 Batch 179: Loss 1.346137285232544
Model None Epoch 46 Batch 180: Loss 1.4481720924377441
Model None Epoch 46 Batch 181: Loss 1.3453375101089478
Model None Epoch 46 Batch 182: Loss 1.329214334487915
Model None Epoch 46 Batch 183: Loss 1.4075011014938354
Model None Epoch 46 Batch 184: Loss 1.404967188835144
Model None Epoch 46 Batch 185: Loss 1.4654288291931152
Model None Epoch 46 Batch 186: Loss 1.4327703714370728
Model None Epoch 46 Batch 187: Loss 1.4392973184585571
Model None Epoch 46 Batch 188: Loss 1.3618648052215576
Model None Epoch 46 Batch 189: Loss 1.5059157609939575
Model None Epoch 46 Batch 190: Loss 1.5011287927627563
Model None Epoch 46 Batch 191: Loss 1.3587521314620972
Model None Epoch 46 Batch 192: Loss 1.445751667022705
Model None Epoch 46 Batch 193: Loss 1.3618439435958862
Model None Epoch 46 Batch 194: Loss 1.308501124382019
Model None Epoch 46 Batch 195: Loss 1.4948747158050537

 Downstream Train loss: 1.4026603491938845 Acc: 0.5655
Downstream Train Epoch: 47 [0/50000 (0%)]	Loss: 1.395705
Model None Epoch 47 Batch 0: Loss 1.3957054615020752
Model None Epoch 47 Batch 1: Loss 1.3283836841583252
Model None Epoch 47 Batch 2: Loss 1.4432135820388794
Model None Epoch 47 Batch 3: Loss 1.4226229190826416
Model None Epoch 47 Batch 4: Loss 1.399466872215271
Model None Epoch 47 Batch 5: Loss 1.4209827184677124
Model None Epoch 47 Batch 6: Loss 1.3834559917449951
Model None Epoch 47 Batch 7: Loss 1.5515799522399902
Model None Epoch 47 Batch 8: Loss 1.4808387756347656
Model None Epoch 47 Batch 9: Loss 1.4078623056411743
Model None Epoch 47 Batch 10: Loss 1.448656678199768
Model None Epoch 47 Batch 11: Loss 1.3560153245925903
Model None Epoch 47 Batch 12: Loss 1.4974297285079956
Model None Epoch 47 Batch 13: Loss 1.3148518800735474
Model None Epoch 47 Batch 14: Loss 1.4251725673675537
Model None Epoch 47 Batch 15: Loss 1.336674451828003
Model None Epoch 47 Batch 16: Loss 1.4003938436508179
Model None Epoch 47 Batch 17: Loss 1.4740322828292847
Model None Epoch 47 Batch 18: Loss 1.374131202697754
Model None Epoch 47 Batch 19: Loss 1.3987826108932495
Model None Epoch 47 Batch 20: Loss 1.3969670534133911
Model None Epoch 47 Batch 21: Loss 1.4663258790969849
Model None Epoch 47 Batch 22: Loss 1.360883355140686
Model None Epoch 47 Batch 23: Loss 1.4409784078598022
Model None Epoch 47 Batch 24: Loss 1.379033088684082
Model None Epoch 47 Batch 25: Loss 1.387370228767395
Model None Epoch 47 Batch 26: Loss 1.5567344427108765
Model None Epoch 47 Batch 27: Loss 1.2627615928649902
Model None Epoch 47 Batch 28: Loss 1.4035005569458008
Model None Epoch 47 Batch 29: Loss 1.3727062940597534
Model None Epoch 47 Batch 30: Loss 1.3762235641479492
Model None Epoch 47 Batch 31: Loss 1.3357737064361572
Model None Epoch 47 Batch 32: Loss 1.369891881942749
Model None Epoch 47 Batch 33: Loss 1.4211623668670654
Model None Epoch 47 Batch 34: Loss 1.357831597328186
Model None Epoch 47 Batch 35: Loss 1.29506254196167
Model None Epoch 47 Batch 36: Loss 1.4215717315673828
Model None Epoch 47 Batch 37: Loss 1.482662558555603
Model None Epoch 47 Batch 38: Loss 1.371121883392334
Model None Epoch 47 Batch 39: Loss 1.428505301475525
Model None Epoch 47 Batch 40: Loss 1.370416522026062
Model None Epoch 47 Batch 41: Loss 1.3923298120498657
Model None Epoch 47 Batch 42: Loss 1.382425308227539
Model None Epoch 47 Batch 43: Loss 1.4177963733673096
Model None Epoch 47 Batch 44: Loss 1.383414387702942
Model None Epoch 47 Batch 45: Loss 1.446326732635498
Model None Epoch 47 Batch 46: Loss 1.4110136032104492
Model None Epoch 47 Batch 47: Loss 1.4723289012908936
Model None Epoch 47 Batch 48: Loss 1.3527874946594238
Model None Epoch 47 Batch 49: Loss 1.374224305152893
Downstream Train Epoch: 47 [12800/50000 (26%)]	Loss: 1.347139
Model None Epoch 47 Batch 50: Loss 1.3471391201019287
Model None Epoch 47 Batch 51: Loss 1.3000762462615967
Model None Epoch 47 Batch 52: Loss 1.4405663013458252
Model None Epoch 47 Batch 53: Loss 1.5095605850219727
Model None Epoch 47 Batch 54: Loss 1.4137519598007202
Model None Epoch 47 Batch 55: Loss 1.4334197044372559
Model None Epoch 47 Batch 56: Loss 1.4852057695388794
Model None Epoch 47 Batch 57: Loss 1.5062955617904663
Model None Epoch 47 Batch 58: Loss 1.481173038482666
Model None Epoch 47 Batch 59: Loss 1.3566380739212036
Model None Epoch 47 Batch 60: Loss 1.455586314201355
Model None Epoch 47 Batch 61: Loss 1.4070295095443726
Model None Epoch 47 Batch 62: Loss 1.4193168878555298
Model None Epoch 47 Batch 63: Loss 1.4323091506958008
Model None Epoch 47 Batch 64: Loss 1.3678867816925049
Model None Epoch 47 Batch 65: Loss 1.5203191041946411
Model None Epoch 47 Batch 66: Loss 1.3418577909469604
Model None Epoch 47 Batch 67: Loss 1.4657766819000244
Model None Epoch 47 Batch 68: Loss 1.4674113988876343
Model None Epoch 47 Batch 69: Loss 1.4114363193511963
Model None Epoch 47 Batch 70: Loss 1.4908562898635864
Model None Epoch 47 Batch 71: Loss 1.3441872596740723
Model None Epoch 47 Batch 72: Loss 1.3837084770202637
Model None Epoch 47 Batch 73: Loss 1.3760491609573364
Model None Epoch 47 Batch 74: Loss 1.4650524854660034
Model None Epoch 47 Batch 75: Loss 1.5056846141815186
Model None Epoch 47 Batch 76: Loss 1.3386222124099731
Model None Epoch 47 Batch 77: Loss 1.381201982498169
Model None Epoch 47 Batch 78: Loss 1.3190982341766357
Model None Epoch 47 Batch 79: Loss 1.4209108352661133
Model None Epoch 47 Batch 80: Loss 1.3881500959396362
Model None Epoch 47 Batch 81: Loss 1.4588923454284668
Model None Epoch 47 Batch 82: Loss 1.3837119340896606
Model None Epoch 47 Batch 83: Loss 1.5013285875320435
Model None Epoch 47 Batch 84: Loss 1.4906909465789795
Model None Epoch 47 Batch 85: Loss 1.4019831418991089
Model None Epoch 47 Batch 86: Loss 1.444756269454956
Model None Epoch 47 Batch 87: Loss 1.332890272140503
Model None Epoch 47 Batch 88: Loss 1.3510370254516602
Model None Epoch 47 Batch 89: Loss 1.429220199584961
Model None Epoch 47 Batch 90: Loss 1.3187174797058105
Model None Epoch 47 Batch 91: Loss 1.3524954319000244
Model None Epoch 47 Batch 92: Loss 1.345922589302063
Model None Epoch 47 Batch 93: Loss 1.4464527368545532
Model None Epoch 47 Batch 94: Loss 1.326129674911499
Model None Epoch 47 Batch 95: Loss 1.3990727663040161
Model None Epoch 47 Batch 96: Loss 1.3112602233886719
Model None Epoch 47 Batch 97: Loss 1.3267292976379395
Model None Epoch 47 Batch 98: Loss 1.4537183046340942
Model None Epoch 47 Batch 99: Loss 1.4314894676208496
Downstream Train Epoch: 47 [25600/50000 (51%)]	Loss: 1.551694
Model None Epoch 47 Batch 100: Loss 1.5516936779022217
Model None Epoch 47 Batch 101: Loss 1.506665825843811
Model None Epoch 47 Batch 102: Loss 1.3152490854263306
Model None Epoch 47 Batch 103: Loss 1.4245800971984863
Model None Epoch 47 Batch 104: Loss 1.426620364189148
Model None Epoch 47 Batch 105: Loss 1.3856827020645142
Model None Epoch 47 Batch 106: Loss 1.4534647464752197
Model None Epoch 47 Batch 107: Loss 1.4250513315200806
Model None Epoch 47 Batch 108: Loss 1.4252880811691284
Model None Epoch 47 Batch 109: Loss 1.4033048152923584
Model None Epoch 47 Batch 110: Loss 1.4211831092834473
Model None Epoch 47 Batch 111: Loss 1.4031641483306885
Model None Epoch 47 Batch 112: Loss 1.350119948387146
Model None Epoch 47 Batch 113: Loss 1.4263896942138672
Model None Epoch 47 Batch 114: Loss 1.4058781862258911
Model None Epoch 47 Batch 115: Loss 1.3978464603424072
Model None Epoch 47 Batch 116: Loss 1.438136100769043
Model None Epoch 47 Batch 117: Loss 1.2882592678070068
Model None Epoch 47 Batch 118: Loss 1.378347396850586
Model None Epoch 47 Batch 119: Loss 1.4139965772628784
Model None Epoch 47 Batch 120: Loss 1.4090527296066284
Model None Epoch 47 Batch 121: Loss 1.3462350368499756
Model None Epoch 47 Batch 122: Loss 1.4474155902862549
Model None Epoch 47 Batch 123: Loss 1.4778121709823608
Model None Epoch 47 Batch 124: Loss 1.535089373588562
Model None Epoch 47 Batch 125: Loss 1.602893590927124
Model None Epoch 47 Batch 126: Loss 1.297777533531189
Model None Epoch 47 Batch 127: Loss 1.295498251914978
Model None Epoch 47 Batch 128: Loss 1.3594413995742798
Model None Epoch 47 Batch 129: Loss 1.4100877046585083
Model None Epoch 47 Batch 130: Loss 1.4841655492782593
Model None Epoch 47 Batch 131: Loss 1.354722499847412
Model None Epoch 47 Batch 132: Loss 1.4014800786972046
Model None Epoch 47 Batch 133: Loss 1.2860276699066162
Model None Epoch 47 Batch 134: Loss 1.4597705602645874
Model None Epoch 47 Batch 135: Loss 1.4032212495803833
Model None Epoch 47 Batch 136: Loss 1.4704673290252686
Model None Epoch 47 Batch 137: Loss 1.4120336771011353
Model None Epoch 47 Batch 138: Loss 1.2733651399612427
Model None Epoch 47 Batch 139: Loss 1.3720961809158325
Model None Epoch 47 Batch 140: Loss 1.3395239114761353
Model None Epoch 47 Batch 141: Loss 1.2679153680801392
Model None Epoch 47 Batch 142: Loss 1.4416906833648682
Model None Epoch 47 Batch 143: Loss 1.2678042650222778
Model None Epoch 47 Batch 144: Loss 1.4180346727371216
Model None Epoch 47 Batch 145: Loss 1.3754245042800903
Model None Epoch 47 Batch 146: Loss 1.3880311250686646
Model None Epoch 47 Batch 147: Loss 1.5309875011444092
Model None Epoch 47 Batch 148: Loss 1.37558913230896
Model None Epoch 47 Batch 149: Loss 1.4019935131072998
Downstream Train Epoch: 47 [38400/50000 (77%)]	Loss: 1.296754
Model None Epoch 47 Batch 150: Loss 1.2967537641525269
Model None Epoch 47 Batch 151: Loss 1.3995450735092163
Model None Epoch 47 Batch 152: Loss 1.3407543897628784
Model None Epoch 47 Batch 153: Loss 1.4453940391540527
Model None Epoch 47 Batch 154: Loss 1.4237465858459473
Model None Epoch 47 Batch 155: Loss 1.3164973258972168
Model None Epoch 47 Batch 156: Loss 1.500747561454773
Model None Epoch 47 Batch 157: Loss 1.3379367589950562
Model None Epoch 47 Batch 158: Loss 1.4429047107696533
Model None Epoch 47 Batch 159: Loss 1.4117341041564941
Model None Epoch 47 Batch 160: Loss 1.4140805006027222
Model None Epoch 47 Batch 161: Loss 1.4321181774139404
Model None Epoch 47 Batch 162: Loss 1.5516225099563599
Model None Epoch 47 Batch 163: Loss 1.3288359642028809
Model None Epoch 47 Batch 164: Loss 1.4418600797653198
Model None Epoch 47 Batch 165: Loss 1.3584078550338745
Model None Epoch 47 Batch 166: Loss 1.3429903984069824
Model None Epoch 47 Batch 167: Loss 1.4522509574890137
Model None Epoch 47 Batch 168: Loss 1.5379362106323242
Model None Epoch 47 Batch 169: Loss 1.4713557958602905
Model None Epoch 47 Batch 170: Loss 1.4888627529144287
Model None Epoch 47 Batch 171: Loss 1.4193261861801147
Model None Epoch 47 Batch 172: Loss 1.5049512386322021
Model None Epoch 47 Batch 173: Loss 1.3779754638671875
Model None Epoch 47 Batch 174: Loss 1.4433531761169434
Model None Epoch 47 Batch 175: Loss 1.4194706678390503
Model None Epoch 47 Batch 176: Loss 1.3269574642181396
Model None Epoch 47 Batch 177: Loss 1.496802806854248
Model None Epoch 47 Batch 178: Loss 1.3920780420303345
Model None Epoch 47 Batch 179: Loss 1.3826390504837036
Model None Epoch 47 Batch 180: Loss 1.3111357688903809
Model None Epoch 47 Batch 181: Loss 1.3884001970291138
Model None Epoch 47 Batch 182: Loss 1.4718422889709473
Model None Epoch 47 Batch 183: Loss 1.4164379835128784
Model None Epoch 47 Batch 184: Loss 1.3960399627685547
Model None Epoch 47 Batch 185: Loss 1.4089136123657227
Model None Epoch 47 Batch 186: Loss 1.353625774383545
Model None Epoch 47 Batch 187: Loss 1.4036152362823486
Model None Epoch 47 Batch 188: Loss 1.5073378086090088
Model None Epoch 47 Batch 189: Loss 1.497464656829834
Model None Epoch 47 Batch 190: Loss 1.4301155805587769
Model None Epoch 47 Batch 191: Loss 1.4579178094863892
Model None Epoch 47 Batch 192: Loss 1.4076093435287476
Model None Epoch 47 Batch 193: Loss 1.52092444896698
Model None Epoch 47 Batch 194: Loss 1.3199610710144043
Model None Epoch 47 Batch 195: Loss 1.210977554321289

 Downstream Train loss: 1.4070507105515928 Acc: 0.5655
Downstream Train Epoch: 48 [0/50000 (0%)]	Loss: 1.447988
Model None Epoch 48 Batch 0: Loss 1.447987675666809
Model None Epoch 48 Batch 1: Loss 1.4191278219223022
Model None Epoch 48 Batch 2: Loss 1.3783981800079346
Model None Epoch 48 Batch 3: Loss 1.4214943647384644
Model None Epoch 48 Batch 4: Loss 1.3745412826538086
Model None Epoch 48 Batch 5: Loss 1.3605656623840332
Model None Epoch 48 Batch 6: Loss 1.4391796588897705
Model None Epoch 48 Batch 7: Loss 1.4130009412765503
Model None Epoch 48 Batch 8: Loss 1.342272162437439
Model None Epoch 48 Batch 9: Loss 1.4133856296539307
Model None Epoch 48 Batch 10: Loss 1.4674568176269531
Model None Epoch 48 Batch 11: Loss 1.3566641807556152
Model None Epoch 48 Batch 12: Loss 1.3620411157608032
Model None Epoch 48 Batch 13: Loss 1.4910155534744263
Model None Epoch 48 Batch 14: Loss 1.3478739261627197
Model None Epoch 48 Batch 15: Loss 1.3984371423721313
Model None Epoch 48 Batch 16: Loss 1.3704495429992676
Model None Epoch 48 Batch 17: Loss 1.3616101741790771
Model None Epoch 48 Batch 18: Loss 1.337735891342163
Model None Epoch 48 Batch 19: Loss 1.378204345703125
Model None Epoch 48 Batch 20: Loss 1.3776596784591675
Model None Epoch 48 Batch 21: Loss 1.4218719005584717
Model None Epoch 48 Batch 22: Loss 1.4430980682373047
Model None Epoch 48 Batch 23: Loss 1.4951938390731812
Model None Epoch 48 Batch 24: Loss 1.4303547143936157
Model None Epoch 48 Batch 25: Loss 1.5034197568893433
Model None Epoch 48 Batch 26: Loss 1.4031747579574585
Model None Epoch 48 Batch 27: Loss 1.453065276145935
Model None Epoch 48 Batch 28: Loss 1.3671554327011108
Model None Epoch 48 Batch 29: Loss 1.3995740413665771
Model None Epoch 48 Batch 30: Loss 1.3325177431106567
Model None Epoch 48 Batch 31: Loss 1.458167552947998
Model None Epoch 48 Batch 32: Loss 1.4810041189193726
Model None Epoch 48 Batch 33: Loss 1.447881817817688
Model None Epoch 48 Batch 34: Loss 1.4100406169891357
Model None Epoch 48 Batch 35: Loss 1.3876452445983887
Model None Epoch 48 Batch 36: Loss 1.4376862049102783
Model None Epoch 48 Batch 37: Loss 1.32670259475708
Model None Epoch 48 Batch 38: Loss 1.4417266845703125
Model None Epoch 48 Batch 39: Loss 1.3347374200820923
Model None Epoch 48 Batch 40: Loss 1.3793901205062866
Model None Epoch 48 Batch 41: Loss 1.1925582885742188
Model None Epoch 48 Batch 42: Loss 1.3487762212753296
Model None Epoch 48 Batch 43: Loss 1.437912940979004
Model None Epoch 48 Batch 44: Loss 1.2869889736175537
Model None Epoch 48 Batch 45: Loss 1.3012850284576416
Model None Epoch 48 Batch 46: Loss 1.439348816871643
Model None Epoch 48 Batch 47: Loss 1.3966255187988281
Model None Epoch 48 Batch 48: Loss 1.1901283264160156
Model None Epoch 48 Batch 49: Loss 1.5056439638137817
Downstream Train Epoch: 48 [12800/50000 (26%)]	Loss: 1.361018
Model None Epoch 48 Batch 50: Loss 1.3610183000564575
Model None Epoch 48 Batch 51: Loss 1.4762980937957764
Model None Epoch 48 Batch 52: Loss 1.3852912187576294
Model None Epoch 48 Batch 53: Loss 1.4269812107086182
Model None Epoch 48 Batch 54: Loss 1.385258436203003
Model None Epoch 48 Batch 55: Loss 1.3417880535125732
Model None Epoch 48 Batch 56: Loss 1.3489493131637573
Model None Epoch 48 Batch 57: Loss 1.4526301622390747
Model None Epoch 48 Batch 58: Loss 1.2729151248931885
Model None Epoch 48 Batch 59: Loss 1.3620100021362305
Model None Epoch 48 Batch 60: Loss 1.3151471614837646
Model None Epoch 48 Batch 61: Loss 1.4579815864562988
Model None Epoch 48 Batch 62: Loss 1.3452553749084473
Model None Epoch 48 Batch 63: Loss 1.3707228899002075
Model None Epoch 48 Batch 64: Loss 1.456015706062317
Model None Epoch 48 Batch 65: Loss 1.4256139993667603
Model None Epoch 48 Batch 66: Loss 1.339931607246399
Model None Epoch 48 Batch 67: Loss 1.2629773616790771
Model None Epoch 48 Batch 68: Loss 1.4310311079025269
Model None Epoch 48 Batch 69: Loss 1.5648006200790405
Model None Epoch 48 Batch 70: Loss 1.47367525100708
Model None Epoch 48 Batch 71: Loss 1.3334197998046875
Model None Epoch 48 Batch 72: Loss 1.4828999042510986
Model None Epoch 48 Batch 73: Loss 1.3961366415023804
Model None Epoch 48 Batch 74: Loss 1.3845560550689697
Model None Epoch 48 Batch 75: Loss 1.2978312969207764
Model None Epoch 48 Batch 76: Loss 1.409595012664795
Model None Epoch 48 Batch 77: Loss 1.3680908679962158
Model None Epoch 48 Batch 78: Loss 1.355027675628662
Model None Epoch 48 Batch 79: Loss 1.3461946249008179
Model None Epoch 48 Batch 80: Loss 1.4950683116912842
Model None Epoch 48 Batch 81: Loss 1.3681056499481201
Model None Epoch 48 Batch 82: Loss 1.4337328672409058
Model None Epoch 48 Batch 83: Loss 1.4087504148483276
Model None Epoch 48 Batch 84: Loss 1.3924007415771484
Model None Epoch 48 Batch 85: Loss 1.4597492218017578
Model None Epoch 48 Batch 86: Loss 1.3963958024978638
Model None Epoch 48 Batch 87: Loss 1.3540178537368774
Model None Epoch 48 Batch 88: Loss 1.4434499740600586
Model None Epoch 48 Batch 89: Loss 1.430365800857544
Model None Epoch 48 Batch 90: Loss 1.3753129243850708
Model None Epoch 48 Batch 91: Loss 1.3229762315750122
Model None Epoch 48 Batch 92: Loss 1.4788557291030884
Model None Epoch 48 Batch 93: Loss 1.4617422819137573
Model None Epoch 48 Batch 94: Loss 1.4166371822357178
Model None Epoch 48 Batch 95: Loss 1.4987627267837524
Model None Epoch 48 Batch 96: Loss 1.4108717441558838
Model None Epoch 48 Batch 97: Loss 1.2829327583312988
Model None Epoch 48 Batch 98: Loss 1.4091086387634277
Model None Epoch 48 Batch 99: Loss 1.355785608291626
Downstream Train Epoch: 48 [25600/50000 (51%)]	Loss: 1.388764
Model None Epoch 48 Batch 100: Loss 1.3887643814086914
Model None Epoch 48 Batch 101: Loss 1.5239887237548828
Model None Epoch 48 Batch 102: Loss 1.3801169395446777
Model None Epoch 48 Batch 103: Loss 1.3790026903152466
Model None Epoch 48 Batch 104: Loss 1.297790288925171
Model None Epoch 48 Batch 105: Loss 1.4340561628341675
Model None Epoch 48 Batch 106: Loss 1.36150324344635
Model None Epoch 48 Batch 107: Loss 1.4416863918304443
Model None Epoch 48 Batch 108: Loss 1.3961730003356934
Model None Epoch 48 Batch 109: Loss 1.3532193899154663
Model None Epoch 48 Batch 110: Loss 1.4071197509765625
Model None Epoch 48 Batch 111: Loss 1.399888515472412
Model None Epoch 48 Batch 112: Loss 1.4976472854614258
Model None Epoch 48 Batch 113: Loss 1.4871822595596313
Model None Epoch 48 Batch 114: Loss 1.3601559400558472
Model None Epoch 48 Batch 115: Loss 1.398795485496521
Model None Epoch 48 Batch 116: Loss 1.4563138484954834
Model None Epoch 48 Batch 117: Loss 1.4832707643508911
Model None Epoch 48 Batch 118: Loss 1.4953631162643433
Model None Epoch 48 Batch 119: Loss 1.3170983791351318
Model None Epoch 48 Batch 120: Loss 1.3779438734054565
Model None Epoch 48 Batch 121: Loss 1.3987488746643066
Model None Epoch 48 Batch 122: Loss 1.4796284437179565
Model None Epoch 48 Batch 123: Loss 1.4270570278167725
Model None Epoch 48 Batch 124: Loss 1.3767729997634888
Model None Epoch 48 Batch 125: Loss 1.3462587594985962
Model None Epoch 48 Batch 126: Loss 1.3958356380462646
Model None Epoch 48 Batch 127: Loss 1.4558801651000977
Model None Epoch 48 Batch 128: Loss 1.3725863695144653
Model None Epoch 48 Batch 129: Loss 1.5607370138168335
Model None Epoch 48 Batch 130: Loss 1.3941400051116943
Model None Epoch 48 Batch 131: Loss 1.3075400590896606
Model None Epoch 48 Batch 132: Loss 1.44003427028656
Model None Epoch 48 Batch 133: Loss 1.4066531658172607
Model None Epoch 48 Batch 134: Loss 1.283048152923584
Model None Epoch 48 Batch 135: Loss 1.3115092515945435
Model None Epoch 48 Batch 136: Loss 1.3905394077301025
Model None Epoch 48 Batch 137: Loss 1.4783625602722168
Model None Epoch 48 Batch 138: Loss 1.451164722442627
Model None Epoch 48 Batch 139: Loss 1.3436915874481201
Model None Epoch 48 Batch 140: Loss 1.288841724395752
Model None Epoch 48 Batch 141: Loss 1.3388376235961914
Model None Epoch 48 Batch 142: Loss 1.4565359354019165
Model None Epoch 48 Batch 143: Loss 1.472893238067627
Model None Epoch 48 Batch 144: Loss 1.3120890855789185
Model None Epoch 48 Batch 145: Loss 1.3323187828063965
Model None Epoch 48 Batch 146: Loss 1.4528577327728271
Model None Epoch 48 Batch 147: Loss 1.4382051229476929
Model None Epoch 48 Batch 148: Loss 1.295445442199707
Model None Epoch 48 Batch 149: Loss 1.413214087486267
Downstream Train Epoch: 48 [38400/50000 (77%)]	Loss: 1.388872
Model None Epoch 48 Batch 150: Loss 1.3888722658157349
Model None Epoch 48 Batch 151: Loss 1.5175766944885254
Model None Epoch 48 Batch 152: Loss 1.4042072296142578
Model None Epoch 48 Batch 153: Loss 1.5239100456237793
Model None Epoch 48 Batch 154: Loss 1.4128265380859375
Model None Epoch 48 Batch 155: Loss 1.419512391090393
Model None Epoch 48 Batch 156: Loss 1.3741960525512695
Model None Epoch 48 Batch 157: Loss 1.457770824432373
Model None Epoch 48 Batch 158: Loss 1.3841757774353027
Model None Epoch 48 Batch 159: Loss 1.519317388534546
Model None Epoch 48 Batch 160: Loss 1.4772181510925293
Model None Epoch 48 Batch 161: Loss 1.4128384590148926
Model None Epoch 48 Batch 162: Loss 1.4362984895706177
Model None Epoch 48 Batch 163: Loss 1.533104419708252
Model None Epoch 48 Batch 164: Loss 1.4264494180679321
Model None Epoch 48 Batch 165: Loss 1.4410905838012695
Model None Epoch 48 Batch 166: Loss 1.5284724235534668
Model None Epoch 48 Batch 167: Loss 1.3014471530914307
Model None Epoch 48 Batch 168: Loss 1.337083101272583
Model None Epoch 48 Batch 169: Loss 1.495384693145752
Model None Epoch 48 Batch 170: Loss 1.410637378692627
Model None Epoch 48 Batch 171: Loss 1.432173490524292
Model None Epoch 48 Batch 172: Loss 1.4593913555145264
Model None Epoch 48 Batch 173: Loss 1.405994176864624
Model None Epoch 48 Batch 174: Loss 1.4777445793151855
Model None Epoch 48 Batch 175: Loss 1.4498907327651978
Model None Epoch 48 Batch 176: Loss 1.4809494018554688
Model None Epoch 48 Batch 177: Loss 1.3256049156188965
Model None Epoch 48 Batch 178: Loss 1.5418239831924438
Model None Epoch 48 Batch 179: Loss 1.4051406383514404
Model None Epoch 48 Batch 180: Loss 1.4198004007339478
Model None Epoch 48 Batch 181: Loss 1.4090856313705444
Model None Epoch 48 Batch 182: Loss 1.418007254600525
Model None Epoch 48 Batch 183: Loss 1.4151954650878906
Model None Epoch 48 Batch 184: Loss 1.4780144691467285
Model None Epoch 48 Batch 185: Loss 1.2338265180587769
Model None Epoch 48 Batch 186: Loss 1.4260826110839844
Model None Epoch 48 Batch 187: Loss 1.443971872329712
Model None Epoch 48 Batch 188: Loss 1.4707592725753784
Model None Epoch 48 Batch 189: Loss 1.3646304607391357
Model None Epoch 48 Batch 190: Loss 1.3565313816070557
Model None Epoch 48 Batch 191: Loss 1.482486605644226
Model None Epoch 48 Batch 192: Loss 1.388134241104126
Model None Epoch 48 Batch 193: Loss 1.354641079902649
Model None Epoch 48 Batch 194: Loss 1.4600067138671875
Model None Epoch 48 Batch 195: Loss 1.4405492544174194

 Downstream Train loss: 1.40479172614156 Acc: 0.5655
Downstream Train Epoch: 49 [0/50000 (0%)]	Loss: 1.381885
Model None Epoch 49 Batch 0: Loss 1.381885290145874
Model None Epoch 49 Batch 1: Loss 1.382554531097412
Model None Epoch 49 Batch 2: Loss 1.5116143226623535
Model None Epoch 49 Batch 3: Loss 1.430192232131958
Model None Epoch 49 Batch 4: Loss 1.3647291660308838
Model None Epoch 49 Batch 5: Loss 1.5003210306167603
Model None Epoch 49 Batch 6: Loss 1.479598879814148
Model None Epoch 49 Batch 7: Loss 1.2967156171798706
Model None Epoch 49 Batch 8: Loss 1.338685154914856
Model None Epoch 49 Batch 9: Loss 1.3822879791259766
Model None Epoch 49 Batch 10: Loss 1.4098776578903198
Model None Epoch 49 Batch 11: Loss 1.495863914489746
Model None Epoch 49 Batch 12: Loss 1.3473516702651978
Model None Epoch 49 Batch 13: Loss 1.289878487586975
Model None Epoch 49 Batch 14: Loss 1.4084206819534302
Model None Epoch 49 Batch 15: Loss 1.4166741371154785
Model None Epoch 49 Batch 16: Loss 1.2726432085037231
Model None Epoch 49 Batch 17: Loss 1.3865060806274414
Model None Epoch 49 Batch 18: Loss 1.5007915496826172
Model None Epoch 49 Batch 19: Loss 1.3192951679229736
Model None Epoch 49 Batch 20: Loss 1.37641179561615
Model None Epoch 49 Batch 21: Loss 1.403511881828308
Model None Epoch 49 Batch 22: Loss 1.478416085243225
Model None Epoch 49 Batch 23: Loss 1.488623857498169
Model None Epoch 49 Batch 24: Loss 1.3391324281692505
Model None Epoch 49 Batch 25: Loss 1.424763560295105
Model None Epoch 49 Batch 26: Loss 1.332581877708435
Model None Epoch 49 Batch 27: Loss 1.4571962356567383
Model None Epoch 49 Batch 28: Loss 1.3520303964614868
Model None Epoch 49 Batch 29: Loss 1.3001121282577515
Model None Epoch 49 Batch 30: Loss 1.3065649271011353
Model None Epoch 49 Batch 31: Loss 1.3269739151000977
Model None Epoch 49 Batch 32: Loss 1.4573254585266113
Model None Epoch 49 Batch 33: Loss 1.3786602020263672
Model None Epoch 49 Batch 34: Loss 1.4242959022521973
Model None Epoch 49 Batch 35: Loss 1.4035284519195557
Model None Epoch 49 Batch 36: Loss 1.3353959321975708
Model None Epoch 49 Batch 37: Loss 1.2928614616394043
Model None Epoch 49 Batch 38: Loss 1.3666768074035645
Model None Epoch 49 Batch 39: Loss 1.4306550025939941
Model None Epoch 49 Batch 40: Loss 1.4609718322753906
Model None Epoch 49 Batch 41: Loss 1.3707001209259033
Model None Epoch 49 Batch 42: Loss 1.4519447088241577
Model None Epoch 49 Batch 43: Loss 1.310494065284729
Model None Epoch 49 Batch 44: Loss 1.400956392288208
Model None Epoch 49 Batch 45: Loss 1.2796515226364136
Model None Epoch 49 Batch 46: Loss 1.4132678508758545
Model None Epoch 49 Batch 47: Loss 1.3141944408416748
Model None Epoch 49 Batch 48: Loss 1.4549200534820557
Model None Epoch 49 Batch 49: Loss 1.4452540874481201
Downstream Train Epoch: 49 [12800/50000 (26%)]	Loss: 1.465074
Model None Epoch 49 Batch 50: Loss 1.4650744199752808
Model None Epoch 49 Batch 51: Loss 1.4455764293670654
Model None Epoch 49 Batch 52: Loss 1.4861325025558472
Model None Epoch 49 Batch 53: Loss 1.4250379800796509
Model None Epoch 49 Batch 54: Loss 1.4405380487442017
Model None Epoch 49 Batch 55: Loss 1.406318187713623
Model None Epoch 49 Batch 56: Loss 1.471555233001709
Model None Epoch 49 Batch 57: Loss 1.4101041555404663
Model None Epoch 49 Batch 58: Loss 1.4739947319030762
Model None Epoch 49 Batch 59: Loss 1.4129256010055542
Model None Epoch 49 Batch 60: Loss 1.4534770250320435
Model None Epoch 49 Batch 61: Loss 1.349330186843872
Model None Epoch 49 Batch 62: Loss 1.4121402502059937
Model None Epoch 49 Batch 63: Loss 1.3099288940429688
Model None Epoch 49 Batch 64: Loss 1.3763928413391113
Model None Epoch 49 Batch 65: Loss 1.4564411640167236
Model None Epoch 49 Batch 66: Loss 1.4326533079147339
Model None Epoch 49 Batch 67: Loss 1.4220306873321533
Model None Epoch 49 Batch 68: Loss 1.2947309017181396
Model None Epoch 49 Batch 69: Loss 1.4071210622787476
Model None Epoch 49 Batch 70: Loss 1.2970868349075317
Model None Epoch 49 Batch 71: Loss 1.422835111618042
Model None Epoch 49 Batch 72: Loss 1.3135348558425903
Model None Epoch 49 Batch 73: Loss 1.4991036653518677
Model None Epoch 49 Batch 74: Loss 1.4699041843414307
Model None Epoch 49 Batch 75: Loss 1.40398108959198
Model None Epoch 49 Batch 76: Loss 1.4972947835922241
Model None Epoch 49 Batch 77: Loss 1.3950228691101074
Model None Epoch 49 Batch 78: Loss 1.4251410961151123
Model None Epoch 49 Batch 79: Loss 1.3301880359649658
Model None Epoch 49 Batch 80: Loss 1.3012126684188843
Model None Epoch 49 Batch 81: Loss 1.3893622159957886
Model None Epoch 49 Batch 82: Loss 1.387669563293457
Model None Epoch 49 Batch 83: Loss 1.4028124809265137
Model None Epoch 49 Batch 84: Loss 1.2809585332870483
Model None Epoch 49 Batch 85: Loss 1.4106425046920776
Model None Epoch 49 Batch 86: Loss 1.4042158126831055
Model None Epoch 49 Batch 87: Loss 1.4058613777160645
Model None Epoch 49 Batch 88: Loss 1.351137399673462
Model None Epoch 49 Batch 89: Loss 1.4348275661468506
Model None Epoch 49 Batch 90: Loss 1.371044397354126
Model None Epoch 49 Batch 91: Loss 1.4050493240356445
Model None Epoch 49 Batch 92: Loss 1.4935966730117798
Model None Epoch 49 Batch 93: Loss 1.4806939363479614
Model None Epoch 49 Batch 94: Loss 1.3425756692886353
Model None Epoch 49 Batch 95: Loss 1.4701443910598755
Model None Epoch 49 Batch 96: Loss 1.3897979259490967
Model None Epoch 49 Batch 97: Loss 1.3585131168365479
Model None Epoch 49 Batch 98: Loss 1.441983938217163
Model None Epoch 49 Batch 99: Loss 1.4033559560775757
Downstream Train Epoch: 49 [25600/50000 (51%)]	Loss: 1.373905
Model None Epoch 49 Batch 100: Loss 1.3739053010940552
Model None Epoch 49 Batch 101: Loss 1.3788255453109741
Model None Epoch 49 Batch 102: Loss 1.3291239738464355
Model None Epoch 49 Batch 103: Loss 1.4388632774353027
Model None Epoch 49 Batch 104: Loss 1.6115249395370483
Model None Epoch 49 Batch 105: Loss 1.471972942352295
Model None Epoch 49 Batch 106: Loss 1.4633084535598755
Model None Epoch 49 Batch 107: Loss 1.4053611755371094
Model None Epoch 49 Batch 108: Loss 1.397904872894287
Model None Epoch 49 Batch 109: Loss 1.4454962015151978
Model None Epoch 49 Batch 110: Loss 1.377483606338501
Model None Epoch 49 Batch 111: Loss 1.5384653806686401
Model None Epoch 49 Batch 112: Loss 1.3407264947891235
Model None Epoch 49 Batch 113: Loss 1.4583549499511719
Model None Epoch 49 Batch 114: Loss 1.40240478515625
Model None Epoch 49 Batch 115: Loss 1.4007751941680908
Model None Epoch 49 Batch 116: Loss 1.4539377689361572
Model None Epoch 49 Batch 117: Loss 1.3841357231140137
Model None Epoch 49 Batch 118: Loss 1.4021291732788086
Model None Epoch 49 Batch 119: Loss 1.427120566368103
Model None Epoch 49 Batch 120: Loss 1.2305244207382202
Model None Epoch 49 Batch 121: Loss 1.3029834032058716
Model None Epoch 49 Batch 122: Loss 1.3768084049224854
Model None Epoch 49 Batch 123: Loss 1.44446861743927
Model None Epoch 49 Batch 124: Loss 1.3732846975326538
Model None Epoch 49 Batch 125: Loss 1.4546349048614502
Model None Epoch 49 Batch 126: Loss 1.3803653717041016
Model None Epoch 49 Batch 127: Loss 1.3994808197021484
Model None Epoch 49 Batch 128: Loss 1.3771512508392334
Model None Epoch 49 Batch 129: Loss 1.4269477128982544
Model None Epoch 49 Batch 130: Loss 1.4507983922958374
Model None Epoch 49 Batch 131: Loss 1.4224525690078735
Model None Epoch 49 Batch 132: Loss 1.3790379762649536
Model None Epoch 49 Batch 133: Loss 1.432710886001587
Model None Epoch 49 Batch 134: Loss 1.4485450983047485
Model None Epoch 49 Batch 135: Loss 1.367284893989563
Model None Epoch 49 Batch 136: Loss 1.484816551208496
Model None Epoch 49 Batch 137: Loss 1.3693883419036865
Model None Epoch 49 Batch 138: Loss 1.459416151046753
Model None Epoch 49 Batch 139: Loss 1.441258430480957
Model None Epoch 49 Batch 140: Loss 1.362742304801941
Model None Epoch 49 Batch 141: Loss 1.4107509851455688
Model None Epoch 49 Batch 142: Loss 1.3902974128723145
Model None Epoch 49 Batch 143: Loss 1.3460938930511475
Model None Epoch 49 Batch 144: Loss 1.4712214469909668
Model None Epoch 49 Batch 145: Loss 1.4896906614303589
Model None Epoch 49 Batch 146: Loss 1.3618563413619995
Model None Epoch 49 Batch 147: Loss 1.311072587966919
Model None Epoch 49 Batch 148: Loss 1.4413928985595703
Model None Epoch 49 Batch 149: Loss 1.3886666297912598
Downstream Train Epoch: 49 [38400/50000 (77%)]	Loss: 1.396336
Model None Epoch 49 Batch 150: Loss 1.3963356018066406
Model None Epoch 49 Batch 151: Loss 1.4015530347824097
Model None Epoch 49 Batch 152: Loss 1.3423434495925903
Model None Epoch 49 Batch 153: Loss 1.4100446701049805
Model None Epoch 49 Batch 154: Loss 1.376803994178772
Model None Epoch 49 Batch 155: Loss 1.3726309537887573
Model None Epoch 49 Batch 156: Loss 1.3885329961776733
Model None Epoch 49 Batch 157: Loss 1.4002748727798462
Model None Epoch 49 Batch 158: Loss 1.385780930519104
Model None Epoch 49 Batch 159: Loss 1.3813977241516113
Model None Epoch 49 Batch 160: Loss 1.2921825647354126
Model None Epoch 49 Batch 161: Loss 1.499382734298706
Model None Epoch 49 Batch 162: Loss 1.5065596103668213
Model None Epoch 49 Batch 163: Loss 1.4224176406860352
Model None Epoch 49 Batch 164: Loss 1.4140028953552246
Model None Epoch 49 Batch 165: Loss 1.3497642278671265
Model None Epoch 49 Batch 166: Loss 1.5022937059402466
Model None Epoch 49 Batch 167: Loss 1.307865858078003
Model None Epoch 49 Batch 168: Loss 1.4867841005325317
Model None Epoch 49 Batch 169: Loss 1.4942132234573364
Model None Epoch 49 Batch 170: Loss 1.4234949350357056
Model None Epoch 49 Batch 171: Loss 1.397740364074707
Model None Epoch 49 Batch 172: Loss 1.4405474662780762
Model None Epoch 49 Batch 173: Loss 1.300905704498291
Model None Epoch 49 Batch 174: Loss 1.368219256401062
Model None Epoch 49 Batch 175: Loss 1.3274730443954468
Model None Epoch 49 Batch 176: Loss 1.4826700687408447
Model None Epoch 49 Batch 177: Loss 1.4598292112350464
Model None Epoch 49 Batch 178: Loss 1.432664394378662
Model None Epoch 49 Batch 179: Loss 1.3414771556854248
Model None Epoch 49 Batch 180: Loss 1.405950665473938
Model None Epoch 49 Batch 181: Loss 1.3823611736297607
Model None Epoch 49 Batch 182: Loss 1.4157878160476685
Model None Epoch 49 Batch 183: Loss 1.391921043395996
Model None Epoch 49 Batch 184: Loss 1.482544183731079
Model None Epoch 49 Batch 185: Loss 1.3977223634719849
Model None Epoch 49 Batch 186: Loss 1.3913671970367432
Model None Epoch 49 Batch 187: Loss 1.3896334171295166
Model None Epoch 49 Batch 188: Loss 1.3293561935424805
Model None Epoch 49 Batch 189: Loss 1.4826220273971558
Model None Epoch 49 Batch 190: Loss 1.4446638822555542
Model None Epoch 49 Batch 191: Loss 1.3597192764282227
Model None Epoch 49 Batch 192: Loss 1.4127172231674194
Model None Epoch 49 Batch 193: Loss 1.4662576913833618
Model None Epoch 49 Batch 194: Loss 1.4032875299453735
Model None Epoch 49 Batch 195: Loss 1.4288358688354492

 Downstream Train loss: 1.4031321941589823 Acc: 0.5655
Downstream Train Epoch: 50 [0/50000 (0%)]	Loss: 1.426994
Model None Epoch 50 Batch 0: Loss 1.426993727684021
Model None Epoch 50 Batch 1: Loss 1.4545044898986816
Model None Epoch 50 Batch 2: Loss 1.4651641845703125
Model None Epoch 50 Batch 3: Loss 1.4708938598632812
Model None Epoch 50 Batch 4: Loss 1.38676118850708
Model None Epoch 50 Batch 5: Loss 1.4191288948059082
Model None Epoch 50 Batch 6: Loss 1.3954393863677979
Model None Epoch 50 Batch 7: Loss 1.4116090536117554
Model None Epoch 50 Batch 8: Loss 1.4923014640808105
Model None Epoch 50 Batch 9: Loss 1.5567049980163574
Model None Epoch 50 Batch 10: Loss 1.3332157135009766
Model None Epoch 50 Batch 11: Loss 1.4944555759429932
Model None Epoch 50 Batch 12: Loss 1.3342899084091187
Model None Epoch 50 Batch 13: Loss 1.4951260089874268
Model None Epoch 50 Batch 14: Loss 1.436518907546997
Model None Epoch 50 Batch 15: Loss 1.452375888824463
Model None Epoch 50 Batch 16: Loss 1.4248682260513306
Model None Epoch 50 Batch 17: Loss 1.36555814743042
Model None Epoch 50 Batch 18: Loss 1.3704935312271118
Model None Epoch 50 Batch 19: Loss 1.4696353673934937
Model None Epoch 50 Batch 20: Loss 1.3425719738006592
Model None Epoch 50 Batch 21: Loss 1.5304454565048218
Model None Epoch 50 Batch 22: Loss 1.4726505279541016
Model None Epoch 50 Batch 23: Loss 1.3863763809204102
Model None Epoch 50 Batch 24: Loss 1.388991355895996
Model None Epoch 50 Batch 25: Loss 1.462336540222168
Model None Epoch 50 Batch 26: Loss 1.3904627561569214
Model None Epoch 50 Batch 27: Loss 1.5194569826126099
Model None Epoch 50 Batch 28: Loss 1.3862957954406738
Model None Epoch 50 Batch 29: Loss 1.398144006729126
Model None Epoch 50 Batch 30: Loss 1.5205497741699219
Model None Epoch 50 Batch 31: Loss 1.410761833190918
Model None Epoch 50 Batch 32: Loss 1.1948446035385132
Model None Epoch 50 Batch 33: Loss 1.469727873802185
Model None Epoch 50 Batch 34: Loss 1.4657191038131714
Model None Epoch 50 Batch 35: Loss 1.4468724727630615
Model None Epoch 50 Batch 36: Loss 1.4544893503189087
Model None Epoch 50 Batch 37: Loss 1.404093861579895
Model None Epoch 50 Batch 38: Loss 1.4723435640335083
Model None Epoch 50 Batch 39: Loss 1.4068210124969482
Model None Epoch 50 Batch 40: Loss 1.355406641960144
Model None Epoch 50 Batch 41: Loss 1.344987154006958
Model None Epoch 50 Batch 42: Loss 1.5046416521072388
Model None Epoch 50 Batch 43: Loss 1.4301995038986206
Model None Epoch 50 Batch 44: Loss 1.5049452781677246
Model None Epoch 50 Batch 45: Loss 1.3202286958694458
Model None Epoch 50 Batch 46: Loss 1.3846371173858643
Model None Epoch 50 Batch 47: Loss 1.415366291999817
Model None Epoch 50 Batch 48: Loss 1.3688026666641235
Model None Epoch 50 Batch 49: Loss 1.511675238609314
Downstream Train Epoch: 50 [12800/50000 (26%)]	Loss: 1.324627
Model None Epoch 50 Batch 50: Loss 1.3246266841888428
Model None Epoch 50 Batch 51: Loss 1.3599858283996582
Model None Epoch 50 Batch 52: Loss 1.4181915521621704
Model None Epoch 50 Batch 53: Loss 1.3896961212158203
Model None Epoch 50 Batch 54: Loss 1.364344596862793
Model None Epoch 50 Batch 55: Loss 1.3711498975753784
Model None Epoch 50 Batch 56: Loss 1.3953787088394165
Model None Epoch 50 Batch 57: Loss 1.2976080179214478
Model None Epoch 50 Batch 58: Loss 1.4790000915527344
Model None Epoch 50 Batch 59: Loss 1.402984380722046
Model None Epoch 50 Batch 60: Loss 1.4758703708648682
Model None Epoch 50 Batch 61: Loss 1.4931023120880127
Model None Epoch 50 Batch 62: Loss 1.3566621541976929
Model None Epoch 50 Batch 63: Loss 1.4013642072677612
Model None Epoch 50 Batch 64: Loss 1.3456337451934814
Model None Epoch 50 Batch 65: Loss 1.3806087970733643
Model None Epoch 50 Batch 66: Loss 1.340692162513733
Model None Epoch 50 Batch 67: Loss 1.312052845954895
Model None Epoch 50 Batch 68: Loss 1.4609284400939941
Model None Epoch 50 Batch 69: Loss 1.390944004058838
Model None Epoch 50 Batch 70: Loss 1.379720687866211
Model None Epoch 50 Batch 71: Loss 1.432559609413147
Model None Epoch 50 Batch 72: Loss 1.3959771394729614
Model None Epoch 50 Batch 73: Loss 1.3460205793380737
Model None Epoch 50 Batch 74: Loss 1.3586994409561157
Model None Epoch 50 Batch 75: Loss 1.3876113891601562
Model None Epoch 50 Batch 76: Loss 1.5421600341796875
Model None Epoch 50 Batch 77: Loss 1.3628488779067993
Model None Epoch 50 Batch 78: Loss 1.4465867280960083
Model None Epoch 50 Batch 79: Loss 1.4613960981369019
Model None Epoch 50 Batch 80: Loss 1.4600268602371216
Model None Epoch 50 Batch 81: Loss 1.472127914428711
Model None Epoch 50 Batch 82: Loss 1.376434326171875
Model None Epoch 50 Batch 83: Loss 1.3869379758834839
Model None Epoch 50 Batch 84: Loss 1.388569951057434
Model None Epoch 50 Batch 85: Loss 1.4188671112060547
Model None Epoch 50 Batch 86: Loss 1.4013515710830688
Model None Epoch 50 Batch 87: Loss 1.4075218439102173
Model None Epoch 50 Batch 88: Loss 1.2614911794662476
Model None Epoch 50 Batch 89: Loss 1.5134074687957764
Model None Epoch 50 Batch 90: Loss 1.4646484851837158
Model None Epoch 50 Batch 91: Loss 1.3953739404678345
Model None Epoch 50 Batch 92: Loss 1.4189751148223877
Model None Epoch 50 Batch 93: Loss 1.368805170059204
Model None Epoch 50 Batch 94: Loss 1.330851674079895
Model None Epoch 50 Batch 95: Loss 1.3749516010284424
Model None Epoch 50 Batch 96: Loss 1.5765067338943481
Model None Epoch 50 Batch 97: Loss 1.3392250537872314
Model None Epoch 50 Batch 98: Loss 1.5264674425125122
Model None Epoch 50 Batch 99: Loss 1.3360059261322021
Downstream Train Epoch: 50 [25600/50000 (51%)]	Loss: 1.447650
Model None Epoch 50 Batch 100: Loss 1.447649598121643
Model None Epoch 50 Batch 101: Loss 1.3723031282424927
Model None Epoch 50 Batch 102: Loss 1.3593355417251587
Model None Epoch 50 Batch 103: Loss 1.2840386629104614
Model None Epoch 50 Batch 104: Loss 1.4085414409637451
Model None Epoch 50 Batch 105: Loss 1.5427639484405518
Model None Epoch 50 Batch 106: Loss 1.3735063076019287
Model None Epoch 50 Batch 107: Loss 1.391947865486145
Model None Epoch 50 Batch 108: Loss 1.3653433322906494
Model None Epoch 50 Batch 109: Loss 1.3461394309997559
Model None Epoch 50 Batch 110: Loss 1.350332260131836
Model None Epoch 50 Batch 111: Loss 1.4355722665786743
Model None Epoch 50 Batch 112: Loss 1.4013670682907104
Model None Epoch 50 Batch 113: Loss 1.4349031448364258
Model None Epoch 50 Batch 114: Loss 1.4096599817276
Model None Epoch 50 Batch 115: Loss 1.3626716136932373
Model None Epoch 50 Batch 116: Loss 1.3829917907714844
Model None Epoch 50 Batch 117: Loss 1.2926578521728516
Model None Epoch 50 Batch 118: Loss 1.4540107250213623
Model None Epoch 50 Batch 119: Loss 1.5071529150009155
Model None Epoch 50 Batch 120: Loss 1.4218034744262695
Model None Epoch 50 Batch 121: Loss 1.400768756866455
Model None Epoch 50 Batch 122: Loss 1.3860526084899902
Model None Epoch 50 Batch 123: Loss 1.4003002643585205
Model None Epoch 50 Batch 124: Loss 1.3119815587997437
Model None Epoch 50 Batch 125: Loss 1.3002010583877563
Model None Epoch 50 Batch 126: Loss 1.602400779724121
Model None Epoch 50 Batch 127: Loss 1.4647889137268066
Model None Epoch 50 Batch 128: Loss 1.4140461683273315
Model None Epoch 50 Batch 129: Loss 1.5312457084655762
Model None Epoch 50 Batch 130: Loss 1.4625279903411865
Model None Epoch 50 Batch 131: Loss 1.6197079420089722
Model None Epoch 50 Batch 132: Loss 1.3703999519348145
Model None Epoch 50 Batch 133: Loss 1.3449560403823853
Model None Epoch 50 Batch 134: Loss 1.4065873622894287
Model None Epoch 50 Batch 135: Loss 1.3727843761444092
Model None Epoch 50 Batch 136: Loss 1.4163624048233032
Model None Epoch 50 Batch 137: Loss 1.5389702320098877
Model None Epoch 50 Batch 138: Loss 1.4384305477142334
Model None Epoch 50 Batch 139: Loss 1.3050997257232666
Model None Epoch 50 Batch 140: Loss 1.2563304901123047
Model None Epoch 50 Batch 141: Loss 1.2981714010238647
Model None Epoch 50 Batch 142: Loss 1.3675779104232788
Model None Epoch 50 Batch 143: Loss 1.4914710521697998
Model None Epoch 50 Batch 144: Loss 1.4462125301361084
Model None Epoch 50 Batch 145: Loss 1.2854785919189453
Model None Epoch 50 Batch 146: Loss 1.3225653171539307
Model None Epoch 50 Batch 147: Loss 1.3850069046020508
Model None Epoch 50 Batch 148: Loss 1.372564673423767
Model None Epoch 50 Batch 149: Loss 1.3452844619750977
Downstream Train Epoch: 50 [38400/50000 (77%)]	Loss: 1.388023
Model None Epoch 50 Batch 150: Loss 1.388022780418396
Model None Epoch 50 Batch 151: Loss 1.3991082906723022
Model None Epoch 50 Batch 152: Loss 1.366876244544983
Model None Epoch 50 Batch 153: Loss 1.3586013317108154
Model None Epoch 50 Batch 154: Loss 1.4466639757156372
Model None Epoch 50 Batch 155: Loss 1.4481943845748901
Model None Epoch 50 Batch 156: Loss 1.4383678436279297
Model None Epoch 50 Batch 157: Loss 1.4061732292175293
Model None Epoch 50 Batch 158: Loss 1.340198278427124
Model None Epoch 50 Batch 159: Loss 1.410427451133728
Model None Epoch 50 Batch 160: Loss 1.3924106359481812
Model None Epoch 50 Batch 161: Loss 1.3826807737350464
Model None Epoch 50 Batch 162: Loss 1.316427230834961
Model None Epoch 50 Batch 163: Loss 1.365240454673767
Model None Epoch 50 Batch 164: Loss 1.4714030027389526
Model None Epoch 50 Batch 165: Loss 1.3759167194366455
Model None Epoch 50 Batch 166: Loss 1.3486825227737427
Model None Epoch 50 Batch 167: Loss 1.3846454620361328
Model None Epoch 50 Batch 168: Loss 1.4056528806686401
Model None Epoch 50 Batch 169: Loss 1.5849506855010986
Model None Epoch 50 Batch 170: Loss 1.3946304321289062
Model None Epoch 50 Batch 171: Loss 1.4192817211151123
Model None Epoch 50 Batch 172: Loss 1.4340293407440186
Model None Epoch 50 Batch 173: Loss 1.3584051132202148
Model None Epoch 50 Batch 174: Loss 1.5188720226287842
Model None Epoch 50 Batch 175: Loss 1.3628106117248535
Model None Epoch 50 Batch 176: Loss 1.3881862163543701
Model None Epoch 50 Batch 177: Loss 1.4291635751724243
Model None Epoch 50 Batch 178: Loss 1.4399839639663696
Model None Epoch 50 Batch 179: Loss 1.3878929615020752
Model None Epoch 50 Batch 180: Loss 1.3589924573898315
Model None Epoch 50 Batch 181: Loss 1.4427449703216553
Model None Epoch 50 Batch 182: Loss 1.4667037725448608
Model None Epoch 50 Batch 183: Loss 1.3910143375396729
Model None Epoch 50 Batch 184: Loss 1.39564049243927
Model None Epoch 50 Batch 185: Loss 1.5003557205200195
Model None Epoch 50 Batch 186: Loss 1.3252009153366089
Model None Epoch 50 Batch 187: Loss 1.3988945484161377
Model None Epoch 50 Batch 188: Loss 1.2697701454162598
Model None Epoch 50 Batch 189: Loss 1.3539626598358154
Model None Epoch 50 Batch 190: Loss 1.3712100982666016
Model None Epoch 50 Batch 191: Loss 1.3250510692596436
Model None Epoch 50 Batch 192: Loss 1.4718127250671387
Model None Epoch 50 Batch 193: Loss 1.3160055875778198
Model None Epoch 50 Batch 194: Loss 1.3878921270370483
Model None Epoch 50 Batch 195: Loss 1.4168800115585327

 Downstream Train loss: 1.4070042688019422 Acc: 0.5655
Downstream Train Epoch: 51 [0/50000 (0%)]	Loss: 1.487780
Model None Epoch 51 Batch 0: Loss 1.4877796173095703
Model None Epoch 51 Batch 1: Loss 1.4262080192565918
Model None Epoch 51 Batch 2: Loss 1.5429010391235352
Model None Epoch 51 Batch 3: Loss 1.263342022895813
Model None Epoch 51 Batch 4: Loss 1.3803634643554688
Model None Epoch 51 Batch 5: Loss 1.3321036100387573
Model None Epoch 51 Batch 6: Loss 1.3623803853988647
Model None Epoch 51 Batch 7: Loss 1.443382740020752
Model None Epoch 51 Batch 8: Loss 1.4073299169540405
Model None Epoch 51 Batch 9: Loss 1.314382553100586
Model None Epoch 51 Batch 10: Loss 1.4963877201080322
Model None Epoch 51 Batch 11: Loss 1.4292997121810913
Model None Epoch 51 Batch 12: Loss 1.3581788539886475
Model None Epoch 51 Batch 13: Loss 1.446246862411499
Model None Epoch 51 Batch 14: Loss 1.3919546604156494
Model None Epoch 51 Batch 15: Loss 1.4177154302597046
Model None Epoch 51 Batch 16: Loss 1.392801284790039
Model None Epoch 51 Batch 17: Loss 1.4166977405548096
Model None Epoch 51 Batch 18: Loss 1.3322348594665527
Model None Epoch 51 Batch 19: Loss 1.3125250339508057
Model None Epoch 51 Batch 20: Loss 1.4260205030441284
Model None Epoch 51 Batch 21: Loss 1.4715112447738647
Model None Epoch 51 Batch 22: Loss 1.3291605710983276
Model None Epoch 51 Batch 23: Loss 1.2665178775787354
Model None Epoch 51 Batch 24: Loss 1.382606029510498
Model None Epoch 51 Batch 25: Loss 1.374953031539917
Model None Epoch 51 Batch 26: Loss 1.4274359941482544
Model None Epoch 51 Batch 27: Loss 1.528715968132019
Model None Epoch 51 Batch 28: Loss 1.3570424318313599
Model None Epoch 51 Batch 29: Loss 1.3676817417144775
Model None Epoch 51 Batch 30: Loss 1.4271379709243774
Model None Epoch 51 Batch 31: Loss 1.3337123394012451
Model None Epoch 51 Batch 32: Loss 1.4556562900543213
Model None Epoch 51 Batch 33: Loss 1.521787166595459
Model None Epoch 51 Batch 34: Loss 1.421925663948059
Model None Epoch 51 Batch 35: Loss 1.4190514087677002
Model None Epoch 51 Batch 36: Loss 1.3648589849472046
Model None Epoch 51 Batch 37: Loss 1.3687970638275146
Model None Epoch 51 Batch 38: Loss 1.466280460357666
Model None Epoch 51 Batch 39: Loss 1.3781498670578003
Model None Epoch 51 Batch 40: Loss 1.486420750617981
Model None Epoch 51 Batch 41: Loss 1.387166142463684
Model None Epoch 51 Batch 42: Loss 1.445853590965271
Model None Epoch 51 Batch 43: Loss 1.383320927619934
Model None Epoch 51 Batch 44: Loss 1.4117685556411743
Model None Epoch 51 Batch 45: Loss 1.3115547895431519
Model None Epoch 51 Batch 46: Loss 1.4089195728302002
Model None Epoch 51 Batch 47: Loss 1.43522047996521
Model None Epoch 51 Batch 48: Loss 1.3394521474838257
Model None Epoch 51 Batch 49: Loss 1.4158058166503906
Downstream Train Epoch: 51 [12800/50000 (26%)]	Loss: 1.381104
Model None Epoch 51 Batch 50: Loss 1.381103515625
Model None Epoch 51 Batch 51: Loss 1.3449772596359253
Model None Epoch 51 Batch 52: Loss 1.4088326692581177
Model None Epoch 51 Batch 53: Loss 1.3217359781265259
Model None Epoch 51 Batch 54: Loss 1.3496479988098145
Model None Epoch 51 Batch 55: Loss 1.392397165298462
Model None Epoch 51 Batch 56: Loss 1.3791906833648682
Model None Epoch 51 Batch 57: Loss 1.3613760471343994
Model None Epoch 51 Batch 58: Loss 1.5970923900604248
Model None Epoch 51 Batch 59: Loss 1.3753008842468262
Model None Epoch 51 Batch 60: Loss 1.320565938949585
Model None Epoch 51 Batch 61: Loss 1.364374041557312
Model None Epoch 51 Batch 62: Loss 1.4260756969451904
Model None Epoch 51 Batch 63: Loss 1.2993059158325195
Model None Epoch 51 Batch 64: Loss 1.3887360095977783
Model None Epoch 51 Batch 65: Loss 1.4331814050674438
Model None Epoch 51 Batch 66: Loss 1.3848108053207397
Model None Epoch 51 Batch 67: Loss 1.406341552734375
Model None Epoch 51 Batch 68: Loss 1.373518943786621
Model None Epoch 51 Batch 69: Loss 1.3127307891845703
Model None Epoch 51 Batch 70: Loss 1.3538929224014282
Model None Epoch 51 Batch 71: Loss 1.364527702331543
Model None Epoch 51 Batch 72: Loss 1.36361825466156
Model None Epoch 51 Batch 73: Loss 1.3516489267349243
Model None Epoch 51 Batch 74: Loss 1.5554661750793457
Model None Epoch 51 Batch 75: Loss 1.4769366979599
Model None Epoch 51 Batch 76: Loss 1.4041858911514282
Model None Epoch 51 Batch 77: Loss 1.2905360460281372
Model None Epoch 51 Batch 78: Loss 1.3304059505462646
Model None Epoch 51 Batch 79: Loss 1.3848464488983154
Model None Epoch 51 Batch 80: Loss 1.3002185821533203
Model None Epoch 51 Batch 81: Loss 1.3398809432983398
Model None Epoch 51 Batch 82: Loss 1.4794312715530396
Model None Epoch 51 Batch 83: Loss 1.407536506652832
Model None Epoch 51 Batch 84: Loss 1.3779430389404297
Model None Epoch 51 Batch 85: Loss 1.4957990646362305
Model None Epoch 51 Batch 86: Loss 1.4839974641799927
Model None Epoch 51 Batch 87: Loss 1.4807255268096924
Model None Epoch 51 Batch 88: Loss 1.478247046470642
Model None Epoch 51 Batch 89: Loss 1.3988310098648071
Model None Epoch 51 Batch 90: Loss 1.5336359739303589
Model None Epoch 51 Batch 91: Loss 1.3625335693359375
Model None Epoch 51 Batch 92: Loss 1.5101591348648071
Model None Epoch 51 Batch 93: Loss 1.4207054376602173
Model None Epoch 51 Batch 94: Loss 1.3104435205459595
Model None Epoch 51 Batch 95: Loss 1.3634499311447144
Model None Epoch 51 Batch 96: Loss 1.358575701713562
Model None Epoch 51 Batch 97: Loss 1.4388091564178467
Model None Epoch 51 Batch 98: Loss 1.3542031049728394
Model None Epoch 51 Batch 99: Loss 1.4730370044708252
Downstream Train Epoch: 51 [25600/50000 (51%)]	Loss: 1.407735
Model None Epoch 51 Batch 100: Loss 1.407734990119934
Model None Epoch 51 Batch 101: Loss 1.432753086090088
Model None Epoch 51 Batch 102: Loss 1.4260084629058838
Model None Epoch 51 Batch 103: Loss 1.4223873615264893
Model None Epoch 51 Batch 104: Loss 1.5194060802459717
Model None Epoch 51 Batch 105: Loss 1.5201135873794556
Model None Epoch 51 Batch 106: Loss 1.288011908531189
Model None Epoch 51 Batch 107: Loss 1.4375841617584229
Model None Epoch 51 Batch 108: Loss 1.3658417463302612
Model None Epoch 51 Batch 109: Loss 1.4401638507843018
Model None Epoch 51 Batch 110: Loss 1.4465417861938477
Model None Epoch 51 Batch 111: Loss 1.4426584243774414
Model None Epoch 51 Batch 112: Loss 1.479817271232605
Model None Epoch 51 Batch 113: Loss 1.3322875499725342
Model None Epoch 51 Batch 114: Loss 1.4008698463439941
Model None Epoch 51 Batch 115: Loss 1.2926424741744995
Model None Epoch 51 Batch 116: Loss 1.3761541843414307
Model None Epoch 51 Batch 117: Loss 1.4024378061294556
Model None Epoch 51 Batch 118: Loss 1.3544398546218872
Model None Epoch 51 Batch 119: Loss 1.4758930206298828
Model None Epoch 51 Batch 120: Loss 1.3035999536514282
Model None Epoch 51 Batch 121: Loss 1.3907660245895386
Model None Epoch 51 Batch 122: Loss 1.3984954357147217
Model None Epoch 51 Batch 123: Loss 1.4629905223846436
Model None Epoch 51 Batch 124: Loss 1.4290485382080078
Model None Epoch 51 Batch 125: Loss 1.4611185789108276
Model None Epoch 51 Batch 126: Loss 1.3547346591949463
Model None Epoch 51 Batch 127: Loss 1.4122332334518433
Model None Epoch 51 Batch 128: Loss 1.4326322078704834
Model None Epoch 51 Batch 129: Loss 1.3785043954849243
Model None Epoch 51 Batch 130: Loss 1.549891710281372
Model None Epoch 51 Batch 131: Loss 1.36855947971344
Model None Epoch 51 Batch 132: Loss 1.3981904983520508
Model None Epoch 51 Batch 133: Loss 1.31318998336792
Model None Epoch 51 Batch 134: Loss 1.3373430967330933
Model None Epoch 51 Batch 135: Loss 1.4606579542160034
Model None Epoch 51 Batch 136: Loss 1.3532018661499023
Model None Epoch 51 Batch 137: Loss 1.3039920330047607
Model None Epoch 51 Batch 138: Loss 1.4524682760238647
Model None Epoch 51 Batch 139: Loss 1.5059278011322021
Model None Epoch 51 Batch 140: Loss 1.4264187812805176
Model None Epoch 51 Batch 141: Loss 1.3378912210464478
Model None Epoch 51 Batch 142: Loss 1.3587297201156616
Model None Epoch 51 Batch 143: Loss 1.4661281108856201
Model None Epoch 51 Batch 144: Loss 1.379984974861145
Model None Epoch 51 Batch 145: Loss 1.4269670248031616
Model None Epoch 51 Batch 146: Loss 1.5073826313018799
Model None Epoch 51 Batch 147: Loss 1.3563404083251953
Model None Epoch 51 Batch 148: Loss 1.3487918376922607
Model None Epoch 51 Batch 149: Loss 1.3903212547302246
Downstream Train Epoch: 51 [38400/50000 (77%)]	Loss: 1.449491
Model None Epoch 51 Batch 150: Loss 1.4494913816452026
Model None Epoch 51 Batch 151: Loss 1.4092321395874023
Model None Epoch 51 Batch 152: Loss 1.4216984510421753
Model None Epoch 51 Batch 153: Loss 1.4349807500839233
Model None Epoch 51 Batch 154: Loss 1.431064486503601
Model None Epoch 51 Batch 155: Loss 1.3535661697387695
Model None Epoch 51 Batch 156: Loss 1.2659639120101929
Model None Epoch 51 Batch 157: Loss 1.4136607646942139
Model None Epoch 51 Batch 158: Loss 1.4587631225585938
Model None Epoch 51 Batch 159: Loss 1.3960950374603271
Model None Epoch 51 Batch 160: Loss 1.4523221254348755
Model None Epoch 51 Batch 161: Loss 1.5216690301895142
Model None Epoch 51 Batch 162: Loss 1.3807498216629028
Model None Epoch 51 Batch 163: Loss 1.3846213817596436
Model None Epoch 51 Batch 164: Loss 1.3969937562942505
Model None Epoch 51 Batch 165: Loss 1.447303056716919
Model None Epoch 51 Batch 166: Loss 1.366585612297058
Model None Epoch 51 Batch 167: Loss 1.450088620185852
Model None Epoch 51 Batch 168: Loss 1.3477330207824707
Model None Epoch 51 Batch 169: Loss 1.373448371887207
Model None Epoch 51 Batch 170: Loss 1.4876651763916016
Model None Epoch 51 Batch 171: Loss 1.4281271696090698
Model None Epoch 51 Batch 172: Loss 1.4029964208602905
Model None Epoch 51 Batch 173: Loss 1.3670387268066406
Model None Epoch 51 Batch 174: Loss 1.4374138116836548
Model None Epoch 51 Batch 175: Loss 1.3515942096710205
Model None Epoch 51 Batch 176: Loss 1.4350942373275757
Model None Epoch 51 Batch 177: Loss 1.4642986059188843
Model None Epoch 51 Batch 178: Loss 1.4275661706924438
Model None Epoch 51 Batch 179: Loss 1.3646093606948853
Model None Epoch 51 Batch 180: Loss 1.2813608646392822
Model None Epoch 51 Batch 181: Loss 1.412283182144165
Model None Epoch 51 Batch 182: Loss 1.3834010362625122
Model None Epoch 51 Batch 183: Loss 1.4029300212860107
Model None Epoch 51 Batch 184: Loss 1.4033435583114624
Model None Epoch 51 Batch 185: Loss 1.3913424015045166
Model None Epoch 51 Batch 186: Loss 1.4786275625228882
Model None Epoch 51 Batch 187: Loss 1.5080426931381226
Model None Epoch 51 Batch 188: Loss 1.3506994247436523
Model None Epoch 51 Batch 189: Loss 1.3357659578323364
Model None Epoch 51 Batch 190: Loss 1.3929117918014526
Model None Epoch 51 Batch 191: Loss 1.4146281480789185
Model None Epoch 51 Batch 192: Loss 1.4348163604736328
Model None Epoch 51 Batch 193: Loss 1.3469172716140747
Model None Epoch 51 Batch 194: Loss 1.4768526554107666
Model None Epoch 51 Batch 195: Loss 1.4512784481048584

 Downstream Train loss: 1.4033270946570806 Acc: 0.5655
Downstream Train Epoch: 52 [0/50000 (0%)]	Loss: 1.403116
Model None Epoch 52 Batch 0: Loss 1.4031155109405518
Model None Epoch 52 Batch 1: Loss 1.2898534536361694
Model None Epoch 52 Batch 2: Loss 1.3595075607299805
Model None Epoch 52 Batch 3: Loss 1.356719732284546
Model None Epoch 52 Batch 4: Loss 1.3455135822296143
Model None Epoch 52 Batch 5: Loss 1.3414274454116821
Model None Epoch 52 Batch 6: Loss 1.3703582286834717
Model None Epoch 52 Batch 7: Loss 1.4123575687408447
Model None Epoch 52 Batch 8: Loss 1.539820909500122
Model None Epoch 52 Batch 9: Loss 1.4415814876556396
Model None Epoch 52 Batch 10: Loss 1.4092155694961548
Model None Epoch 52 Batch 11: Loss 1.5130615234375
Model None Epoch 52 Batch 12: Loss 1.45175039768219
Model None Epoch 52 Batch 13: Loss 1.5129239559173584
Model None Epoch 52 Batch 14: Loss 1.4079869985580444
Model None Epoch 52 Batch 15: Loss 1.3455688953399658
Model None Epoch 52 Batch 16: Loss 1.3943418264389038
Model None Epoch 52 Batch 17: Loss 1.337056040763855
Model None Epoch 52 Batch 18: Loss 1.3643527030944824
Model None Epoch 52 Batch 19: Loss 1.4633442163467407
Model None Epoch 52 Batch 20: Loss 1.4107847213745117
Model None Epoch 52 Batch 21: Loss 1.3464820384979248
Model None Epoch 52 Batch 22: Loss 1.4735133647918701
Model None Epoch 52 Batch 23: Loss 1.4324861764907837
Model None Epoch 52 Batch 24: Loss 1.387505054473877
Model None Epoch 52 Batch 25: Loss 1.5002027750015259
Model None Epoch 52 Batch 26: Loss 1.3987256288528442
Model None Epoch 52 Batch 27: Loss 1.4100886583328247
Model None Epoch 52 Batch 28: Loss 1.3751357793807983
Model None Epoch 52 Batch 29: Loss 1.4339882135391235
Model None Epoch 52 Batch 30: Loss 1.3247044086456299
Model None Epoch 52 Batch 31: Loss 1.3357863426208496
Model None Epoch 52 Batch 32: Loss 1.3293793201446533
Model None Epoch 52 Batch 33: Loss 1.3582004308700562
Model None Epoch 52 Batch 34: Loss 1.5049819946289062
Model None Epoch 52 Batch 35: Loss 1.4473872184753418
Model None Epoch 52 Batch 36: Loss 1.4636776447296143
Model None Epoch 52 Batch 37: Loss 1.3398964405059814
Model None Epoch 52 Batch 38: Loss 1.421642780303955
Model None Epoch 52 Batch 39: Loss 1.398927092552185
Model None Epoch 52 Batch 40: Loss 1.5213531255722046
Model None Epoch 52 Batch 41: Loss 1.4509973526000977
Model None Epoch 52 Batch 42: Loss 1.3528581857681274
Model None Epoch 52 Batch 43: Loss 1.3753219842910767
Model None Epoch 52 Batch 44: Loss 1.3970792293548584
Model None Epoch 52 Batch 45: Loss 1.3549665212631226
Model None Epoch 52 Batch 46: Loss 1.347264289855957
Model None Epoch 52 Batch 47: Loss 1.384054183959961
Model None Epoch 52 Batch 48: Loss 1.4128621816635132
Model None Epoch 52 Batch 49: Loss 1.4928979873657227
Downstream Train Epoch: 52 [12800/50000 (26%)]	Loss: 1.331173
Model None Epoch 52 Batch 50: Loss 1.3311727046966553
Model None Epoch 52 Batch 51: Loss 1.4073808193206787
Model None Epoch 52 Batch 52: Loss 1.3784844875335693
Model None Epoch 52 Batch 53: Loss 1.4014427661895752
Model None Epoch 52 Batch 54: Loss 1.4341652393341064
Model None Epoch 52 Batch 55: Loss 1.3721307516098022
Model None Epoch 52 Batch 56: Loss 1.3582217693328857
Model None Epoch 52 Batch 57: Loss 1.386520504951477
Model None Epoch 52 Batch 58: Loss 1.336826205253601
Model None Epoch 52 Batch 59: Loss 1.394822120666504
Model None Epoch 52 Batch 60: Loss 1.43564772605896
Model None Epoch 52 Batch 61: Loss 1.2183645963668823
Model None Epoch 52 Batch 62: Loss 1.3720412254333496
Model None Epoch 52 Batch 63: Loss 1.349568486213684
Model None Epoch 52 Batch 64: Loss 1.3634474277496338
Model None Epoch 52 Batch 65: Loss 1.3538241386413574
Model None Epoch 52 Batch 66: Loss 1.5090105533599854
Model None Epoch 52 Batch 67: Loss 1.3085978031158447
Model None Epoch 52 Batch 68: Loss 1.3540856838226318
Model None Epoch 52 Batch 69: Loss 1.294557809829712
Model None Epoch 52 Batch 70: Loss 1.481552004814148
Model None Epoch 52 Batch 71: Loss 1.3257230520248413
Model None Epoch 52 Batch 72: Loss 1.4467623233795166
Model None Epoch 52 Batch 73: Loss 1.2872745990753174
Model None Epoch 52 Batch 74: Loss 1.478596806526184
Model None Epoch 52 Batch 75: Loss 1.2982538938522339
Model None Epoch 52 Batch 76: Loss 1.4636774063110352
Model None Epoch 52 Batch 77: Loss 1.3572245836257935
Model None Epoch 52 Batch 78: Loss 1.427436351776123
Model None Epoch 52 Batch 79: Loss 1.585119605064392
Model None Epoch 52 Batch 80: Loss 1.3921961784362793
Model None Epoch 52 Batch 81: Loss 1.34467351436615
Model None Epoch 52 Batch 82: Loss 1.326228380203247
Model None Epoch 52 Batch 83: Loss 1.5305790901184082
Model None Epoch 52 Batch 84: Loss 1.516709327697754
Model None Epoch 52 Batch 85: Loss 1.5136841535568237
Model None Epoch 52 Batch 86: Loss 1.4222087860107422
Model None Epoch 52 Batch 87: Loss 1.3598570823669434
Model None Epoch 52 Batch 88: Loss 1.4366636276245117
Model None Epoch 52 Batch 89: Loss 1.3705165386199951
Model None Epoch 52 Batch 90: Loss 1.4423654079437256
Model None Epoch 52 Batch 91: Loss 1.4213008880615234
Model None Epoch 52 Batch 92: Loss 1.4437741041183472
Model None Epoch 52 Batch 93: Loss 1.4111641645431519
Model None Epoch 52 Batch 94: Loss 1.3020460605621338
Model None Epoch 52 Batch 95: Loss 1.4201208353042603
Model None Epoch 52 Batch 96: Loss 1.4233900308609009
Model None Epoch 52 Batch 97: Loss 1.5008792877197266
Model None Epoch 52 Batch 98: Loss 1.4396764039993286
Model None Epoch 52 Batch 99: Loss 1.430152416229248
Downstream Train Epoch: 52 [25600/50000 (51%)]	Loss: 1.384940
Model None Epoch 52 Batch 100: Loss 1.3849395513534546
Model None Epoch 52 Batch 101: Loss 1.3746289014816284
Model None Epoch 52 Batch 102: Loss 1.5324137210845947
Model None Epoch 52 Batch 103: Loss 1.3416510820388794
Model None Epoch 52 Batch 104: Loss 1.3805088996887207
Model None Epoch 52 Batch 105: Loss 1.4302729368209839
Model None Epoch 52 Batch 106: Loss 1.4117199182510376
Model None Epoch 52 Batch 107: Loss 1.4112186431884766
Model None Epoch 52 Batch 108: Loss 1.4369179010391235
Model None Epoch 52 Batch 109: Loss 1.3961424827575684
Model None Epoch 52 Batch 110: Loss 1.4332275390625
Model None Epoch 52 Batch 111: Loss 1.3374502658843994
Model None Epoch 52 Batch 112: Loss 1.4303425550460815
Model None Epoch 52 Batch 113: Loss 1.443260908126831
Model None Epoch 52 Batch 114: Loss 1.417630910873413
Model None Epoch 52 Batch 115: Loss 1.3479015827178955
Model None Epoch 52 Batch 116: Loss 1.277256965637207
Model None Epoch 52 Batch 117: Loss 1.3399899005889893
Model None Epoch 52 Batch 118: Loss 1.3944467306137085
Model None Epoch 52 Batch 119: Loss 1.422698736190796
Model None Epoch 52 Batch 120: Loss 1.3218989372253418
Model None Epoch 52 Batch 121: Loss 1.4091905355453491
Model None Epoch 52 Batch 122: Loss 1.4191954135894775
Model None Epoch 52 Batch 123: Loss 1.493219017982483
Model None Epoch 52 Batch 124: Loss 1.3268344402313232
Model None Epoch 52 Batch 125: Loss 1.2769548892974854
Model None Epoch 52 Batch 126: Loss 1.4257556200027466
Model None Epoch 52 Batch 127: Loss 1.370361328125
Model None Epoch 52 Batch 128: Loss 1.3528242111206055
Model None Epoch 52 Batch 129: Loss 1.3852728605270386
Model None Epoch 52 Batch 130: Loss 1.4430701732635498
Model None Epoch 52 Batch 131: Loss 1.3157423734664917
Model None Epoch 52 Batch 132: Loss 1.4521254301071167
Model None Epoch 52 Batch 133: Loss 1.4602136611938477
Model None Epoch 52 Batch 134: Loss 1.5107725858688354
Model None Epoch 52 Batch 135: Loss 1.4212822914123535
Model None Epoch 52 Batch 136: Loss 1.4669092893600464
Model None Epoch 52 Batch 137: Loss 1.516546368598938
Model None Epoch 52 Batch 138: Loss 1.476481318473816
Model None Epoch 52 Batch 139: Loss 1.3306618928909302
Model None Epoch 52 Batch 140: Loss 1.4638631343841553
Model None Epoch 52 Batch 141: Loss 1.3816149234771729
Model None Epoch 52 Batch 142: Loss 1.5385099649429321
Model None Epoch 52 Batch 143: Loss 1.2234476804733276
Model None Epoch 52 Batch 144: Loss 1.2641164064407349
Model None Epoch 52 Batch 145: Loss 1.5181174278259277
Model None Epoch 52 Batch 146: Loss 1.3958418369293213
Model None Epoch 52 Batch 147: Loss 1.432163953781128
Model None Epoch 52 Batch 148: Loss 1.4281620979309082
Model None Epoch 52 Batch 149: Loss 1.3611798286437988
Downstream Train Epoch: 52 [38400/50000 (77%)]	Loss: 1.391778
Model None Epoch 52 Batch 150: Loss 1.3917778730392456
Model None Epoch 52 Batch 151: Loss 1.3965485095977783
Model None Epoch 52 Batch 152: Loss 1.431846022605896
Model None Epoch 52 Batch 153: Loss 1.3475254774093628
Model None Epoch 52 Batch 154: Loss 1.328808307647705
Model None Epoch 52 Batch 155: Loss 1.3428603410720825
Model None Epoch 52 Batch 156: Loss 1.2674366235733032
Model None Epoch 52 Batch 157: Loss 1.344007134437561
Model None Epoch 52 Batch 158: Loss 1.4137283563613892
Model None Epoch 52 Batch 159: Loss 1.3394471406936646
Model None Epoch 52 Batch 160: Loss 1.2595417499542236
Model None Epoch 52 Batch 161: Loss 1.4954679012298584
Model None Epoch 52 Batch 162: Loss 1.3281505107879639
Model None Epoch 52 Batch 163: Loss 1.3961398601531982
Model None Epoch 52 Batch 164: Loss 1.3853142261505127
Model None Epoch 52 Batch 165: Loss 1.475893259048462
Model None Epoch 52 Batch 166: Loss 1.4017173051834106
Model None Epoch 52 Batch 167: Loss 1.3267788887023926
Model None Epoch 52 Batch 168: Loss 1.4685168266296387
Model None Epoch 52 Batch 169: Loss 1.364566683769226
Model None Epoch 52 Batch 170: Loss 1.369266390800476
Model None Epoch 52 Batch 171: Loss 1.334769368171692
Model None Epoch 52 Batch 172: Loss 1.409045696258545
Model None Epoch 52 Batch 173: Loss 1.3855366706848145
Model None Epoch 52 Batch 174: Loss 1.3914700746536255
Model None Epoch 52 Batch 175: Loss 1.554133653640747
Model None Epoch 52 Batch 176: Loss 1.4041160345077515
Model None Epoch 52 Batch 177: Loss 1.3387629985809326
Model None Epoch 52 Batch 178: Loss 1.4182111024856567
Model None Epoch 52 Batch 179: Loss 1.3587965965270996
Model None Epoch 52 Batch 180: Loss 1.4476624727249146
Model None Epoch 52 Batch 181: Loss 1.5515310764312744
Model None Epoch 52 Batch 182: Loss 1.4596774578094482
Model None Epoch 52 Batch 183: Loss 1.432206630706787
Model None Epoch 52 Batch 184: Loss 1.4091885089874268
Model None Epoch 52 Batch 185: Loss 1.3233392238616943
Model None Epoch 52 Batch 186: Loss 1.4881629943847656
Model None Epoch 52 Batch 187: Loss 1.379987120628357
Model None Epoch 52 Batch 188: Loss 1.4870082139968872
Model None Epoch 52 Batch 189: Loss 1.3918137550354004
Model None Epoch 52 Batch 190: Loss 1.3581650257110596
Model None Epoch 52 Batch 191: Loss 1.3088666200637817
Model None Epoch 52 Batch 192: Loss 1.3860526084899902
Model None Epoch 52 Batch 193: Loss 1.4743465185165405
Model None Epoch 52 Batch 194: Loss 1.4798270463943481
Model None Epoch 52 Batch 195: Loss 1.3007346391677856

 Downstream Train loss: 1.4004021937749824 Acc: 0.5655
Downstream Train Epoch: 53 [0/50000 (0%)]	Loss: 1.390768
Model None Epoch 53 Batch 0: Loss 1.390768051147461
Model None Epoch 53 Batch 1: Loss 1.5629515647888184
Model None Epoch 53 Batch 2: Loss 1.550190806388855
Model None Epoch 53 Batch 3: Loss 1.5463026762008667
Model None Epoch 53 Batch 4: Loss 1.4253160953521729
Model None Epoch 53 Batch 5: Loss 1.340287208557129
Model None Epoch 53 Batch 6: Loss 1.4517664909362793
Model None Epoch 53 Batch 7: Loss 1.3314682245254517
Model None Epoch 53 Batch 8: Loss 1.3890938758850098
Model None Epoch 53 Batch 9: Loss 1.4645262956619263
Model None Epoch 53 Batch 10: Loss 1.3439784049987793
Model None Epoch 53 Batch 11: Loss 1.3846416473388672
Model None Epoch 53 Batch 12: Loss 1.333646535873413
Model None Epoch 53 Batch 13: Loss 1.2907569408416748
Model None Epoch 53 Batch 14: Loss 1.5244526863098145
Model None Epoch 53 Batch 15: Loss 1.5139676332473755
Model None Epoch 53 Batch 16: Loss 1.4627364873886108
Model None Epoch 53 Batch 17: Loss 1.3108450174331665
Model None Epoch 53 Batch 18: Loss 1.3349322080612183
Model None Epoch 53 Batch 19: Loss 1.4732847213745117
Model None Epoch 53 Batch 20: Loss 1.4416359663009644
Model None Epoch 53 Batch 21: Loss 1.3459020853042603
Model None Epoch 53 Batch 22: Loss 1.3846564292907715
Model None Epoch 53 Batch 23: Loss 1.5522211790084839
Model None Epoch 53 Batch 24: Loss 1.4069299697875977
Model None Epoch 53 Batch 25: Loss 1.341221809387207
Model None Epoch 53 Batch 26: Loss 1.4293406009674072
Model None Epoch 53 Batch 27: Loss 1.3124454021453857
Model None Epoch 53 Batch 28: Loss 1.461082935333252
Model None Epoch 53 Batch 29: Loss 1.4331157207489014
Model None Epoch 53 Batch 30: Loss 1.39606511592865
Model None Epoch 53 Batch 31: Loss 1.4306694269180298
Model None Epoch 53 Batch 32: Loss 1.3250808715820312
Model None Epoch 53 Batch 33: Loss 1.4105544090270996
Model None Epoch 53 Batch 34: Loss 1.4179736375808716
Model None Epoch 53 Batch 35: Loss 1.4125438928604126
Model None Epoch 53 Batch 36: Loss 1.4013856649398804
Model None Epoch 53 Batch 37: Loss 1.396144986152649
Model None Epoch 53 Batch 38: Loss 1.4583024978637695
Model None Epoch 53 Batch 39: Loss 1.3843681812286377
Model None Epoch 53 Batch 40: Loss 1.3866382837295532
Model None Epoch 53 Batch 41: Loss 1.429880142211914
Model None Epoch 53 Batch 42: Loss 1.5260505676269531
Model None Epoch 53 Batch 43: Loss 1.4345507621765137
Model None Epoch 53 Batch 44: Loss 1.5187053680419922
Model None Epoch 53 Batch 45: Loss 1.4813799858093262
Model None Epoch 53 Batch 46: Loss 1.5021634101867676
Model None Epoch 53 Batch 47: Loss 1.447205901145935
Model None Epoch 53 Batch 48: Loss 1.4570952653884888
Model None Epoch 53 Batch 49: Loss 1.3939512968063354
Downstream Train Epoch: 53 [12800/50000 (26%)]	Loss: 1.244509
Model None Epoch 53 Batch 50: Loss 1.244508981704712
Model None Epoch 53 Batch 51: Loss 1.3420058488845825
Model None Epoch 53 Batch 52: Loss 1.4368199110031128
Model None Epoch 53 Batch 53: Loss 1.3490928411483765
Model None Epoch 53 Batch 54: Loss 1.4332637786865234
Model None Epoch 53 Batch 55: Loss 1.3847277164459229
Model None Epoch 53 Batch 56: Loss 1.4162788391113281
Model None Epoch 53 Batch 57: Loss 1.397411823272705
Model None Epoch 53 Batch 58: Loss 1.38673734664917
Model None Epoch 53 Batch 59: Loss 1.3817802667617798
Model None Epoch 53 Batch 60: Loss 1.4282971620559692
Model None Epoch 53 Batch 61: Loss 1.3052982091903687
Model None Epoch 53 Batch 62: Loss 1.4226163625717163
Model None Epoch 53 Batch 63: Loss 1.3175530433654785
Model None Epoch 53 Batch 64: Loss 1.4651858806610107
Model None Epoch 53 Batch 65: Loss 1.365897297859192
Model None Epoch 53 Batch 66: Loss 1.3339390754699707
Model None Epoch 53 Batch 67: Loss 1.3286333084106445
Model None Epoch 53 Batch 68: Loss 1.3290574550628662
Model None Epoch 53 Batch 69: Loss 1.338722586631775
Model None Epoch 53 Batch 70: Loss 1.3978314399719238
Model None Epoch 53 Batch 71: Loss 1.3665218353271484
Model None Epoch 53 Batch 72: Loss 1.4182547330856323
Model None Epoch 53 Batch 73: Loss 1.4903700351715088
Model None Epoch 53 Batch 74: Loss 1.3549299240112305
Model None Epoch 53 Batch 75: Loss 1.4255038499832153
Model None Epoch 53 Batch 76: Loss 1.3674523830413818
Model None Epoch 53 Batch 77: Loss 1.4419677257537842
Model None Epoch 53 Batch 78: Loss 1.3358675241470337
Model None Epoch 53 Batch 79: Loss 1.4744032621383667
Model None Epoch 53 Batch 80: Loss 1.3491551876068115
Model None Epoch 53 Batch 81: Loss 1.422927737236023
Model None Epoch 53 Batch 82: Loss 1.280662178993225
Model None Epoch 53 Batch 83: Loss 1.3819350004196167
Model None Epoch 53 Batch 84: Loss 1.3127838373184204
Model None Epoch 53 Batch 85: Loss 1.5098711252212524
Model None Epoch 53 Batch 86: Loss 1.4656982421875
Model None Epoch 53 Batch 87: Loss 1.4432133436203003
Model None Epoch 53 Batch 88: Loss 1.3629560470581055
Model None Epoch 53 Batch 89: Loss 1.4427640438079834
Model None Epoch 53 Batch 90: Loss 1.4547245502471924
Model None Epoch 53 Batch 91: Loss 1.3336955308914185
Model None Epoch 53 Batch 92: Loss 1.398138165473938
Model None Epoch 53 Batch 93: Loss 1.4365715980529785
Model None Epoch 53 Batch 94: Loss 1.331753134727478
Model None Epoch 53 Batch 95: Loss 1.3995245695114136
Model None Epoch 53 Batch 96: Loss 1.333375096321106
Model None Epoch 53 Batch 97: Loss 1.4122062921524048
Model None Epoch 53 Batch 98: Loss 1.396662712097168
Model None Epoch 53 Batch 99: Loss 1.399775743484497
Downstream Train Epoch: 53 [25600/50000 (51%)]	Loss: 1.449343
Model None Epoch 53 Batch 100: Loss 1.4493433237075806
Model None Epoch 53 Batch 101: Loss 1.3735216856002808
Model None Epoch 53 Batch 102: Loss 1.3682165145874023
Model None Epoch 53 Batch 103: Loss 1.320451021194458
Model None Epoch 53 Batch 104: Loss 1.3977077007293701
Model None Epoch 53 Batch 105: Loss 1.3371713161468506
Model None Epoch 53 Batch 106: Loss 1.496464490890503
Model None Epoch 53 Batch 107: Loss 1.3484725952148438
Model None Epoch 53 Batch 108: Loss 1.4307795763015747
Model None Epoch 53 Batch 109: Loss 1.3283535242080688
Model None Epoch 53 Batch 110: Loss 1.3875852823257446
Model None Epoch 53 Batch 111: Loss 1.3450617790222168
Model None Epoch 53 Batch 112: Loss 1.3411110639572144
Model None Epoch 53 Batch 113: Loss 1.3752634525299072
Model None Epoch 53 Batch 114: Loss 1.4199321269989014
Model None Epoch 53 Batch 115: Loss 1.4415770769119263
Model None Epoch 53 Batch 116: Loss 1.3711750507354736
Model None Epoch 53 Batch 117: Loss 1.375639796257019
Model None Epoch 53 Batch 118: Loss 1.4266808032989502
Model None Epoch 53 Batch 119: Loss 1.4876055717468262
Model None Epoch 53 Batch 120: Loss 1.2116214036941528
Model None Epoch 53 Batch 121: Loss 1.3645433187484741
Model None Epoch 53 Batch 122: Loss 1.4315576553344727
Model None Epoch 53 Batch 123: Loss 1.4552263021469116
Model None Epoch 53 Batch 124: Loss 1.36093008518219
Model None Epoch 53 Batch 125: Loss 1.4052947759628296
Model None Epoch 53 Batch 126: Loss 1.3133543729782104
Model None Epoch 53 Batch 127: Loss 1.5435938835144043
Model None Epoch 53 Batch 128: Loss 1.4423365592956543
Model None Epoch 53 Batch 129: Loss 1.4614804983139038
Model None Epoch 53 Batch 130: Loss 1.4498869180679321
Model None Epoch 53 Batch 131: Loss 1.3378798961639404
Model None Epoch 53 Batch 132: Loss 1.3405122756958008
Model None Epoch 53 Batch 133: Loss 1.4832146167755127
Model None Epoch 53 Batch 134: Loss 1.3270037174224854
Model None Epoch 53 Batch 135: Loss 1.3372665643692017
Model None Epoch 53 Batch 136: Loss 1.5391732454299927
Model None Epoch 53 Batch 137: Loss 1.462424397468567
Model None Epoch 53 Batch 138: Loss 1.4459501504898071
Model None Epoch 53 Batch 139: Loss 1.3400274515151978
Model None Epoch 53 Batch 140: Loss 1.4338123798370361
Model None Epoch 53 Batch 141: Loss 1.458121418952942
Model None Epoch 53 Batch 142: Loss 1.4083423614501953
Model None Epoch 53 Batch 143: Loss 1.4036237001419067
Model None Epoch 53 Batch 144: Loss 1.5267632007598877
Model None Epoch 53 Batch 145: Loss 1.4487460851669312
Model None Epoch 53 Batch 146: Loss 1.4075500965118408
Model None Epoch 53 Batch 147: Loss 1.355226755142212
Model None Epoch 53 Batch 148: Loss 1.494065761566162
Model None Epoch 53 Batch 149: Loss 1.447068452835083
Downstream Train Epoch: 53 [38400/50000 (77%)]	Loss: 1.380533
Model None Epoch 53 Batch 150: Loss 1.3805325031280518
Model None Epoch 53 Batch 151: Loss 1.4237849712371826
Model None Epoch 53 Batch 152: Loss 1.4382810592651367
Model None Epoch 53 Batch 153: Loss 1.3090094327926636
Model None Epoch 53 Batch 154: Loss 1.5428624153137207
Model None Epoch 53 Batch 155: Loss 1.4024144411087036
Model None Epoch 53 Batch 156: Loss 1.5162644386291504
Model None Epoch 53 Batch 157: Loss 1.3886550664901733
Model None Epoch 53 Batch 158: Loss 1.3558201789855957
Model None Epoch 53 Batch 159: Loss 1.3207571506500244
Model None Epoch 53 Batch 160: Loss 1.3332605361938477
Model None Epoch 53 Batch 161: Loss 1.3190925121307373
Model None Epoch 53 Batch 162: Loss 1.3451662063598633
Model None Epoch 53 Batch 163: Loss 1.320580005645752
Model None Epoch 53 Batch 164: Loss 1.392561674118042
Model None Epoch 53 Batch 165: Loss 1.3808947801589966
Model None Epoch 53 Batch 166: Loss 1.4022356271743774
Model None Epoch 53 Batch 167: Loss 1.364424228668213
Model None Epoch 53 Batch 168: Loss 1.3876878023147583
Model None Epoch 53 Batch 169: Loss 1.390602946281433
Model None Epoch 53 Batch 170: Loss 1.3417062759399414
Model None Epoch 53 Batch 171: Loss 1.3530126810073853
Model None Epoch 53 Batch 172: Loss 1.306606411933899
Model None Epoch 53 Batch 173: Loss 1.4340429306030273
Model None Epoch 53 Batch 174: Loss 1.4924391508102417
Model None Epoch 53 Batch 175: Loss 1.4412341117858887
Model None Epoch 53 Batch 176: Loss 1.5051933526992798
Model None Epoch 53 Batch 177: Loss 1.348036766052246
Model None Epoch 53 Batch 178: Loss 1.2706563472747803
Model None Epoch 53 Batch 179: Loss 1.3376504182815552
Model None Epoch 53 Batch 180: Loss 1.3703458309173584
Model None Epoch 53 Batch 181: Loss 1.432186484336853
Model None Epoch 53 Batch 182: Loss 1.342766284942627
Model None Epoch 53 Batch 183: Loss 1.4067163467407227
Model None Epoch 53 Batch 184: Loss 1.5198677778244019
Model None Epoch 53 Batch 185: Loss 1.4620425701141357
Model None Epoch 53 Batch 186: Loss 1.2597750425338745
Model None Epoch 53 Batch 187: Loss 1.3615971803665161
Model None Epoch 53 Batch 188: Loss 1.420206069946289
Model None Epoch 53 Batch 189: Loss 1.275916337966919
Model None Epoch 53 Batch 190: Loss 1.3423599004745483
Model None Epoch 53 Batch 191: Loss 1.4049816131591797
Model None Epoch 53 Batch 192: Loss 1.3425421714782715
Model None Epoch 53 Batch 193: Loss 1.429140329360962
Model None Epoch 53 Batch 194: Loss 1.3441038131713867
Model None Epoch 53 Batch 195: Loss 1.2353670597076416

 Downstream Train loss: 1.3992275163835408 Acc: 0.5655
Downstream Train Epoch: 54 [0/50000 (0%)]	Loss: 1.379529
Model None Epoch 54 Batch 0: Loss 1.379528522491455
Model None Epoch 54 Batch 1: Loss 1.3800439834594727
Model None Epoch 54 Batch 2: Loss 1.3912880420684814
Model None Epoch 54 Batch 3: Loss 1.4498385190963745
Model None Epoch 54 Batch 4: Loss 1.3911311626434326
Model None Epoch 54 Batch 5: Loss 1.4487833976745605
Model None Epoch 54 Batch 6: Loss 1.4334508180618286
Model None Epoch 54 Batch 7: Loss 1.4756159782409668
Model None Epoch 54 Batch 8: Loss 1.4465588331222534
Model None Epoch 54 Batch 9: Loss 1.3174887895584106
Model None Epoch 54 Batch 10: Loss 1.4540619850158691
Model None Epoch 54 Batch 11: Loss 1.426795482635498
Model None Epoch 54 Batch 12: Loss 1.3044251203536987
Model None Epoch 54 Batch 13: Loss 1.4671438932418823
Model None Epoch 54 Batch 14: Loss 1.3854066133499146
Model None Epoch 54 Batch 15: Loss 1.3520444631576538
Model None Epoch 54 Batch 16: Loss 1.4720968008041382
Model None Epoch 54 Batch 17: Loss 1.3939942121505737
Model None Epoch 54 Batch 18: Loss 1.485314130783081
Model None Epoch 54 Batch 19: Loss 1.3398131132125854
Model None Epoch 54 Batch 20: Loss 1.4284415245056152
Model None Epoch 54 Batch 21: Loss 1.4465044736862183
Model None Epoch 54 Batch 22: Loss 1.4899080991744995
Model None Epoch 54 Batch 23: Loss 1.3435022830963135
Model None Epoch 54 Batch 24: Loss 1.4579038619995117
Model None Epoch 54 Batch 25: Loss 1.5057322978973389
Model None Epoch 54 Batch 26: Loss 1.4348493814468384
Model None Epoch 54 Batch 27: Loss 1.462827205657959
Model None Epoch 54 Batch 28: Loss 1.3329929113388062
Model None Epoch 54 Batch 29: Loss 1.403954029083252
Model None Epoch 54 Batch 30: Loss 1.2855212688446045
Model None Epoch 54 Batch 31: Loss 1.366107702255249
Model None Epoch 54 Batch 32: Loss 1.455359935760498
Model None Epoch 54 Batch 33: Loss 1.4281525611877441
Model None Epoch 54 Batch 34: Loss 1.3633854389190674
Model None Epoch 54 Batch 35: Loss 1.323333501815796
Model None Epoch 54 Batch 36: Loss 1.5467522144317627
Model None Epoch 54 Batch 37: Loss 1.4129388332366943
Model None Epoch 54 Batch 38: Loss 1.3507730960845947
Model None Epoch 54 Batch 39: Loss 1.336666226387024
Model None Epoch 54 Batch 40: Loss 1.4512851238250732
Model None Epoch 54 Batch 41: Loss 1.4338281154632568
Model None Epoch 54 Batch 42: Loss 1.3214417695999146
Model None Epoch 54 Batch 43: Loss 1.4390015602111816
Model None Epoch 54 Batch 44: Loss 1.413535237312317
Model None Epoch 54 Batch 45: Loss 1.404449701309204
Model None Epoch 54 Batch 46: Loss 1.278701901435852
Model None Epoch 54 Batch 47: Loss 1.4053184986114502
Model None Epoch 54 Batch 48: Loss 1.368172287940979
Model None Epoch 54 Batch 49: Loss 1.415356993675232
Downstream Train Epoch: 54 [12800/50000 (26%)]	Loss: 1.341442
Model None Epoch 54 Batch 50: Loss 1.3414422273635864
Model None Epoch 54 Batch 51: Loss 1.4906443357467651
Model None Epoch 54 Batch 52: Loss 1.5249699354171753
Model None Epoch 54 Batch 53: Loss 1.4924118518829346
Model None Epoch 54 Batch 54: Loss 1.475645899772644
Model None Epoch 54 Batch 55: Loss 1.3739488124847412
Model None Epoch 54 Batch 56: Loss 1.4506077766418457
Model None Epoch 54 Batch 57: Loss 1.316804051399231
Model None Epoch 54 Batch 58: Loss 1.3222562074661255
Model None Epoch 54 Batch 59: Loss 1.3395065069198608
Model None Epoch 54 Batch 60: Loss 1.3644851446151733
Model None Epoch 54 Batch 61: Loss 1.4593567848205566
Model None Epoch 54 Batch 62: Loss 1.3645061254501343
Model None Epoch 54 Batch 63: Loss 1.437656044960022
Model None Epoch 54 Batch 64: Loss 1.3592525720596313
Model None Epoch 54 Batch 65: Loss 1.3007745742797852
Model None Epoch 54 Batch 66: Loss 1.3684630393981934
Model None Epoch 54 Batch 67: Loss 1.4195060729980469
Model None Epoch 54 Batch 68: Loss 1.3734631538391113
Model None Epoch 54 Batch 69: Loss 1.406516432762146
Model None Epoch 54 Batch 70: Loss 1.3775829076766968
Model None Epoch 54 Batch 71: Loss 1.325987458229065
Model None Epoch 54 Batch 72: Loss 1.3527376651763916
Model None Epoch 54 Batch 73: Loss 1.4421147108078003
Model None Epoch 54 Batch 74: Loss 1.3385534286499023
Model None Epoch 54 Batch 75: Loss 1.4287493228912354
Model None Epoch 54 Batch 76: Loss 1.3145116567611694
Model None Epoch 54 Batch 77: Loss 1.498486876487732
Model None Epoch 54 Batch 78: Loss 1.3673166036605835
Model None Epoch 54 Batch 79: Loss 1.4398490190505981
Model None Epoch 54 Batch 80: Loss 1.3891396522521973
Model None Epoch 54 Batch 81: Loss 1.5055769681930542
Model None Epoch 54 Batch 82: Loss 1.4182997941970825
Model None Epoch 54 Batch 83: Loss 1.462746500968933
Model None Epoch 54 Batch 84: Loss 1.4073456525802612
Model None Epoch 54 Batch 85: Loss 1.2825367450714111
Model None Epoch 54 Batch 86: Loss 1.4368396997451782
Model None Epoch 54 Batch 87: Loss 1.4371941089630127
Model None Epoch 54 Batch 88: Loss 1.3812823295593262
Model None Epoch 54 Batch 89: Loss 1.3486549854278564
Model None Epoch 54 Batch 90: Loss 1.2911285161972046
Model None Epoch 54 Batch 91: Loss 1.4459936618804932
Model None Epoch 54 Batch 92: Loss 1.2003682851791382
Model None Epoch 54 Batch 93: Loss 1.4886362552642822
Model None Epoch 54 Batch 94: Loss 1.4025992155075073
Model None Epoch 54 Batch 95: Loss 1.4448591470718384
Model None Epoch 54 Batch 96: Loss 1.2948002815246582
Model None Epoch 54 Batch 97: Loss 1.465507984161377
Model None Epoch 54 Batch 98: Loss 1.4136141538619995
Model None Epoch 54 Batch 99: Loss 1.4633886814117432
Downstream Train Epoch: 54 [25600/50000 (51%)]	Loss: 1.528387
Model None Epoch 54 Batch 100: Loss 1.5283869504928589
Model None Epoch 54 Batch 101: Loss 1.409633755683899
Model None Epoch 54 Batch 102: Loss 1.3894195556640625
Model None Epoch 54 Batch 103: Loss 1.4396886825561523
Model None Epoch 54 Batch 104: Loss 1.4901812076568604
Model None Epoch 54 Batch 105: Loss 1.3920429944992065
Model None Epoch 54 Batch 106: Loss 1.356852650642395
Model None Epoch 54 Batch 107: Loss 1.4384849071502686
Model None Epoch 54 Batch 108: Loss 1.4896233081817627
Model None Epoch 54 Batch 109: Loss 1.3679875135421753
Model None Epoch 54 Batch 110: Loss 1.367079496383667
Model None Epoch 54 Batch 111: Loss 1.304721474647522
Model None Epoch 54 Batch 112: Loss 1.4510074853897095
Model None Epoch 54 Batch 113: Loss 1.3798638582229614
Model None Epoch 54 Batch 114: Loss 1.3129554986953735
Model None Epoch 54 Batch 115: Loss 1.5273432731628418
Model None Epoch 54 Batch 116: Loss 1.396713137626648
Model None Epoch 54 Batch 117: Loss 1.3531982898712158
Model None Epoch 54 Batch 118: Loss 1.3839439153671265
Model None Epoch 54 Batch 119: Loss 1.3119856119155884
Model None Epoch 54 Batch 120: Loss 1.358197569847107
Model None Epoch 54 Batch 121: Loss 1.369746208190918
Model None Epoch 54 Batch 122: Loss 1.3955926895141602
Model None Epoch 54 Batch 123: Loss 1.4017404317855835
Model None Epoch 54 Batch 124: Loss 1.3955296277999878
Model None Epoch 54 Batch 125: Loss 1.4195255041122437
Model None Epoch 54 Batch 126: Loss 1.4371564388275146
Model None Epoch 54 Batch 127: Loss 1.3566420078277588
Model None Epoch 54 Batch 128: Loss 1.3376586437225342
Model None Epoch 54 Batch 129: Loss 1.4045029878616333
Model None Epoch 54 Batch 130: Loss 1.3798534870147705
Model None Epoch 54 Batch 131: Loss 1.4462496042251587
Model None Epoch 54 Batch 132: Loss 1.425711989402771
Model None Epoch 54 Batch 133: Loss 1.3651148080825806
Model None Epoch 54 Batch 134: Loss 1.4446895122528076
Model None Epoch 54 Batch 135: Loss 1.4130334854125977
Model None Epoch 54 Batch 136: Loss 1.3484010696411133
Model None Epoch 54 Batch 137: Loss 1.3924357891082764
Model None Epoch 54 Batch 138: Loss 1.445454478263855
Model None Epoch 54 Batch 139: Loss 1.380790114402771
Model None Epoch 54 Batch 140: Loss 1.3363503217697144
Model None Epoch 54 Batch 141: Loss 1.4219926595687866
Model None Epoch 54 Batch 142: Loss 1.385308027267456
Model None Epoch 54 Batch 143: Loss 1.4769465923309326
Model None Epoch 54 Batch 144: Loss 1.4463117122650146
Model None Epoch 54 Batch 145: Loss 1.4004144668579102
Model None Epoch 54 Batch 146: Loss 1.3935621976852417
Model None Epoch 54 Batch 147: Loss 1.4386951923370361
Model None Epoch 54 Batch 148: Loss 1.373669981956482
Model None Epoch 54 Batch 149: Loss 1.439346432685852
Downstream Train Epoch: 54 [38400/50000 (77%)]	Loss: 1.473655
Model None Epoch 54 Batch 150: Loss 1.473655104637146
Model None Epoch 54 Batch 151: Loss 1.2985458374023438
Model None Epoch 54 Batch 152: Loss 1.3812644481658936
Model None Epoch 54 Batch 153: Loss 1.3678114414215088
Model None Epoch 54 Batch 154: Loss 1.4639772176742554
Model None Epoch 54 Batch 155: Loss 1.4168627262115479
Model None Epoch 54 Batch 156: Loss 1.3630460500717163
Model None Epoch 54 Batch 157: Loss 1.3869397640228271
Model None Epoch 54 Batch 158: Loss 1.3598926067352295
Model None Epoch 54 Batch 159: Loss 1.4217027425765991
Model None Epoch 54 Batch 160: Loss 1.4283710718154907
Model None Epoch 54 Batch 161: Loss 1.4759972095489502
Model None Epoch 54 Batch 162: Loss 1.3735514879226685
Model None Epoch 54 Batch 163: Loss 1.3466638326644897
Model None Epoch 54 Batch 164: Loss 1.4380066394805908
Model None Epoch 54 Batch 165: Loss 1.3586970567703247
Model None Epoch 54 Batch 166: Loss 1.405597448348999
Model None Epoch 54 Batch 167: Loss 1.3281341791152954
Model None Epoch 54 Batch 168: Loss 1.5193203687667847
Model None Epoch 54 Batch 169: Loss 1.4544594287872314
Model None Epoch 54 Batch 170: Loss 1.5083355903625488
Model None Epoch 54 Batch 171: Loss 1.349339485168457
Model None Epoch 54 Batch 172: Loss 1.4580632448196411
Model None Epoch 54 Batch 173: Loss 1.409340500831604
Model None Epoch 54 Batch 174: Loss 1.468220829963684
Model None Epoch 54 Batch 175: Loss 1.488385796546936
Model None Epoch 54 Batch 176: Loss 1.4162757396697998
Model None Epoch 54 Batch 177: Loss 1.3912566900253296
Model None Epoch 54 Batch 178: Loss 1.4128353595733643
Model None Epoch 54 Batch 179: Loss 1.4937925338745117
Model None Epoch 54 Batch 180: Loss 1.3775101900100708
Model None Epoch 54 Batch 181: Loss 1.3769587278366089
Model None Epoch 54 Batch 182: Loss 1.3029468059539795
Model None Epoch 54 Batch 183: Loss 1.3813447952270508
Model None Epoch 54 Batch 184: Loss 1.3787250518798828
Model None Epoch 54 Batch 185: Loss 1.3015649318695068
Model None Epoch 54 Batch 186: Loss 1.3751416206359863
Model None Epoch 54 Batch 187: Loss 1.368876338005066
Model None Epoch 54 Batch 188: Loss 1.54335355758667
Model None Epoch 54 Batch 189: Loss 1.3548312187194824
Model None Epoch 54 Batch 190: Loss 1.4538227319717407
Model None Epoch 54 Batch 191: Loss 1.3054375648498535
Model None Epoch 54 Batch 192: Loss 1.3585456609725952
Model None Epoch 54 Batch 193: Loss 1.3337178230285645
Model None Epoch 54 Batch 194: Loss 1.53263258934021
Model None Epoch 54 Batch 195: Loss 1.5302562713623047

 Downstream Train loss: 1.4030912634061308 Acc: 0.5655
Downstream Train Epoch: 55 [0/50000 (0%)]	Loss: 1.429399
Model None Epoch 55 Batch 0: Loss 1.4293991327285767
Model None Epoch 55 Batch 1: Loss 1.4333522319793701
Model None Epoch 55 Batch 2: Loss 1.2390954494476318
Model None Epoch 55 Batch 3: Loss 1.406142234802246
Model None Epoch 55 Batch 4: Loss 1.3543046712875366
Model None Epoch 55 Batch 5: Loss 1.2789864540100098
Model None Epoch 55 Batch 6: Loss 1.4320321083068848
Model None Epoch 55 Batch 7: Loss 1.4081907272338867
Model None Epoch 55 Batch 8: Loss 1.4554924964904785
Model None Epoch 55 Batch 9: Loss 1.4022036790847778
Model None Epoch 55 Batch 10: Loss 1.3597208261489868
Model None Epoch 55 Batch 11: Loss 1.4449503421783447
Model None Epoch 55 Batch 12: Loss 1.3874591588974
Model None Epoch 55 Batch 13: Loss 1.3951072692871094
Model None Epoch 55 Batch 14: Loss 1.2969694137573242
Model None Epoch 55 Batch 15: Loss 1.3189280033111572
Model None Epoch 55 Batch 16: Loss 1.4565956592559814
Model None Epoch 55 Batch 17: Loss 1.311865210533142
Model None Epoch 55 Batch 18: Loss 1.4612761735916138
Model None Epoch 55 Batch 19: Loss 1.221794843673706
Model None Epoch 55 Batch 20: Loss 1.4419232606887817
Model None Epoch 55 Batch 21: Loss 1.4392353296279907
Model None Epoch 55 Batch 22: Loss 1.42764151096344
Model None Epoch 55 Batch 23: Loss 1.3595503568649292
Model None Epoch 55 Batch 24: Loss 1.3125766515731812
Model None Epoch 55 Batch 25: Loss 1.4693881273269653
Model None Epoch 55 Batch 26: Loss 1.434390902519226
Model None Epoch 55 Batch 27: Loss 1.4036744832992554
Model None Epoch 55 Batch 28: Loss 1.4776825904846191
Model None Epoch 55 Batch 29: Loss 1.3822267055511475
Model None Epoch 55 Batch 30: Loss 1.3521610498428345
Model None Epoch 55 Batch 31: Loss 1.403964638710022
Model None Epoch 55 Batch 32: Loss 1.4413155317306519
Model None Epoch 55 Batch 33: Loss 1.3884559869766235
Model None Epoch 55 Batch 34: Loss 1.5148861408233643
Model None Epoch 55 Batch 35: Loss 1.3042222261428833
Model None Epoch 55 Batch 36: Loss 1.5338941812515259
Model None Epoch 55 Batch 37: Loss 1.3892855644226074
Model None Epoch 55 Batch 38: Loss 1.3501002788543701
Model None Epoch 55 Batch 39: Loss 1.4407427310943604
Model None Epoch 55 Batch 40: Loss 1.4191522598266602
Model None Epoch 55 Batch 41: Loss 1.3651295900344849
Model None Epoch 55 Batch 42: Loss 1.4049867391586304
Model None Epoch 55 Batch 43: Loss 1.460036277770996
Model None Epoch 55 Batch 44: Loss 1.3555759191513062
Model None Epoch 55 Batch 45: Loss 1.2827298641204834
Model None Epoch 55 Batch 46: Loss 1.4356253147125244
Model None Epoch 55 Batch 47: Loss 1.3009170293807983
Model None Epoch 55 Batch 48: Loss 1.5784012079238892
Model None Epoch 55 Batch 49: Loss 1.4821207523345947
Downstream Train Epoch: 55 [12800/50000 (26%)]	Loss: 1.415612
Model None Epoch 55 Batch 50: Loss 1.4156118631362915
Model None Epoch 55 Batch 51: Loss 1.2060960531234741
Model None Epoch 55 Batch 52: Loss 1.3704267740249634
Model None Epoch 55 Batch 53: Loss 1.4790374040603638
Model None Epoch 55 Batch 54: Loss 1.323215126991272
Model None Epoch 55 Batch 55: Loss 1.528497338294983
Model None Epoch 55 Batch 56: Loss 1.3173717260360718
Model None Epoch 55 Batch 57: Loss 1.3702574968338013
Model None Epoch 55 Batch 58: Loss 1.495982050895691
Model None Epoch 55 Batch 59: Loss 1.3915448188781738
Model None Epoch 55 Batch 60: Loss 1.5168782472610474
Model None Epoch 55 Batch 61: Loss 1.5461020469665527
Model None Epoch 55 Batch 62: Loss 1.5004899501800537
Model None Epoch 55 Batch 63: Loss 1.41395103931427
Model None Epoch 55 Batch 64: Loss 1.4414410591125488
Model None Epoch 55 Batch 65: Loss 1.3962904214859009
Model None Epoch 55 Batch 66: Loss 1.3506091833114624
Model None Epoch 55 Batch 67: Loss 1.2829415798187256
Model None Epoch 55 Batch 68: Loss 1.3629190921783447
Model None Epoch 55 Batch 69: Loss 1.3879128694534302
Model None Epoch 55 Batch 70: Loss 1.3050360679626465
Model None Epoch 55 Batch 71: Loss 1.453702449798584
Model None Epoch 55 Batch 72: Loss 1.3962805271148682
Model None Epoch 55 Batch 73: Loss 1.4139899015426636
Model None Epoch 55 Batch 74: Loss 1.3960829973220825
Model None Epoch 55 Batch 75: Loss 1.3532088994979858
Model None Epoch 55 Batch 76: Loss 1.422348141670227
Model None Epoch 55 Batch 77: Loss 1.4555819034576416
Model None Epoch 55 Batch 78: Loss 1.4187523126602173
Model None Epoch 55 Batch 79: Loss 1.3323060274124146
Model None Epoch 55 Batch 80: Loss 1.5170559883117676
Model None Epoch 55 Batch 81: Loss 1.3081640005111694
Model None Epoch 55 Batch 82: Loss 1.3912230730056763
Model None Epoch 55 Batch 83: Loss 1.3786656856536865
Model None Epoch 55 Batch 84: Loss 1.342108964920044
Model None Epoch 55 Batch 85: Loss 1.386379361152649
Model None Epoch 55 Batch 86: Loss 1.3287533521652222
Model None Epoch 55 Batch 87: Loss 1.3440048694610596
Model None Epoch 55 Batch 88: Loss 1.3590104579925537
Model None Epoch 55 Batch 89: Loss 1.4997804164886475
Model None Epoch 55 Batch 90: Loss 1.4041917324066162
Model None Epoch 55 Batch 91: Loss 1.4876484870910645
Model None Epoch 55 Batch 92: Loss 1.3007014989852905
Model None Epoch 55 Batch 93: Loss 1.4353841543197632
Model None Epoch 55 Batch 94: Loss 1.360981822013855
Model None Epoch 55 Batch 95: Loss 1.4338558912277222
Model None Epoch 55 Batch 96: Loss 1.4715845584869385
Model None Epoch 55 Batch 97: Loss 1.4131797552108765
Model None Epoch 55 Batch 98: Loss 1.3900567293167114
Model None Epoch 55 Batch 99: Loss 1.4398658275604248
Downstream Train Epoch: 55 [25600/50000 (51%)]	Loss: 1.349250
Model None Epoch 55 Batch 100: Loss 1.3492498397827148
Model None Epoch 55 Batch 101: Loss 1.4579371213912964
Model None Epoch 55 Batch 102: Loss 1.3905194997787476
Model None Epoch 55 Batch 103: Loss 1.4016215801239014
Model None Epoch 55 Batch 104: Loss 1.40586519241333
Model None Epoch 55 Batch 105: Loss 1.4510313272476196
Model None Epoch 55 Batch 106: Loss 1.2509459257125854
Model None Epoch 55 Batch 107: Loss 1.5009405612945557
Model None Epoch 55 Batch 108: Loss 1.4123644828796387
Model None Epoch 55 Batch 109: Loss 1.3695175647735596
Model None Epoch 55 Batch 110: Loss 1.4664249420166016
Model None Epoch 55 Batch 111: Loss 1.4298473596572876
Model None Epoch 55 Batch 112: Loss 1.3004299402236938
Model None Epoch 55 Batch 113: Loss 1.5638704299926758
Model None Epoch 55 Batch 114: Loss 1.3651660680770874
Model None Epoch 55 Batch 115: Loss 1.4326292276382446
Model None Epoch 55 Batch 116: Loss 1.1818183660507202
Model None Epoch 55 Batch 117: Loss 1.3529772758483887
Model None Epoch 55 Batch 118: Loss 1.3630906343460083
Model None Epoch 55 Batch 119: Loss 1.4632233381271362
Model None Epoch 55 Batch 120: Loss 1.4393717050552368
Model None Epoch 55 Batch 121: Loss 1.4685128927230835
Model None Epoch 55 Batch 122: Loss 1.3039933443069458
Model None Epoch 55 Batch 123: Loss 1.3006017208099365
Model None Epoch 55 Batch 124: Loss 1.3793026208877563
Model None Epoch 55 Batch 125: Loss 1.3027538061141968
Model None Epoch 55 Batch 126: Loss 1.3547465801239014
Model None Epoch 55 Batch 127: Loss 1.3768543004989624
Model None Epoch 55 Batch 128: Loss 1.3992209434509277
Model None Epoch 55 Batch 129: Loss 1.4385673999786377
Model None Epoch 55 Batch 130: Loss 1.3700873851776123
Model None Epoch 55 Batch 131: Loss 1.308801293373108
Model None Epoch 55 Batch 132: Loss 1.344634771347046
Model None Epoch 55 Batch 133: Loss 1.3796528577804565
Model None Epoch 55 Batch 134: Loss 1.4104723930358887
Model None Epoch 55 Batch 135: Loss 1.4230903387069702
Model None Epoch 55 Batch 136: Loss 1.2878835201263428
Model None Epoch 55 Batch 137: Loss 1.543533205986023
Model None Epoch 55 Batch 138: Loss 1.5296674966812134
Model None Epoch 55 Batch 139: Loss 1.4444977045059204
Model None Epoch 55 Batch 140: Loss 1.414022445678711
Model None Epoch 55 Batch 141: Loss 1.3806558847427368
Model None Epoch 55 Batch 142: Loss 1.4647059440612793
Model None Epoch 55 Batch 143: Loss 1.499484896659851
Model None Epoch 55 Batch 144: Loss 1.4794697761535645
Model None Epoch 55 Batch 145: Loss 1.4702630043029785
Model None Epoch 55 Batch 146: Loss 1.4101901054382324
Model None Epoch 55 Batch 147: Loss 1.3316174745559692
Model None Epoch 55 Batch 148: Loss 1.4400724172592163
Model None Epoch 55 Batch 149: Loss 1.4587016105651855
Downstream Train Epoch: 55 [38400/50000 (77%)]	Loss: 1.437878
Model None Epoch 55 Batch 150: Loss 1.437878131866455
Model None Epoch 55 Batch 151: Loss 1.495194673538208
Model None Epoch 55 Batch 152: Loss 1.5104752779006958
Model None Epoch 55 Batch 153: Loss 1.5068081617355347
Model None Epoch 55 Batch 154: Loss 1.4077527523040771
Model None Epoch 55 Batch 155: Loss 1.462884545326233
Model None Epoch 55 Batch 156: Loss 1.383712649345398
Model None Epoch 55 Batch 157: Loss 1.3502084016799927
Model None Epoch 55 Batch 158: Loss 1.3832366466522217
Model None Epoch 55 Batch 159: Loss 1.4633537530899048
Model None Epoch 55 Batch 160: Loss 1.4213151931762695
Model None Epoch 55 Batch 161: Loss 1.3324187994003296
Model None Epoch 55 Batch 162: Loss 1.3466238975524902
Model None Epoch 55 Batch 163: Loss 1.3234736919403076
Model None Epoch 55 Batch 164: Loss 1.304139256477356
Model None Epoch 55 Batch 165: Loss 1.3978098630905151
Model None Epoch 55 Batch 166: Loss 1.3582468032836914
Model None Epoch 55 Batch 167: Loss 1.438084602355957
Model None Epoch 55 Batch 168: Loss 1.4501991271972656
Model None Epoch 55 Batch 169: Loss 1.4659723043441772
Model None Epoch 55 Batch 170: Loss 1.283332347869873
Model None Epoch 55 Batch 171: Loss 1.337981939315796
Model None Epoch 55 Batch 172: Loss 1.3238657712936401
Model None Epoch 55 Batch 173: Loss 1.4104830026626587
Model None Epoch 55 Batch 174: Loss 1.4749248027801514
Model None Epoch 55 Batch 175: Loss 1.4307327270507812
Model None Epoch 55 Batch 176: Loss 1.413441777229309
Model None Epoch 55 Batch 177: Loss 1.3449519872665405
Model None Epoch 55 Batch 178: Loss 1.4917093515396118
Model None Epoch 55 Batch 179: Loss 1.4151890277862549
Model None Epoch 55 Batch 180: Loss 1.3086129426956177
Model None Epoch 55 Batch 181: Loss 1.328797698020935
Model None Epoch 55 Batch 182: Loss 1.4650870561599731
Model None Epoch 55 Batch 183: Loss 1.4085571765899658
Model None Epoch 55 Batch 184: Loss 1.4650001525878906
Model None Epoch 55 Batch 185: Loss 1.4458328485488892
Model None Epoch 55 Batch 186: Loss 1.3804473876953125
Model None Epoch 55 Batch 187: Loss 1.3914507627487183
Model None Epoch 55 Batch 188: Loss 1.3949342966079712
Model None Epoch 55 Batch 189: Loss 1.2605297565460205
Model None Epoch 55 Batch 190: Loss 1.4160622358322144
Model None Epoch 55 Batch 191: Loss 1.425713062286377
Model None Epoch 55 Batch 192: Loss 1.2910892963409424
Model None Epoch 55 Batch 193: Loss 1.3613605499267578
Model None Epoch 55 Batch 194: Loss 1.4381556510925293
Model None Epoch 55 Batch 195: Loss 1.3113073110580444

 Downstream Train loss: 1.3990181696658233 Acc: 0.5655
Downstream Train Epoch: 56 [0/50000 (0%)]	Loss: 1.398866
Model None Epoch 56 Batch 0: Loss 1.3988655805587769
Model None Epoch 56 Batch 1: Loss 1.4996743202209473
Model None Epoch 56 Batch 2: Loss 1.3995975255966187
Model None Epoch 56 Batch 3: Loss 1.4742408990859985
Model None Epoch 56 Batch 4: Loss 1.372504711151123
Model None Epoch 56 Batch 5: Loss 1.4168903827667236
Model None Epoch 56 Batch 6: Loss 1.4531396627426147
Model None Epoch 56 Batch 7: Loss 1.4206749200820923
Model None Epoch 56 Batch 8: Loss 1.355187177658081
Model None Epoch 56 Batch 9: Loss 1.3231240510940552
Model None Epoch 56 Batch 10: Loss 1.4811210632324219
Model None Epoch 56 Batch 11: Loss 1.4241633415222168
Model None Epoch 56 Batch 12: Loss 1.3946751356124878
Model None Epoch 56 Batch 13: Loss 1.490514874458313
Model None Epoch 56 Batch 14: Loss 1.4526406526565552
Model None Epoch 56 Batch 15: Loss 1.4814188480377197
Model None Epoch 56 Batch 16: Loss 1.3889191150665283
Model None Epoch 56 Batch 17: Loss 1.4179511070251465
Model None Epoch 56 Batch 18: Loss 1.3567200899124146
Model None Epoch 56 Batch 19: Loss 1.396920919418335
Model None Epoch 56 Batch 20: Loss 1.3997763395309448
Model None Epoch 56 Batch 21: Loss 1.2955117225646973
Model None Epoch 56 Batch 22: Loss 1.3137620687484741
Model None Epoch 56 Batch 23: Loss 1.37721848487854
Model None Epoch 56 Batch 24: Loss 1.308516502380371
Model None Epoch 56 Batch 25: Loss 1.3387646675109863
Model None Epoch 56 Batch 26: Loss 1.3581204414367676
Model None Epoch 56 Batch 27: Loss 1.4315714836120605
Model None Epoch 56 Batch 28: Loss 1.3774359226226807
Model None Epoch 56 Batch 29: Loss 1.374934196472168
Model None Epoch 56 Batch 30: Loss 1.4005404710769653
Model None Epoch 56 Batch 31: Loss 1.2956881523132324
Model None Epoch 56 Batch 32: Loss 1.3807038068771362
Model None Epoch 56 Batch 33: Loss 1.4256483316421509
Model None Epoch 56 Batch 34: Loss 1.3810112476348877
Model None Epoch 56 Batch 35: Loss 1.292027473449707
Model None Epoch 56 Batch 36: Loss 1.428046703338623
Model None Epoch 56 Batch 37: Loss 1.3394131660461426
Model None Epoch 56 Batch 38: Loss 1.3467962741851807
Model None Epoch 56 Batch 39: Loss 1.36771559715271
Model None Epoch 56 Batch 40: Loss 1.3497486114501953
Model None Epoch 56 Batch 41: Loss 1.3850148916244507
Model None Epoch 56 Batch 42: Loss 1.4465702772140503
Model None Epoch 56 Batch 43: Loss 1.4242011308670044
Model None Epoch 56 Batch 44: Loss 1.431241750717163
Model None Epoch 56 Batch 45: Loss 1.4280920028686523
Model None Epoch 56 Batch 46: Loss 1.2770843505859375
Model None Epoch 56 Batch 47: Loss 1.3317419290542603
Model None Epoch 56 Batch 48: Loss 1.3338209390640259
Model None Epoch 56 Batch 49: Loss 1.2674050331115723
Downstream Train Epoch: 56 [12800/50000 (26%)]	Loss: 1.554923
Model None Epoch 56 Batch 50: Loss 1.5549229383468628
Model None Epoch 56 Batch 51: Loss 1.444985032081604
Model None Epoch 56 Batch 52: Loss 1.5430692434310913
Model None Epoch 56 Batch 53: Loss 1.4450464248657227
Model None Epoch 56 Batch 54: Loss 1.3717544078826904
Model None Epoch 56 Batch 55: Loss 1.4477978944778442
Model None Epoch 56 Batch 56: Loss 1.3665918111801147
Model None Epoch 56 Batch 57: Loss 1.350965142250061
Model None Epoch 56 Batch 58: Loss 1.3619896173477173
Model None Epoch 56 Batch 59: Loss 1.3726173639297485
Model None Epoch 56 Batch 60: Loss 1.3391237258911133
Model None Epoch 56 Batch 61: Loss 1.4555459022521973
Model None Epoch 56 Batch 62: Loss 1.3367573022842407
Model None Epoch 56 Batch 63: Loss 1.4544533491134644
Model None Epoch 56 Batch 64: Loss 1.3933250904083252
Model None Epoch 56 Batch 65: Loss 1.454001545906067
Model None Epoch 56 Batch 66: Loss 1.3306092023849487
Model None Epoch 56 Batch 67: Loss 1.478208065032959
Model None Epoch 56 Batch 68: Loss 1.401003360748291
Model None Epoch 56 Batch 69: Loss 1.4715430736541748
Model None Epoch 56 Batch 70: Loss 1.4038732051849365
Model None Epoch 56 Batch 71: Loss 1.479073166847229
Model None Epoch 56 Batch 72: Loss 1.4058300256729126
Model None Epoch 56 Batch 73: Loss 1.4838316440582275
Model None Epoch 56 Batch 74: Loss 1.3843498229980469
Model None Epoch 56 Batch 75: Loss 1.508459210395813
Model None Epoch 56 Batch 76: Loss 1.458484172821045
Model None Epoch 56 Batch 77: Loss 1.3531020879745483
Model None Epoch 56 Batch 78: Loss 1.3574247360229492
Model None Epoch 56 Batch 79: Loss 1.4285250902175903
Model None Epoch 56 Batch 80: Loss 1.4055310487747192
Model None Epoch 56 Batch 81: Loss 1.4255369901657104
Model None Epoch 56 Batch 82: Loss 1.3715609312057495
Model None Epoch 56 Batch 83: Loss 1.4121668338775635
Model None Epoch 56 Batch 84: Loss 1.372098684310913
Model None Epoch 56 Batch 85: Loss 1.4493401050567627
Model None Epoch 56 Batch 86: Loss 1.362500548362732
Model None Epoch 56 Batch 87: Loss 1.3849018812179565
Model None Epoch 56 Batch 88: Loss 1.339951753616333
Model None Epoch 56 Batch 89: Loss 1.3990919589996338
Model None Epoch 56 Batch 90: Loss 1.4207048416137695
Model None Epoch 56 Batch 91: Loss 1.5550012588500977
Model None Epoch 56 Batch 92: Loss 1.2783015966415405
Model None Epoch 56 Batch 93: Loss 1.4488948583602905
Model None Epoch 56 Batch 94: Loss 1.3356739282608032
Model None Epoch 56 Batch 95: Loss 1.391339659690857
Model None Epoch 56 Batch 96: Loss 1.4023834466934204
Model None Epoch 56 Batch 97: Loss 1.4859166145324707
Model None Epoch 56 Batch 98: Loss 1.4028546810150146
Model None Epoch 56 Batch 99: Loss 1.3385566473007202
Downstream Train Epoch: 56 [25600/50000 (51%)]	Loss: 1.496395
Model None Epoch 56 Batch 100: Loss 1.4963951110839844
Model None Epoch 56 Batch 101: Loss 1.512452483177185
Model None Epoch 56 Batch 102: Loss 1.363116979598999
Model None Epoch 56 Batch 103: Loss 1.4878686666488647
Model None Epoch 56 Batch 104: Loss 1.3410965204238892
Model None Epoch 56 Batch 105: Loss 1.4606783390045166
Model None Epoch 56 Batch 106: Loss 1.3639203310012817
Model None Epoch 56 Batch 107: Loss 1.4171258211135864
Model None Epoch 56 Batch 108: Loss 1.406683087348938
Model None Epoch 56 Batch 109: Loss 1.3428770303726196
Model None Epoch 56 Batch 110: Loss 1.325209140777588
Model None Epoch 56 Batch 111: Loss 1.4442005157470703
Model None Epoch 56 Batch 112: Loss 1.358880639076233
Model None Epoch 56 Batch 113: Loss 1.3873833417892456
Model None Epoch 56 Batch 114: Loss 1.4355592727661133
Model None Epoch 56 Batch 115: Loss 1.353174090385437
Model None Epoch 56 Batch 116: Loss 1.342444896697998
Model None Epoch 56 Batch 117: Loss 1.3689908981323242
Model None Epoch 56 Batch 118: Loss 1.2898098230361938
Model None Epoch 56 Batch 119: Loss 1.3461191654205322
Model None Epoch 56 Batch 120: Loss 1.3693017959594727
Model None Epoch 56 Batch 121: Loss 1.4791771173477173
Model None Epoch 56 Batch 122: Loss 1.4084962606430054
Model None Epoch 56 Batch 123: Loss 1.3106646537780762
Model None Epoch 56 Batch 124: Loss 1.4434648752212524
Model None Epoch 56 Batch 125: Loss 1.4090733528137207
Model None Epoch 56 Batch 126: Loss 1.3836842775344849
Model None Epoch 56 Batch 127: Loss 1.2973958253860474
Model None Epoch 56 Batch 128: Loss 1.4720736742019653
Model None Epoch 56 Batch 129: Loss 1.4682271480560303
Model None Epoch 56 Batch 130: Loss 1.4811556339263916
Model None Epoch 56 Batch 131: Loss 1.4055999517440796
Model None Epoch 56 Batch 132: Loss 1.4044561386108398
Model None Epoch 56 Batch 133: Loss 1.3372360467910767
Model None Epoch 56 Batch 134: Loss 1.400937557220459
Model None Epoch 56 Batch 135: Loss 1.3821592330932617
Model None Epoch 56 Batch 136: Loss 1.3148128986358643
Model None Epoch 56 Batch 137: Loss 1.395688772201538
Model None Epoch 56 Batch 138: Loss 1.3504266738891602
Model None Epoch 56 Batch 139: Loss 1.3087345361709595
Model None Epoch 56 Batch 140: Loss 1.3430755138397217
Model None Epoch 56 Batch 141: Loss 1.4236783981323242
Model None Epoch 56 Batch 142: Loss 1.5040754079818726
Model None Epoch 56 Batch 143: Loss 1.400030493736267
Model None Epoch 56 Batch 144: Loss 1.4461686611175537
Model None Epoch 56 Batch 145: Loss 1.4225348234176636
Model None Epoch 56 Batch 146: Loss 1.4825990200042725
Model None Epoch 56 Batch 147: Loss 1.4460140466690063
Model None Epoch 56 Batch 148: Loss 1.426409363746643
Model None Epoch 56 Batch 149: Loss 1.4089182615280151
Downstream Train Epoch: 56 [38400/50000 (77%)]	Loss: 1.312587
Model None Epoch 56 Batch 150: Loss 1.3125866651535034
Model None Epoch 56 Batch 151: Loss 1.323211908340454
Model None Epoch 56 Batch 152: Loss 1.3260716199874878
Model None Epoch 56 Batch 153: Loss 1.3562325239181519
Model None Epoch 56 Batch 154: Loss 1.3904452323913574
Model None Epoch 56 Batch 155: Loss 1.3090100288391113
Model None Epoch 56 Batch 156: Loss 1.3676148653030396
Model None Epoch 56 Batch 157: Loss 1.4144083261489868
Model None Epoch 56 Batch 158: Loss 1.3119704723358154
Model None Epoch 56 Batch 159: Loss 1.423222303390503
Model None Epoch 56 Batch 160: Loss 1.4006774425506592
Model None Epoch 56 Batch 161: Loss 1.4576025009155273
Model None Epoch 56 Batch 162: Loss 1.4795976877212524
Model None Epoch 56 Batch 163: Loss 1.381822109222412
Model None Epoch 56 Batch 164: Loss 1.476211667060852
Model None Epoch 56 Batch 165: Loss 1.4555540084838867
Model None Epoch 56 Batch 166: Loss 1.348158836364746
Model None Epoch 56 Batch 167: Loss 1.4402506351470947
Model None Epoch 56 Batch 168: Loss 1.4279907941818237
Model None Epoch 56 Batch 169: Loss 1.4461458921432495
Model None Epoch 56 Batch 170: Loss 1.4442652463912964
Model None Epoch 56 Batch 171: Loss 1.4243465662002563
Model None Epoch 56 Batch 172: Loss 1.335868000984192
Model None Epoch 56 Batch 173: Loss 1.4159694910049438
Model None Epoch 56 Batch 174: Loss 1.3210742473602295
Model None Epoch 56 Batch 175: Loss 1.4632436037063599
Model None Epoch 56 Batch 176: Loss 1.4226406812667847
Model None Epoch 56 Batch 177: Loss 1.4871658086776733
Model None Epoch 56 Batch 178: Loss 1.3174315690994263
Model None Epoch 56 Batch 179: Loss 1.4980536699295044
Model None Epoch 56 Batch 180: Loss 1.4923508167266846
Model None Epoch 56 Batch 181: Loss 1.4021341800689697
Model None Epoch 56 Batch 182: Loss 1.5155794620513916
Model None Epoch 56 Batch 183: Loss 1.3668339252471924
Model None Epoch 56 Batch 184: Loss 1.3312979936599731
Model None Epoch 56 Batch 185: Loss 1.3971425294876099
Model None Epoch 56 Batch 186: Loss 1.4203211069107056
Model None Epoch 56 Batch 187: Loss 1.4774101972579956
Model None Epoch 56 Batch 188: Loss 1.383809208869934
Model None Epoch 56 Batch 189: Loss 1.3984016180038452
Model None Epoch 56 Batch 190: Loss 1.3927472829818726
Model None Epoch 56 Batch 191: Loss 1.5218429565429688
Model None Epoch 56 Batch 192: Loss 1.4831384420394897
Model None Epoch 56 Batch 193: Loss 1.3659049272537231
Model None Epoch 56 Batch 194: Loss 1.488533616065979
Model None Epoch 56 Batch 195: Loss 1.3629485368728638

 Downstream Train loss: 1.4014088675683858 Acc: 0.5662
Downstream Train Epoch: 57 [0/50000 (0%)]	Loss: 1.384647
Model None Epoch 57 Batch 0: Loss 1.384647250175476
Model None Epoch 57 Batch 1: Loss 1.461157202720642
Model None Epoch 57 Batch 2: Loss 1.3836690187454224
Model None Epoch 57 Batch 3: Loss 1.5277422666549683
Model None Epoch 57 Batch 4: Loss 1.4653453826904297
Model None Epoch 57 Batch 5: Loss 1.5594509840011597
Model None Epoch 57 Batch 6: Loss 1.4487258195877075
Model None Epoch 57 Batch 7: Loss 1.2662618160247803
Model None Epoch 57 Batch 8: Loss 1.3629791736602783
Model None Epoch 57 Batch 9: Loss 1.320105791091919
Model None Epoch 57 Batch 10: Loss 1.5095709562301636
Model None Epoch 57 Batch 11: Loss 1.3584659099578857
Model None Epoch 57 Batch 12: Loss 1.4003632068634033
Model None Epoch 57 Batch 13: Loss 1.361759901046753
Model None Epoch 57 Batch 14: Loss 1.4814419746398926
Model None Epoch 57 Batch 15: Loss 1.3826913833618164
Model None Epoch 57 Batch 16: Loss 1.2792797088623047
Model None Epoch 57 Batch 17: Loss 1.4496514797210693
Model None Epoch 57 Batch 18: Loss 1.3171595335006714
Model None Epoch 57 Batch 19: Loss 1.45497465133667
Model None Epoch 57 Batch 20: Loss 1.3668651580810547
Model None Epoch 57 Batch 21: Loss 1.505841851234436
Model None Epoch 57 Batch 22: Loss 1.4233527183532715
Model None Epoch 57 Batch 23: Loss 1.4733061790466309
Model None Epoch 57 Batch 24: Loss 1.4952701330184937
Model None Epoch 57 Batch 25: Loss 1.3429440259933472
Model None Epoch 57 Batch 26: Loss 1.4360923767089844
Model None Epoch 57 Batch 27: Loss 1.319459080696106
Model None Epoch 57 Batch 28: Loss 1.3776193857192993
Model None Epoch 57 Batch 29: Loss 1.42425537109375
Model None Epoch 57 Batch 30: Loss 1.4468461275100708
Model None Epoch 57 Batch 31: Loss 1.3722666501998901
Model None Epoch 57 Batch 32: Loss 1.4569664001464844
Model None Epoch 57 Batch 33: Loss 1.4892061948776245
Model None Epoch 57 Batch 34: Loss 1.4332135915756226
Model None Epoch 57 Batch 35: Loss 1.2660876512527466
Model None Epoch 57 Batch 36: Loss 1.3939436674118042
Model None Epoch 57 Batch 37: Loss 1.4032832384109497
Model None Epoch 57 Batch 38: Loss 1.3872230052947998
Model None Epoch 57 Batch 39: Loss 1.3624588251113892
Model None Epoch 57 Batch 40: Loss 1.4482421875
Model None Epoch 57 Batch 41: Loss 1.4243725538253784
Model None Epoch 57 Batch 42: Loss 1.401924729347229
Model None Epoch 57 Batch 43: Loss 1.2927944660186768
Model None Epoch 57 Batch 44: Loss 1.3977506160736084
Model None Epoch 57 Batch 45: Loss 1.334208607673645
Model None Epoch 57 Batch 46: Loss 1.4120393991470337
Model None Epoch 57 Batch 47: Loss 1.3957672119140625
Model None Epoch 57 Batch 48: Loss 1.4026340246200562
Model None Epoch 57 Batch 49: Loss 1.41240394115448
Downstream Train Epoch: 57 [12800/50000 (26%)]	Loss: 1.420957
Model None Epoch 57 Batch 50: Loss 1.4209566116333008
Model None Epoch 57 Batch 51: Loss 1.3604449033737183
Model None Epoch 57 Batch 52: Loss 1.3840512037277222
Model None Epoch 57 Batch 53: Loss 1.3442171812057495
Model None Epoch 57 Batch 54: Loss 1.5559418201446533
Model None Epoch 57 Batch 55: Loss 1.4531927108764648
Model None Epoch 57 Batch 56: Loss 1.383483648300171
Model None Epoch 57 Batch 57: Loss 1.4474049806594849
Model None Epoch 57 Batch 58: Loss 1.429257869720459
Model None Epoch 57 Batch 59: Loss 1.304603934288025
Model None Epoch 57 Batch 60: Loss 1.3596588373184204
Model None Epoch 57 Batch 61: Loss 1.4332280158996582
Model None Epoch 57 Batch 62: Loss 1.3711568117141724
Model None Epoch 57 Batch 63: Loss 1.4100265502929688
Model None Epoch 57 Batch 64: Loss 1.3765543699264526
Model None Epoch 57 Batch 65: Loss 1.3327440023422241
Model None Epoch 57 Batch 66: Loss 1.3982388973236084
Model None Epoch 57 Batch 67: Loss 1.4438683986663818
Model None Epoch 57 Batch 68: Loss 1.4795609712600708
Model None Epoch 57 Batch 69: Loss 1.3881274461746216
Model None Epoch 57 Batch 70: Loss 1.44731867313385
Model None Epoch 57 Batch 71: Loss 1.3826440572738647
Model None Epoch 57 Batch 72: Loss 1.3685898780822754
Model None Epoch 57 Batch 73: Loss 1.2775053977966309
Model None Epoch 57 Batch 74: Loss 1.2175788879394531
Model None Epoch 57 Batch 75: Loss 1.383562445640564
Model None Epoch 57 Batch 76: Loss 1.390091061592102
Model None Epoch 57 Batch 77: Loss 1.2893280982971191
Model None Epoch 57 Batch 78: Loss 1.449083685874939
Model None Epoch 57 Batch 79: Loss 1.4905195236206055
Model None Epoch 57 Batch 80: Loss 1.4535105228424072
Model None Epoch 57 Batch 81: Loss 1.43594229221344
Model None Epoch 57 Batch 82: Loss 1.3174816370010376
Model None Epoch 57 Batch 83: Loss 1.40825617313385
Model None Epoch 57 Batch 84: Loss 1.4359021186828613
Model None Epoch 57 Batch 85: Loss 1.3325105905532837
Model None Epoch 57 Batch 86: Loss 1.3565713167190552
Model None Epoch 57 Batch 87: Loss 1.3777985572814941
Model None Epoch 57 Batch 88: Loss 1.4624022245407104
Model None Epoch 57 Batch 89: Loss 1.4795573949813843
Model None Epoch 57 Batch 90: Loss 1.4053531885147095
Model None Epoch 57 Batch 91: Loss 1.4157922267913818
Model None Epoch 57 Batch 92: Loss 1.4834740161895752
Model None Epoch 57 Batch 93: Loss 1.3852325677871704
Model None Epoch 57 Batch 94: Loss 1.4541946649551392
Model None Epoch 57 Batch 95: Loss 1.3748791217803955
Model None Epoch 57 Batch 96: Loss 1.2491110563278198
Model None Epoch 57 Batch 97: Loss 1.4741482734680176
Model None Epoch 57 Batch 98: Loss 1.3513314723968506
Model None Epoch 57 Batch 99: Loss 1.348355770111084
Downstream Train Epoch: 57 [25600/50000 (51%)]	Loss: 1.385700
Model None Epoch 57 Batch 100: Loss 1.385699987411499
Model None Epoch 57 Batch 101: Loss 1.390938401222229
Model None Epoch 57 Batch 102: Loss 1.3397879600524902
Model None Epoch 57 Batch 103: Loss 1.3350718021392822
Model None Epoch 57 Batch 104: Loss 1.3217476606369019
Model None Epoch 57 Batch 105: Loss 1.4294991493225098
Model None Epoch 57 Batch 106: Loss 1.3542448282241821
Model None Epoch 57 Batch 107: Loss 1.2723383903503418
Model None Epoch 57 Batch 108: Loss 1.3733057975769043
Model None Epoch 57 Batch 109: Loss 1.273165225982666
Model None Epoch 57 Batch 110: Loss 1.3979958295822144
Model None Epoch 57 Batch 111: Loss 1.480623483657837
Model None Epoch 57 Batch 112: Loss 1.301298975944519
Model None Epoch 57 Batch 113: Loss 1.256407380104065
Model None Epoch 57 Batch 114: Loss 1.530788540840149
Model None Epoch 57 Batch 115: Loss 1.5187616348266602
Model None Epoch 57 Batch 116: Loss 1.4388880729675293
Model None Epoch 57 Batch 117: Loss 1.3486143350601196
Model None Epoch 57 Batch 118: Loss 1.3905525207519531
Model None Epoch 57 Batch 119: Loss 1.4856348037719727
Model None Epoch 57 Batch 120: Loss 1.4518500566482544
Model None Epoch 57 Batch 121: Loss 1.3353222608566284
Model None Epoch 57 Batch 122: Loss 1.5067204236984253
Model None Epoch 57 Batch 123: Loss 1.3833398818969727
Model None Epoch 57 Batch 124: Loss 1.2801567316055298
Model None Epoch 57 Batch 125: Loss 1.3623433113098145
Model None Epoch 57 Batch 126: Loss 1.4092533588409424
Model None Epoch 57 Batch 127: Loss 1.2106375694274902
Model None Epoch 57 Batch 128: Loss 1.3886693716049194
Model None Epoch 57 Batch 129: Loss 1.4183505773544312
Model None Epoch 57 Batch 130: Loss 1.4983196258544922
Model None Epoch 57 Batch 131: Loss 1.4586411714553833
Model None Epoch 57 Batch 132: Loss 1.4032214879989624
Model None Epoch 57 Batch 133: Loss 1.3150911331176758
Model None Epoch 57 Batch 134: Loss 1.3507080078125
Model None Epoch 57 Batch 135: Loss 1.351264238357544
Model None Epoch 57 Batch 136: Loss 1.4093310832977295
Model None Epoch 57 Batch 137: Loss 1.315110683441162
Model None Epoch 57 Batch 138: Loss 1.5052005052566528
Model None Epoch 57 Batch 139: Loss 1.312747597694397
Model None Epoch 57 Batch 140: Loss 1.4686367511749268
Model None Epoch 57 Batch 141: Loss 1.4380983114242554
Model None Epoch 57 Batch 142: Loss 1.4247198104858398
Model None Epoch 57 Batch 143: Loss 1.3960657119750977
Model None Epoch 57 Batch 144: Loss 1.410523772239685
Model None Epoch 57 Batch 145: Loss 1.3018618822097778
Model None Epoch 57 Batch 146: Loss 1.5143132209777832
Model None Epoch 57 Batch 147: Loss 1.4992010593414307
Model None Epoch 57 Batch 148: Loss 1.4595041275024414
Model None Epoch 57 Batch 149: Loss 1.3384281396865845
Downstream Train Epoch: 57 [38400/50000 (77%)]	Loss: 1.272170
Model None Epoch 57 Batch 150: Loss 1.272169828414917
Model None Epoch 57 Batch 151: Loss 1.4994583129882812
Model None Epoch 57 Batch 152: Loss 1.4412355422973633
Model None Epoch 57 Batch 153: Loss 1.522984504699707
Model None Epoch 57 Batch 154: Loss 1.4197708368301392
Model None Epoch 57 Batch 155: Loss 1.4566621780395508
Model None Epoch 57 Batch 156: Loss 1.3170065879821777
Model None Epoch 57 Batch 157: Loss 1.3758114576339722
Model None Epoch 57 Batch 158: Loss 1.4419611692428589
Model None Epoch 57 Batch 159: Loss 1.3309522867202759
Model None Epoch 57 Batch 160: Loss 1.4455311298370361
Model None Epoch 57 Batch 161: Loss 1.3617063760757446
Model None Epoch 57 Batch 162: Loss 1.412253975868225
Model None Epoch 57 Batch 163: Loss 1.4164786338806152
Model None Epoch 57 Batch 164: Loss 1.382606029510498
Model None Epoch 57 Batch 165: Loss 1.4033665657043457
Model None Epoch 57 Batch 166: Loss 1.3618718385696411
Model None Epoch 57 Batch 167: Loss 1.5106160640716553
Model None Epoch 57 Batch 168: Loss 1.3281936645507812
Model None Epoch 57 Batch 169: Loss 1.3826173543930054
Model None Epoch 57 Batch 170: Loss 1.3414474725723267
Model None Epoch 57 Batch 171: Loss 1.4220353364944458
Model None Epoch 57 Batch 172: Loss 1.4538779258728027
Model None Epoch 57 Batch 173: Loss 1.3301060199737549
Model None Epoch 57 Batch 174: Loss 1.3292033672332764
Model None Epoch 57 Batch 175: Loss 1.3754854202270508
Model None Epoch 57 Batch 176: Loss 1.4128103256225586
Model None Epoch 57 Batch 177: Loss 1.3687481880187988
Model None Epoch 57 Batch 178: Loss 1.412247896194458
Model None Epoch 57 Batch 179: Loss 1.4189666509628296
Model None Epoch 57 Batch 180: Loss 1.3102761507034302
Model None Epoch 57 Batch 181: Loss 1.344528317451477
Model None Epoch 57 Batch 182: Loss 1.3627469539642334
Model None Epoch 57 Batch 183: Loss 1.46891188621521
Model None Epoch 57 Batch 184: Loss 1.4066972732543945
Model None Epoch 57 Batch 185: Loss 1.3463923931121826
Model None Epoch 57 Batch 186: Loss 1.344651699066162
Model None Epoch 57 Batch 187: Loss 1.4565250873565674
Model None Epoch 57 Batch 188: Loss 1.3986659049987793
Model None Epoch 57 Batch 189: Loss 1.323730707168579
Model None Epoch 57 Batch 190: Loss 1.430835485458374
Model None Epoch 57 Batch 191: Loss 1.454485297203064
Model None Epoch 57 Batch 192: Loss 1.3445627689361572
Model None Epoch 57 Batch 193: Loss 1.529858946800232
Model None Epoch 57 Batch 194: Loss 1.3900455236434937
Model None Epoch 57 Batch 195: Loss 1.4465620517730713

 Downstream Train loss: 1.3974463513919286 Acc: 0.5662
Downstream Train Epoch: 58 [0/50000 (0%)]	Loss: 1.379773
Model None Epoch 58 Batch 0: Loss 1.3797730207443237
Model None Epoch 58 Batch 1: Loss 1.2764735221862793
Model None Epoch 58 Batch 2: Loss 1.5519330501556396
Model None Epoch 58 Batch 3: Loss 1.3651608228683472
Model None Epoch 58 Batch 4: Loss 1.3271411657333374
Model None Epoch 58 Batch 5: Loss 1.3976154327392578
Model None Epoch 58 Batch 6: Loss 1.5561771392822266
Model None Epoch 58 Batch 7: Loss 1.3463592529296875
Model None Epoch 58 Batch 8: Loss 1.4562864303588867
Model None Epoch 58 Batch 9: Loss 1.3564070463180542
Model None Epoch 58 Batch 10: Loss 1.3743377923965454
Model None Epoch 58 Batch 11: Loss 1.3167625665664673
Model None Epoch 58 Batch 12: Loss 1.4086129665374756
Model None Epoch 58 Batch 13: Loss 1.307218074798584
Model None Epoch 58 Batch 14: Loss 1.4468914270401
Model None Epoch 58 Batch 15: Loss 1.470395565032959
Model None Epoch 58 Batch 16: Loss 1.4375673532485962
Model None Epoch 58 Batch 17: Loss 1.4741390943527222
Model None Epoch 58 Batch 18: Loss 1.4128248691558838
Model None Epoch 58 Batch 19: Loss 1.4081443548202515
Model None Epoch 58 Batch 20: Loss 1.3894623517990112
Model None Epoch 58 Batch 21: Loss 1.401171326637268
Model None Epoch 58 Batch 22: Loss 1.4064733982086182
Model None Epoch 58 Batch 23: Loss 1.419850468635559
Model None Epoch 58 Batch 24: Loss 1.3766412734985352
Model None Epoch 58 Batch 25: Loss 1.366761326789856
Model None Epoch 58 Batch 26: Loss 1.275012731552124
Model None Epoch 58 Batch 27: Loss 1.368148684501648
Model None Epoch 58 Batch 28: Loss 1.4265493154525757
Model None Epoch 58 Batch 29: Loss 1.4366562366485596
Model None Epoch 58 Batch 30: Loss 1.3788260221481323
Model None Epoch 58 Batch 31: Loss 1.3038712739944458
Model None Epoch 58 Batch 32: Loss 1.468330979347229
Model None Epoch 58 Batch 33: Loss 1.3895002603530884
Model None Epoch 58 Batch 34: Loss 1.5631794929504395
Model None Epoch 58 Batch 35: Loss 1.3505231142044067
Model None Epoch 58 Batch 36: Loss 1.3544912338256836
Model None Epoch 58 Batch 37: Loss 1.4454864263534546
Model None Epoch 58 Batch 38: Loss 1.3860142230987549
Model None Epoch 58 Batch 39: Loss 1.44380784034729
Model None Epoch 58 Batch 40: Loss 1.332763910293579
Model None Epoch 58 Batch 41: Loss 1.4415642023086548
Model None Epoch 58 Batch 42: Loss 1.5064382553100586
Model None Epoch 58 Batch 43: Loss 1.4637932777404785
Model None Epoch 58 Batch 44: Loss 1.3709635734558105
Model None Epoch 58 Batch 45: Loss 1.3734424114227295
Model None Epoch 58 Batch 46: Loss 1.3891332149505615
Model None Epoch 58 Batch 47: Loss 1.518002986907959
Model None Epoch 58 Batch 48: Loss 1.38236665725708
Model None Epoch 58 Batch 49: Loss 1.4050943851470947
Downstream Train Epoch: 58 [12800/50000 (26%)]	Loss: 1.347431
Model None Epoch 58 Batch 50: Loss 1.3474313020706177
Model None Epoch 58 Batch 51: Loss 1.3450090885162354
Model None Epoch 58 Batch 52: Loss 1.489553689956665
Model None Epoch 58 Batch 53: Loss 1.4846643209457397
Model None Epoch 58 Batch 54: Loss 1.3753341436386108
Model None Epoch 58 Batch 55: Loss 1.4347972869873047
Model None Epoch 58 Batch 56: Loss 1.4231942892074585
Model None Epoch 58 Batch 57: Loss 1.3922830820083618
Model None Epoch 58 Batch 58: Loss 1.366057276725769
Model None Epoch 58 Batch 59: Loss 1.4638466835021973
Model None Epoch 58 Batch 60: Loss 1.3535131216049194
Model None Epoch 58 Batch 61: Loss 1.3768333196640015
Model None Epoch 58 Batch 62: Loss 1.351853847503662
Model None Epoch 58 Batch 63: Loss 1.556046485900879
Model None Epoch 58 Batch 64: Loss 1.3980790376663208
Model None Epoch 58 Batch 65: Loss 1.3513178825378418
Model None Epoch 58 Batch 66: Loss 1.4831557273864746
Model None Epoch 58 Batch 67: Loss 1.4810813665390015
Model None Epoch 58 Batch 68: Loss 1.4797881841659546
Model None Epoch 58 Batch 69: Loss 1.3757648468017578
Model None Epoch 58 Batch 70: Loss 1.4633830785751343
Model None Epoch 58 Batch 71: Loss 1.376891016960144
Model None Epoch 58 Batch 72: Loss 1.3683866262435913
Model None Epoch 58 Batch 73: Loss 1.297521710395813
Model None Epoch 58 Batch 74: Loss 1.3695034980773926
Model None Epoch 58 Batch 75: Loss 1.3492087125778198
Model None Epoch 58 Batch 76: Loss 1.448954701423645
Model None Epoch 58 Batch 77: Loss 1.4077409505844116
Model None Epoch 58 Batch 78: Loss 1.5018635988235474
Model None Epoch 58 Batch 79: Loss 1.4425532817840576
Model None Epoch 58 Batch 80: Loss 1.36750328540802
Model None Epoch 58 Batch 81: Loss 1.3808733224868774
Model None Epoch 58 Batch 82: Loss 1.3384051322937012
Model None Epoch 58 Batch 83: Loss 1.3365743160247803
Model None Epoch 58 Batch 84: Loss 1.3614192008972168
Model None Epoch 58 Batch 85: Loss 1.5000439882278442
Model None Epoch 58 Batch 86: Loss 1.3140640258789062
Model None Epoch 58 Batch 87: Loss 1.4114186763763428
Model None Epoch 58 Batch 88: Loss 1.4507980346679688
Model None Epoch 58 Batch 89: Loss 1.4728559255599976
Model None Epoch 58 Batch 90: Loss 1.3154966831207275
Model None Epoch 58 Batch 91: Loss 1.389646291732788
Model None Epoch 58 Batch 92: Loss 1.389902114868164
Model None Epoch 58 Batch 93: Loss 1.3335741758346558
Model None Epoch 58 Batch 94: Loss 1.3157827854156494
Model None Epoch 58 Batch 95: Loss 1.3398724794387817
Model None Epoch 58 Batch 96: Loss 1.4220812320709229
Model None Epoch 58 Batch 97: Loss 1.3438465595245361
Model None Epoch 58 Batch 98: Loss 1.3706110715866089
Model None Epoch 58 Batch 99: Loss 1.3451128005981445
Downstream Train Epoch: 58 [25600/50000 (51%)]	Loss: 1.407849
Model None Epoch 58 Batch 100: Loss 1.4078487157821655
Model None Epoch 58 Batch 101: Loss 1.3431499004364014
Model None Epoch 58 Batch 102: Loss 1.3400417566299438
Model None Epoch 58 Batch 103: Loss 1.5006932020187378
Model None Epoch 58 Batch 104: Loss 1.3277415037155151
Model None Epoch 58 Batch 105: Loss 1.4318186044692993
Model None Epoch 58 Batch 106: Loss 1.3383979797363281
Model None Epoch 58 Batch 107: Loss 1.4348112344741821
Model None Epoch 58 Batch 108: Loss 1.465919852256775
Model None Epoch 58 Batch 109: Loss 1.4099756479263306
Model None Epoch 58 Batch 110: Loss 1.4850776195526123
Model None Epoch 58 Batch 111: Loss 1.4178680181503296
Model None Epoch 58 Batch 112: Loss 1.3503170013427734
Model None Epoch 58 Batch 113: Loss 1.3668315410614014
Model None Epoch 58 Batch 114: Loss 1.35160231590271
Model None Epoch 58 Batch 115: Loss 1.370874047279358
Model None Epoch 58 Batch 116: Loss 1.4036201238632202
Model None Epoch 58 Batch 117: Loss 1.4899420738220215
Model None Epoch 58 Batch 118: Loss 1.5334231853485107
Model None Epoch 58 Batch 119: Loss 1.3278403282165527
Model None Epoch 58 Batch 120: Loss 1.4481476545333862
Model None Epoch 58 Batch 121: Loss 1.2711349725723267
Model None Epoch 58 Batch 122: Loss 1.454218864440918
Model None Epoch 58 Batch 123: Loss 1.4010334014892578
Model None Epoch 58 Batch 124: Loss 1.3670824766159058
Model None Epoch 58 Batch 125: Loss 1.4733264446258545
Model None Epoch 58 Batch 126: Loss 1.424615740776062
Model None Epoch 58 Batch 127: Loss 1.4256939888000488
Model None Epoch 58 Batch 128: Loss 1.273149013519287
Model None Epoch 58 Batch 129: Loss 1.288514494895935
Model None Epoch 58 Batch 130: Loss 1.3809642791748047
Model None Epoch 58 Batch 131: Loss 1.3921219110488892
Model None Epoch 58 Batch 132: Loss 1.3074954748153687
Model None Epoch 58 Batch 133: Loss 1.4377319812774658
Model None Epoch 58 Batch 134: Loss 1.3410987854003906
Model None Epoch 58 Batch 135: Loss 1.3227205276489258
Model None Epoch 58 Batch 136: Loss 1.4353444576263428
Model None Epoch 58 Batch 137: Loss 1.3655328750610352
Model None Epoch 58 Batch 138: Loss 1.3071315288543701
Model None Epoch 58 Batch 139: Loss 1.4012178182601929
Model None Epoch 58 Batch 140: Loss 1.3852752447128296
Model None Epoch 58 Batch 141: Loss 1.3483612537384033
Model None Epoch 58 Batch 142: Loss 1.4648544788360596
Model None Epoch 58 Batch 143: Loss 1.4724552631378174
Model None Epoch 58 Batch 144: Loss 1.3512791395187378
Model None Epoch 58 Batch 145: Loss 1.323270559310913
Model None Epoch 58 Batch 146: Loss 1.281158447265625
Model None Epoch 58 Batch 147: Loss 1.365668535232544
Model None Epoch 58 Batch 148: Loss 1.3828121423721313
Model None Epoch 58 Batch 149: Loss 1.3103280067443848
Downstream Train Epoch: 58 [38400/50000 (77%)]	Loss: 1.254089
Model None Epoch 58 Batch 150: Loss 1.254089117050171
Model None Epoch 58 Batch 151: Loss 1.4762744903564453
Model None Epoch 58 Batch 152: Loss 1.345497727394104
Model None Epoch 58 Batch 153: Loss 1.5003328323364258
Model None Epoch 58 Batch 154: Loss 1.414656639099121
Model None Epoch 58 Batch 155: Loss 1.3851178884506226
Model None Epoch 58 Batch 156: Loss 1.3394136428833008
Model None Epoch 58 Batch 157: Loss 1.3703306913375854
Model None Epoch 58 Batch 158: Loss 1.377528190612793
Model None Epoch 58 Batch 159: Loss 1.4126077890396118
Model None Epoch 58 Batch 160: Loss 1.472501516342163
Model None Epoch 58 Batch 161: Loss 1.3001470565795898
Model None Epoch 58 Batch 162: Loss 1.4634798765182495
Model None Epoch 58 Batch 163: Loss 1.2590585947036743
Model None Epoch 58 Batch 164: Loss 1.5377299785614014
Model None Epoch 58 Batch 165: Loss 1.33556067943573
Model None Epoch 58 Batch 166: Loss 1.3330895900726318
Model None Epoch 58 Batch 167: Loss 1.3071143627166748
Model None Epoch 58 Batch 168: Loss 1.2497426271438599
Model None Epoch 58 Batch 169: Loss 1.3672007322311401
Model None Epoch 58 Batch 170: Loss 1.4501605033874512
Model None Epoch 58 Batch 171: Loss 1.4509930610656738
Model None Epoch 58 Batch 172: Loss 1.3293911218643188
Model None Epoch 58 Batch 173: Loss 1.3327021598815918
Model None Epoch 58 Batch 174: Loss 1.378068208694458
Model None Epoch 58 Batch 175: Loss 1.413000226020813
Model None Epoch 58 Batch 176: Loss 1.353297233581543
Model None Epoch 58 Batch 177: Loss 1.3755700588226318
Model None Epoch 58 Batch 178: Loss 1.3797073364257812
Model None Epoch 58 Batch 179: Loss 1.3960069417953491
Model None Epoch 58 Batch 180: Loss 1.3412269353866577
Model None Epoch 58 Batch 181: Loss 1.3583518266677856
Model None Epoch 58 Batch 182: Loss 1.3015835285186768
Model None Epoch 58 Batch 183: Loss 1.3465583324432373
Model None Epoch 58 Batch 184: Loss 1.2781832218170166
Model None Epoch 58 Batch 185: Loss 1.4779082536697388
Model None Epoch 58 Batch 186: Loss 1.485660433769226
Model None Epoch 58 Batch 187: Loss 1.4374091625213623
Model None Epoch 58 Batch 188: Loss 1.52005136013031
Model None Epoch 58 Batch 189: Loss 1.4242733716964722
Model None Epoch 58 Batch 190: Loss 1.4433780908584595
Model None Epoch 58 Batch 191: Loss 1.4433364868164062
Model None Epoch 58 Batch 192: Loss 1.3634824752807617
Model None Epoch 58 Batch 193: Loss 1.4127167463302612
Model None Epoch 58 Batch 194: Loss 1.4292534589767456
Model None Epoch 58 Batch 195: Loss 1.4629671573638916

 Downstream Train loss: 1.3941238887455998 Acc: 0.5662
Downstream Train Epoch: 59 [0/50000 (0%)]	Loss: 1.315298
Model None Epoch 59 Batch 0: Loss 1.3152979612350464
Model None Epoch 59 Batch 1: Loss 1.3630435466766357
Model None Epoch 59 Batch 2: Loss 1.4803693294525146
Model None Epoch 59 Batch 3: Loss 1.3855854272842407
Model None Epoch 59 Batch 4: Loss 1.4412039518356323
Model None Epoch 59 Batch 5: Loss 1.3985005617141724
Model None Epoch 59 Batch 6: Loss 1.3394018411636353
Model None Epoch 59 Batch 7: Loss 1.5084275007247925
Model None Epoch 59 Batch 8: Loss 1.4887597560882568
Model None Epoch 59 Batch 9: Loss 1.366983413696289
Model None Epoch 59 Batch 10: Loss 1.5038260221481323
Model None Epoch 59 Batch 11: Loss 1.435286521911621
Model None Epoch 59 Batch 12: Loss 1.4515434503555298
Model None Epoch 59 Batch 13: Loss 1.3139808177947998
Model None Epoch 59 Batch 14: Loss 1.3230472803115845
Model None Epoch 59 Batch 15: Loss 1.4224438667297363
Model None Epoch 59 Batch 16: Loss 1.3780337572097778
Model None Epoch 59 Batch 17: Loss 1.3352099657058716
Model None Epoch 59 Batch 18: Loss 1.3042494058609009
Model None Epoch 59 Batch 19: Loss 1.3551239967346191
Model None Epoch 59 Batch 20: Loss 1.439532995223999
Model None Epoch 59 Batch 21: Loss 1.2977243661880493
Model None Epoch 59 Batch 22: Loss 1.5236659049987793
Model None Epoch 59 Batch 23: Loss 1.386265516281128
Model None Epoch 59 Batch 24: Loss 1.3961726427078247
Model None Epoch 59 Batch 25: Loss 1.3586899042129517
Model None Epoch 59 Batch 26: Loss 1.3605809211730957
Model None Epoch 59 Batch 27: Loss 1.3633402585983276
Model None Epoch 59 Batch 28: Loss 1.354841947555542
Model None Epoch 59 Batch 29: Loss 1.4517720937728882
Model None Epoch 59 Batch 30: Loss 1.4243115186691284
Model None Epoch 59 Batch 31: Loss 1.4497733116149902
Model None Epoch 59 Batch 32: Loss 1.3888596296310425
Model None Epoch 59 Batch 33: Loss 1.4024711847305298
Model None Epoch 59 Batch 34: Loss 1.3629212379455566
Model None Epoch 59 Batch 35: Loss 1.3203846216201782
Model None Epoch 59 Batch 36: Loss 1.4678456783294678
Model None Epoch 59 Batch 37: Loss 1.3678910732269287
Model None Epoch 59 Batch 38: Loss 1.3599714040756226
Model None Epoch 59 Batch 39: Loss 1.3506180047988892
Model None Epoch 59 Batch 40: Loss 1.4034053087234497
Model None Epoch 59 Batch 41: Loss 1.400485634803772
Model None Epoch 59 Batch 42: Loss 1.2958043813705444
Model None Epoch 59 Batch 43: Loss 1.484680414199829
Model None Epoch 59 Batch 44: Loss 1.4621306657791138
Model None Epoch 59 Batch 45: Loss 1.4188120365142822
Model None Epoch 59 Batch 46: Loss 1.4551665782928467
Model None Epoch 59 Batch 47: Loss 1.4791678190231323
Model None Epoch 59 Batch 48: Loss 1.4305293560028076
Model None Epoch 59 Batch 49: Loss 1.3840525150299072
Downstream Train Epoch: 59 [12800/50000 (26%)]	Loss: 1.425928
Model None Epoch 59 Batch 50: Loss 1.4259278774261475
Model None Epoch 59 Batch 51: Loss 1.40998113155365
Model None Epoch 59 Batch 52: Loss 1.345755696296692
Model None Epoch 59 Batch 53: Loss 1.4443588256835938
Model None Epoch 59 Batch 54: Loss 1.3427822589874268
Model None Epoch 59 Batch 55: Loss 1.3819689750671387
Model None Epoch 59 Batch 56: Loss 1.4411358833312988
Model None Epoch 59 Batch 57: Loss 1.3298310041427612
Model None Epoch 59 Batch 58: Loss 1.3300384283065796
Model None Epoch 59 Batch 59: Loss 1.4101738929748535
Model None Epoch 59 Batch 60: Loss 1.5164793729782104
Model None Epoch 59 Batch 61: Loss 1.3943358659744263
Model None Epoch 59 Batch 62: Loss 1.469294548034668
Model None Epoch 59 Batch 63: Loss 1.4379608631134033
Model None Epoch 59 Batch 64: Loss 1.3658722639083862
Model None Epoch 59 Batch 65: Loss 1.3901116847991943
Model None Epoch 59 Batch 66: Loss 1.3677937984466553
Model None Epoch 59 Batch 67: Loss 1.535659670829773
Model None Epoch 59 Batch 68: Loss 1.477866768836975
Model None Epoch 59 Batch 69: Loss 1.2878105640411377
Model None Epoch 59 Batch 70: Loss 1.4462532997131348
Model None Epoch 59 Batch 71: Loss 1.3755372762680054
Model None Epoch 59 Batch 72: Loss 1.3331525325775146
Model None Epoch 59 Batch 73: Loss 1.4537718296051025
Model None Epoch 59 Batch 74: Loss 1.3983900547027588
Model None Epoch 59 Batch 75: Loss 1.3421233892440796
Model None Epoch 59 Batch 76: Loss 1.5031628608703613
Model None Epoch 59 Batch 77: Loss 1.3438080549240112
Model None Epoch 59 Batch 78: Loss 1.341391921043396
Model None Epoch 59 Batch 79: Loss 1.4409970045089722
Model None Epoch 59 Batch 80: Loss 1.453324317932129
Model None Epoch 59 Batch 81: Loss 1.4342617988586426
Model None Epoch 59 Batch 82: Loss 1.3731447458267212
Model None Epoch 59 Batch 83: Loss 1.4245991706848145
Model None Epoch 59 Batch 84: Loss 1.48081374168396
Model None Epoch 59 Batch 85: Loss 1.4422969818115234
Model None Epoch 59 Batch 86: Loss 1.553687334060669
Model None Epoch 59 Batch 87: Loss 1.3872147798538208
Model None Epoch 59 Batch 88: Loss 1.4376970529556274
Model None Epoch 59 Batch 89: Loss 1.392318606376648
Model None Epoch 59 Batch 90: Loss 1.4108459949493408
Model None Epoch 59 Batch 91: Loss 1.4397027492523193
Model None Epoch 59 Batch 92: Loss 1.3752965927124023
Model None Epoch 59 Batch 93: Loss 1.3851745128631592
Model None Epoch 59 Batch 94: Loss 1.3410755395889282
Model None Epoch 59 Batch 95: Loss 1.2866747379302979
Model None Epoch 59 Batch 96: Loss 1.4366823434829712
Model None Epoch 59 Batch 97: Loss 1.3837069272994995
Model None Epoch 59 Batch 98: Loss 1.3853702545166016
Model None Epoch 59 Batch 99: Loss 1.5022269487380981
Downstream Train Epoch: 59 [25600/50000 (51%)]	Loss: 1.353549
Model None Epoch 59 Batch 100: Loss 1.3535494804382324
Model None Epoch 59 Batch 101: Loss 1.4681100845336914
Model None Epoch 59 Batch 102: Loss 1.4413167238235474
Model None Epoch 59 Batch 103: Loss 1.4749584197998047
Model None Epoch 59 Batch 104: Loss 1.3680647611618042
Model None Epoch 59 Batch 105: Loss 1.2580875158309937
Model None Epoch 59 Batch 106: Loss 1.4121156930923462
Model None Epoch 59 Batch 107: Loss 1.4516140222549438
Model None Epoch 59 Batch 108: Loss 1.3168165683746338
Model None Epoch 59 Batch 109: Loss 1.3160117864608765
Model None Epoch 59 Batch 110: Loss 1.3068000078201294
Model None Epoch 59 Batch 111: Loss 1.384010910987854
Model None Epoch 59 Batch 112: Loss 1.4365636110305786
Model None Epoch 59 Batch 113: Loss 1.3731170892715454
Model None Epoch 59 Batch 114: Loss 1.2772858142852783
Model None Epoch 59 Batch 115: Loss 1.410293698310852
Model None Epoch 59 Batch 116: Loss 1.3765034675598145
Model None Epoch 59 Batch 117: Loss 1.409415364265442
Model None Epoch 59 Batch 118: Loss 1.3036844730377197
Model None Epoch 59 Batch 119: Loss 1.4287927150726318
Model None Epoch 59 Batch 120: Loss 1.4369730949401855
Model None Epoch 59 Batch 121: Loss 1.4309852123260498
Model None Epoch 59 Batch 122: Loss 1.5268526077270508
Model None Epoch 59 Batch 123: Loss 1.3732681274414062
Model None Epoch 59 Batch 124: Loss 1.3452227115631104
Model None Epoch 59 Batch 125: Loss 1.318482756614685
Model None Epoch 59 Batch 126: Loss 1.430058479309082
Model None Epoch 59 Batch 127: Loss 1.3183777332305908
Model None Epoch 59 Batch 128: Loss 1.4460408687591553
Model None Epoch 59 Batch 129: Loss 1.4483821392059326
Model None Epoch 59 Batch 130: Loss 1.3736515045166016
Model None Epoch 59 Batch 131: Loss 1.4928832054138184
Model None Epoch 59 Batch 132: Loss 1.4800359010696411
Model None Epoch 59 Batch 133: Loss 1.393955111503601
Model None Epoch 59 Batch 134: Loss 1.3651758432388306
Model None Epoch 59 Batch 135: Loss 1.4667229652404785
Model None Epoch 59 Batch 136: Loss 1.3971338272094727
Model None Epoch 59 Batch 137: Loss 1.3869218826293945
Model None Epoch 59 Batch 138: Loss 1.390764832496643
Model None Epoch 59 Batch 139: Loss 1.3754981756210327
Model None Epoch 59 Batch 140: Loss 1.3235890865325928
Model None Epoch 59 Batch 141: Loss 1.4479265213012695
Model None Epoch 59 Batch 142: Loss 1.4580409526824951
Model None Epoch 59 Batch 143: Loss 1.5251882076263428
Model None Epoch 59 Batch 144: Loss 1.3714356422424316
Model None Epoch 59 Batch 145: Loss 1.3374221324920654
Model None Epoch 59 Batch 146: Loss 1.3253977298736572
Model None Epoch 59 Batch 147: Loss 1.455827236175537
Model None Epoch 59 Batch 148: Loss 1.3516730070114136
Model None Epoch 59 Batch 149: Loss 1.4913790225982666
Downstream Train Epoch: 59 [38400/50000 (77%)]	Loss: 1.361872
Model None Epoch 59 Batch 150: Loss 1.3618717193603516
Model None Epoch 59 Batch 151: Loss 1.418224811553955
Model None Epoch 59 Batch 152: Loss 1.4181874990463257
Model None Epoch 59 Batch 153: Loss 1.331115484237671
Model None Epoch 59 Batch 154: Loss 1.409698486328125
Model None Epoch 59 Batch 155: Loss 1.3348649740219116
Model None Epoch 59 Batch 156: Loss 1.3995213508605957
Model None Epoch 59 Batch 157: Loss 1.350132942199707
Model None Epoch 59 Batch 158: Loss 1.4083393812179565
Model None Epoch 59 Batch 159: Loss 1.428916096687317
Model None Epoch 59 Batch 160: Loss 1.4742136001586914
Model None Epoch 59 Batch 161: Loss 1.3518903255462646
Model None Epoch 59 Batch 162: Loss 1.3675521612167358
Model None Epoch 59 Batch 163: Loss 1.3734545707702637
Model None Epoch 59 Batch 164: Loss 1.3395394086837769
Model None Epoch 59 Batch 165: Loss 1.429215431213379
Model None Epoch 59 Batch 166: Loss 1.4622993469238281
Model None Epoch 59 Batch 167: Loss 1.4122064113616943
Model None Epoch 59 Batch 168: Loss 1.421561598777771
Model None Epoch 59 Batch 169: Loss 1.4304617643356323
Model None Epoch 59 Batch 170: Loss 1.3217129707336426
Model None Epoch 59 Batch 171: Loss 1.3891792297363281
Model None Epoch 59 Batch 172: Loss 1.4979312419891357
Model None Epoch 59 Batch 173: Loss 1.4028866291046143
Model None Epoch 59 Batch 174: Loss 1.4001415967941284
Model None Epoch 59 Batch 175: Loss 1.410305142402649
Model None Epoch 59 Batch 176: Loss 1.4083551168441772
Model None Epoch 59 Batch 177: Loss 1.453926920890808
Model None Epoch 59 Batch 178: Loss 1.387837529182434
Model None Epoch 59 Batch 179: Loss 1.43560791015625
Model None Epoch 59 Batch 180: Loss 1.4673051834106445
Model None Epoch 59 Batch 181: Loss 1.4099026918411255
Model None Epoch 59 Batch 182: Loss 1.3005955219268799
Model None Epoch 59 Batch 183: Loss 1.3818057775497437
Model None Epoch 59 Batch 184: Loss 1.4680912494659424
Model None Epoch 59 Batch 185: Loss 1.4685406684875488
Model None Epoch 59 Batch 186: Loss 1.4784599542617798
Model None Epoch 59 Batch 187: Loss 1.381468653678894
Model None Epoch 59 Batch 188: Loss 1.3374428749084473
Model None Epoch 59 Batch 189: Loss 1.3239350318908691
Model None Epoch 59 Batch 190: Loss 1.3836567401885986
Model None Epoch 59 Batch 191: Loss 1.3549809455871582
Model None Epoch 59 Batch 192: Loss 1.4144788980484009
Model None Epoch 59 Batch 193: Loss 1.2989050149917603
Model None Epoch 59 Batch 194: Loss 1.468549370765686
Model None Epoch 59 Batch 195: Loss 1.157620906829834

 Downstream Train loss: 1.3997005096503667 Acc: 0.5662
Downstream Train Epoch: 60 [0/50000 (0%)]	Loss: 1.443096
Model None Epoch 60 Batch 0: Loss 1.4430955648422241
Model None Epoch 60 Batch 1: Loss 1.345374345779419
Model None Epoch 60 Batch 2: Loss 1.4205832481384277
Model None Epoch 60 Batch 3: Loss 1.3129411935806274
Model None Epoch 60 Batch 4: Loss 1.3192061185836792
Model None Epoch 60 Batch 5: Loss 1.3476009368896484
Model None Epoch 60 Batch 6: Loss 1.4010545015335083
Model None Epoch 60 Batch 7: Loss 1.3890306949615479
Model None Epoch 60 Batch 8: Loss 1.4702314138412476
Model None Epoch 60 Batch 9: Loss 1.2826203107833862
Model None Epoch 60 Batch 10: Loss 1.4749658107757568
Model None Epoch 60 Batch 11: Loss 1.4453016519546509
Model None Epoch 60 Batch 12: Loss 1.358036994934082
Model None Epoch 60 Batch 13: Loss 1.4220387935638428
Model None Epoch 60 Batch 14: Loss 1.488034725189209
Model None Epoch 60 Batch 15: Loss 1.3257975578308105
Model None Epoch 60 Batch 16: Loss 1.4497843980789185
Model None Epoch 60 Batch 17: Loss 1.4501137733459473
Model None Epoch 60 Batch 18: Loss 1.3352925777435303
Model None Epoch 60 Batch 19: Loss 1.2944101095199585
Model None Epoch 60 Batch 20: Loss 1.3426786661148071
Model None Epoch 60 Batch 21: Loss 1.4221928119659424
Model None Epoch 60 Batch 22: Loss 1.3774334192276
Model None Epoch 60 Batch 23: Loss 1.3463118076324463
Model None Epoch 60 Batch 24: Loss 1.4103506803512573
Model None Epoch 60 Batch 25: Loss 1.3333858251571655
Model None Epoch 60 Batch 26: Loss 1.4476232528686523
Model None Epoch 60 Batch 27: Loss 1.3022228479385376
Model None Epoch 60 Batch 28: Loss 1.412123441696167
Model None Epoch 60 Batch 29: Loss 1.4234431982040405
Model None Epoch 60 Batch 30: Loss 1.4661428928375244
Model None Epoch 60 Batch 31: Loss 1.3755849599838257
Model None Epoch 60 Batch 32: Loss 1.332673192024231
Model None Epoch 60 Batch 33: Loss 1.39971923828125
Model None Epoch 60 Batch 34: Loss 1.356689453125
Model None Epoch 60 Batch 35: Loss 1.4167544841766357
Model None Epoch 60 Batch 36: Loss 1.4005650281906128
Model None Epoch 60 Batch 37: Loss 1.409635066986084
Model None Epoch 60 Batch 38: Loss 1.4584745168685913
Model None Epoch 60 Batch 39: Loss 1.4202442169189453
Model None Epoch 60 Batch 40: Loss 1.455302357673645
Model None Epoch 60 Batch 41: Loss 1.4525353908538818
Model None Epoch 60 Batch 42: Loss 1.323203206062317
Model None Epoch 60 Batch 43: Loss 1.5217843055725098
Model None Epoch 60 Batch 44: Loss 1.4400075674057007
Model None Epoch 60 Batch 45: Loss 1.4853178262710571
Model None Epoch 60 Batch 46: Loss 1.5022201538085938
Model None Epoch 60 Batch 47: Loss 1.3982031345367432
Model None Epoch 60 Batch 48: Loss 1.310838222503662
Model None Epoch 60 Batch 49: Loss 1.3202821016311646
Downstream Train Epoch: 60 [12800/50000 (26%)]	Loss: 1.384095
Model None Epoch 60 Batch 50: Loss 1.3840954303741455
Model None Epoch 60 Batch 51: Loss 1.4216312170028687
Model None Epoch 60 Batch 52: Loss 1.4454632997512817
Model None Epoch 60 Batch 53: Loss 1.4766690731048584
Model None Epoch 60 Batch 54: Loss 1.3690052032470703
Model None Epoch 60 Batch 55: Loss 1.4158523082733154
Model None Epoch 60 Batch 56: Loss 1.3771097660064697
Model None Epoch 60 Batch 57: Loss 1.4204413890838623
Model None Epoch 60 Batch 58: Loss 1.3565242290496826
Model None Epoch 60 Batch 59: Loss 1.3518131971359253
Model None Epoch 60 Batch 60: Loss 1.462074875831604
Model None Epoch 60 Batch 61: Loss 1.377121090888977
Model None Epoch 60 Batch 62: Loss 1.3737335205078125
Model None Epoch 60 Batch 63: Loss 1.3341995477676392
Model None Epoch 60 Batch 64: Loss 1.3578367233276367
Model None Epoch 60 Batch 65: Loss 1.5279152393341064
Model None Epoch 60 Batch 66: Loss 1.416157603263855
Model None Epoch 60 Batch 67: Loss 1.4054059982299805
Model None Epoch 60 Batch 68: Loss 1.3285934925079346
Model None Epoch 60 Batch 69: Loss 1.4502876996994019
Model None Epoch 60 Batch 70: Loss 1.378319501876831
Model None Epoch 60 Batch 71: Loss 1.4228103160858154
Model None Epoch 60 Batch 72: Loss 1.3022687435150146
Model None Epoch 60 Batch 73: Loss 1.414433240890503
Model None Epoch 60 Batch 74: Loss 1.30793035030365
Model None Epoch 60 Batch 75: Loss 1.479388952255249
Model None Epoch 60 Batch 76: Loss 1.351495623588562
Model None Epoch 60 Batch 77: Loss 1.5332601070404053
Model None Epoch 60 Batch 78: Loss 1.4070442914962769
Model None Epoch 60 Batch 79: Loss 1.4184576272964478
Model None Epoch 60 Batch 80: Loss 1.419417381286621
Model None Epoch 60 Batch 81: Loss 1.351925015449524
Model None Epoch 60 Batch 82: Loss 1.3398728370666504
Model None Epoch 60 Batch 83: Loss 1.3620777130126953
Model None Epoch 60 Batch 84: Loss 1.4878060817718506
Model None Epoch 60 Batch 85: Loss 1.5118402242660522
Model None Epoch 60 Batch 86: Loss 1.4252310991287231
Model None Epoch 60 Batch 87: Loss 1.3865981101989746
Model None Epoch 60 Batch 88: Loss 1.4758285284042358
Model None Epoch 60 Batch 89: Loss 1.346069097518921
Model None Epoch 60 Batch 90: Loss 1.4124069213867188
Model None Epoch 60 Batch 91: Loss 1.4100241661071777
Model None Epoch 60 Batch 92: Loss 1.3885307312011719
Model None Epoch 60 Batch 93: Loss 1.5140866041183472
Model None Epoch 60 Batch 94: Loss 1.4205973148345947
Model None Epoch 60 Batch 95: Loss 1.4402192831039429
Model None Epoch 60 Batch 96: Loss 1.2923643589019775
Model None Epoch 60 Batch 97: Loss 1.3886860609054565
Model None Epoch 60 Batch 98: Loss 1.3598521947860718
Model None Epoch 60 Batch 99: Loss 1.5241361856460571
Downstream Train Epoch: 60 [25600/50000 (51%)]	Loss: 1.421562
Model None Epoch 60 Batch 100: Loss 1.4215623140335083
Model None Epoch 60 Batch 101: Loss 1.5864076614379883
Model None Epoch 60 Batch 102: Loss 1.4052486419677734
Model None Epoch 60 Batch 103: Loss 1.4061297178268433
Model None Epoch 60 Batch 104: Loss 1.4340566396713257
Model None Epoch 60 Batch 105: Loss 1.2720094919204712
Model None Epoch 60 Batch 106: Loss 1.3496572971343994
Model None Epoch 60 Batch 107: Loss 1.325562834739685
Model None Epoch 60 Batch 108: Loss 1.3152024745941162
Model None Epoch 60 Batch 109: Loss 1.374538540840149
Model None Epoch 60 Batch 110: Loss 1.2704002857208252
Model None Epoch 60 Batch 111: Loss 1.380379557609558
Model None Epoch 60 Batch 112: Loss 1.4098467826843262
Model None Epoch 60 Batch 113: Loss 1.3529577255249023
Model None Epoch 60 Batch 114: Loss 1.3541646003723145
Model None Epoch 60 Batch 115: Loss 1.409664511680603
Model None Epoch 60 Batch 116: Loss 1.4311150312423706
Model None Epoch 60 Batch 117: Loss 1.519431710243225
Model None Epoch 60 Batch 118: Loss 1.4443825483322144
Model None Epoch 60 Batch 119: Loss 1.3995541334152222
Model None Epoch 60 Batch 120: Loss 1.4187510013580322
Model None Epoch 60 Batch 121: Loss 1.351890206336975
Model None Epoch 60 Batch 122: Loss 1.4503251314163208
Model None Epoch 60 Batch 123: Loss 1.435209035873413
Model None Epoch 60 Batch 124: Loss 1.362762212753296
Model None Epoch 60 Batch 125: Loss 1.3539400100708008
Model None Epoch 60 Batch 126: Loss 1.396306037902832
Model None Epoch 60 Batch 127: Loss 1.4852218627929688
Model None Epoch 60 Batch 128: Loss 1.402199387550354
Model None Epoch 60 Batch 129: Loss 1.3606420755386353
Model None Epoch 60 Batch 130: Loss 1.5032933950424194
Model None Epoch 60 Batch 131: Loss 1.477597951889038
Model None Epoch 60 Batch 132: Loss 1.4787132740020752
Model None Epoch 60 Batch 133: Loss 1.5046567916870117
Model None Epoch 60 Batch 134: Loss 1.3524413108825684
Model None Epoch 60 Batch 135: Loss 1.45502769947052
Model None Epoch 60 Batch 136: Loss 1.3781230449676514
Model None Epoch 60 Batch 137: Loss 1.3169019222259521
Model None Epoch 60 Batch 138: Loss 1.4868437051773071
Model None Epoch 60 Batch 139: Loss 1.4833189249038696
Model None Epoch 60 Batch 140: Loss 1.3349007368087769
Model None Epoch 60 Batch 141: Loss 1.3552309274673462
Model None Epoch 60 Batch 142: Loss 1.3473392724990845
Model None Epoch 60 Batch 143: Loss 1.498659372329712
Model None Epoch 60 Batch 144: Loss 1.3492718935012817
Model None Epoch 60 Batch 145: Loss 1.4367845058441162
Model None Epoch 60 Batch 146: Loss 1.4326328039169312
Model None Epoch 60 Batch 147: Loss 1.4309577941894531
Model None Epoch 60 Batch 148: Loss 1.4143588542938232
Model None Epoch 60 Batch 149: Loss 1.4507237672805786
Downstream Train Epoch: 60 [38400/50000 (77%)]	Loss: 1.360807
Model None Epoch 60 Batch 150: Loss 1.360806941986084
Model None Epoch 60 Batch 151: Loss 1.3338440656661987
Model None Epoch 60 Batch 152: Loss 1.4301198720932007
Model None Epoch 60 Batch 153: Loss 1.4736549854278564
Model None Epoch 60 Batch 154: Loss 1.5343142747879028
Model None Epoch 60 Batch 155: Loss 1.38972806930542
Model None Epoch 60 Batch 156: Loss 1.4112640619277954
Model None Epoch 60 Batch 157: Loss 1.3839548826217651
Model None Epoch 60 Batch 158: Loss 1.4387149810791016
Model None Epoch 60 Batch 159: Loss 1.287904143333435
Model None Epoch 60 Batch 160: Loss 1.4612302780151367
Model None Epoch 60 Batch 161: Loss 1.376524567604065
Model None Epoch 60 Batch 162: Loss 1.4840785264968872
Model None Epoch 60 Batch 163: Loss 1.328902006149292
Model None Epoch 60 Batch 164: Loss 1.402408242225647
Model None Epoch 60 Batch 165: Loss 1.4966636896133423
Model None Epoch 60 Batch 166: Loss 1.416903018951416
Model None Epoch 60 Batch 167: Loss 1.3476223945617676
Model None Epoch 60 Batch 168: Loss 1.3895233869552612
Model None Epoch 60 Batch 169: Loss 1.4127812385559082
Model None Epoch 60 Batch 170: Loss 1.37063729763031
Model None Epoch 60 Batch 171: Loss 1.4641733169555664
Model None Epoch 60 Batch 172: Loss 1.432854413986206
Model None Epoch 60 Batch 173: Loss 1.3305447101593018
Model None Epoch 60 Batch 174: Loss 1.469787836074829
Model None Epoch 60 Batch 175: Loss 1.3687083721160889
Model None Epoch 60 Batch 176: Loss 1.3859988451004028
Model None Epoch 60 Batch 177: Loss 1.3641808032989502
Model None Epoch 60 Batch 178: Loss 1.3853214979171753
Model None Epoch 60 Batch 179: Loss 1.4877554178237915
Model None Epoch 60 Batch 180: Loss 1.4073872566223145
Model None Epoch 60 Batch 181: Loss 1.4241787195205688
Model None Epoch 60 Batch 182: Loss 1.4724338054656982
Model None Epoch 60 Batch 183: Loss 1.37366783618927
Model None Epoch 60 Batch 184: Loss 1.4323327541351318
Model None Epoch 60 Batch 185: Loss 1.293923020362854
Model None Epoch 60 Batch 186: Loss 1.459643006324768
Model None Epoch 60 Batch 187: Loss 1.390771508216858
Model None Epoch 60 Batch 188: Loss 1.333611011505127
Model None Epoch 60 Batch 189: Loss 1.4186313152313232
Model None Epoch 60 Batch 190: Loss 1.3950051069259644
Model None Epoch 60 Batch 191: Loss 1.4621880054473877
Model None Epoch 60 Batch 192: Loss 1.413233995437622
Model None Epoch 60 Batch 193: Loss 1.3329631090164185
Model None Epoch 60 Batch 194: Loss 1.3525350093841553
Model None Epoch 60 Batch 195: Loss 1.6227948665618896

 Downstream Train loss: 1.4047850583280836 Acc: 0.5662
 
 Results after 30 global rounds of training:
|----Average Classifier Test Accuracy for 5 agents is: 56.62%

 Total Run Time: 1249.7188
Namespace(exact_diffusion=False, decentralized=False, edge_prob=1, epochs=30, num_users=5, frac=1.0, local_ep=5.0, local_bs=256, lr=0.001, momentum=0.9, num_workers=16, model='resnet', batch_size=256, weight_decay=0.0005, dataset='mnist', backbone='resnet18', num_classes=10, gpu='0', optimizer='adam', save_name_suffix='', iid=1, verbose=0, seed=1, feature_dim=128, temperature=0.5, k=200, ssl_method='simclr', x_noniid=False, dirichlet=False, test_intermediate=False, dir_beta=0.5, imagenet_based_cluster=False, y_partition=False, log_file_name='results/05-18_04:52_testingPhi_iid_dec-false_ED-false_ep0_e30_le5_a5/skew_ssl_comm', num_clusters=1, imagenet100=False, y_partition_skew=True, y_partition_ratio=0.0, x_shift_dirichlet=False, reg_scale=1, load_pretrained_path='', full_size=False, local_rank=0, distributed_training=False, log_directory='results/05-18_04:52_testingPhi_iid_dec-false_ED-false_ep0_e30_le5_a5', emd=0, dist_url='env://', average_without_bn=False, model_continue_training=0, finetuning_epoch=60, script_name='', x_shift_skew=False, load_dataset_to_memory=False, model_time='05-18_04:53_3082253', start_time=datetime.datetime(2024, 5, 18, 4, 53, 16, 538881))
dec_False_ED_False_a_5_resnet_256_30__dec_ssl_simclr
